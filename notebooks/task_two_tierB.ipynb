{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4193aa71",
   "metadata": {},
   "source": [
    "# Tier B: The Semanticist - AI vs Human Text Detection\n",
    "## Using Averaged Pre-trained Word Embeddings (GloVe) + Feedforward NN\n",
    "\n",
    "This notebook implements a binary classifier that distinguishes AI-generated from human-written text using:\n",
    "- **Pre-trained GloVe embeddings** (glove.6B.100d)\n",
    "- **Averaged embedding vectors** for each paragraph\n",
    "- **Feedforward Neural Network** (PyTorch)\n",
    "\n",
    "**Author**: Tier B Implementation  \n",
    "**Dataset**: Human novels (class1) + AI-generated paragraphs (class2)  \n",
    "**Model**: Feedforward NN with averaged embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99d0dd",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ab19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                              f1_score, roc_auc_score, confusion_matrix, \n",
    "                              classification_report)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162cb44",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Preparation\n",
    "\n",
    "**Data Sources:**\n",
    "- **Class 1 (Human)**: Cleaned novel texts from 5 novels, chunked into ~200-word paragraphs\n",
    "- **Class 2 (AI-generated)**: Pre-generated paragraphs (500 per novel = 2500 total)\n",
    "\n",
    "**Process:**\n",
    "1. Load cleaned human text and chunk into paragraphs\n",
    "2. Load AI-generated JSONL files\n",
    "3. Combine into a single dataset with labels (0=Human, 1=AI)\n",
    "4. Create a DataFrame and save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths for Kaggle vs Local execution\n",
    "# In Kaggle, you'll upload the output/ folder as a dataset\n",
    "# For local testing, use relative paths\n",
    "\n",
    "import os\n",
    "\n",
    "# Detect if running in Kaggle\n",
    "IN_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    # Kaggle paths - adjust based on your uploaded dataset name\n",
    "    BASE_PATH = Path('/kaggle/input/precog-novels-data')  # Change this to your dataset name\n",
    "    CLASS1_PATH = BASE_PATH / 'class1'\n",
    "    CLASS2_PATH = BASE_PATH / 'class2'\n",
    "else:\n",
    "    # Local paths\n",
    "    BASE_PATH = Path('../output')\n",
    "    CLASS1_PATH = BASE_PATH / 'class1'\n",
    "    CLASS2_PATH = BASE_PATH / 'class2'\n",
    "\n",
    "print(f\"Running in: {'Kaggle' if IN_KAGGLE else 'Local'}\")\n",
    "print(f\"Base path: {BASE_PATH}\")\n",
    "print(f\"Class1 path exists: {CLASS1_PATH.exists()}\")\n",
    "print(f\"Class2 path exists: {CLASS2_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=200):\n",
    "    \"\"\"\n",
    "    Chunk text into paragraphs of approximately chunk_size words.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        chunk_size: Target number of words per chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if len(chunk.split()) >= 50:  # Minimum 50 words per chunk\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def load_human_data(class1_path):\n",
    "    \"\"\"\n",
    "    Load human-written text from cleaned novel files and chunk them.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with 'text' and 'label' keys\n",
    "    \"\"\"\n",
    "    novels = [\n",
    "        'heart_of_darkness_cleaned.txt',\n",
    "        'lord_jim_cleaned.txt',\n",
    "        'metamorphosis_cleaned.txt',\n",
    "        'the_trial_cleaned.txt',\n",
    "        'typhoon_cleaned.txt'\n",
    "    ]\n",
    "    \n",
    "    human_data = []\n",
    "    \n",
    "    for novel_file in novels:\n",
    "        file_path = class1_path / novel_file\n",
    "        if file_path.exists():\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Chunk the text\n",
    "            chunks = chunk_text(text, chunk_size=200)\n",
    "            \n",
    "            # Add to dataset\n",
    "            for chunk in chunks:\n",
    "                human_data.append({\n",
    "                    'text': chunk,\n",
    "                    'label': 0,  # 0 = Human\n",
    "                    'source': novel_file.replace('_cleaned.txt', '')\n",
    "                })\n",
    "            \n",
    "            print(f\"✓ Loaded {novel_file}: {len(chunks)} chunks\")\n",
    "        else:\n",
    "            print(f\"✗ File not found: {file_path}\")\n",
    "    \n",
    "    return human_data\n",
    "\n",
    "\n",
    "def load_ai_data(class2_path):\n",
    "    \"\"\"\n",
    "    Load AI-generated text from JSONL files.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with 'text' and 'label' keys\n",
    "    \"\"\"\n",
    "    novels = [\n",
    "        'heart_of_darkness_generic.jsonl',\n",
    "        'lord_jim_generic.jsonl',\n",
    "        'metamorphosis_generic.jsonl',\n",
    "        'the_trial_generic.jsonl',\n",
    "        'typhoon_generic.jsonl'\n",
    "    ]\n",
    "    \n",
    "    ai_data = []\n",
    "    \n",
    "    for novel_file in novels:\n",
    "        file_path = class2_path / novel_file\n",
    "        if file_path.exists():\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            # Parse JSONL\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    entry = json.loads(line.strip())\n",
    "                    # Extract text (adjust key based on your JSONL structure)\n",
    "                    text = entry.get('text') or entry.get('paragraph') or entry.get('content', '')\n",
    "                    \n",
    "                    if text and len(text.split()) >= 50:  # Minimum 50 words\n",
    "                        ai_data.append({\n",
    "                            'text': text,\n",
    "                            'label': 1,  # 1 = AI\n",
    "                            'source': novel_file.replace('_generic.jsonl', '')\n",
    "                        })\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"✓ Loaded {novel_file}: {len([d for d in ai_data if novel_file.replace('_generic.jsonl', '') in d['source']])} paragraphs\")\n",
    "        else:\n",
    "            print(f\"✗ File not found: {file_path}\")\n",
    "    \n",
    "    return ai_data\n",
    "\n",
    "\n",
    "# Load all data\n",
    "print(\"Loading Human data (Class 1)...\")\n",
    "human_data = load_human_data(CLASS1_PATH)\n",
    "\n",
    "print(\"\\nLoading AI data (Class 2)...\")\n",
    "ai_data = load_ai_data(CLASS2_PATH)\n",
    "\n",
    "# Combine datasets\n",
    "all_data = human_data + ai_data\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total Human paragraphs: {len(human_data)}\")\n",
    "print(f\"Total AI paragraphs: {len(ai_data)}\")\n",
    "print(f\"Total dataset size: {len(all_data)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1700c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display dataset info\n",
    "print(\"Dataset Overview:\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSource distribution:\")\n",
    "print(df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 80/20 train-test split (stratified)\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    stratify=df['label'], \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"\\nTraining set label distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(f\"\\nTest set label distribution:\")\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c72be4",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Pre-trained GloVe Embeddings\n",
    "\n",
    "**GloVe (Global Vectors for Word Representation)**\n",
    "- Using `glove.6B.100d` (100-dimensional embeddings trained on 6B tokens)\n",
    "- In Kaggle, GloVe embeddings are available as a dataset\n",
    "- We'll load them into a dictionary: `{word: vector}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path, embedding_dim=100):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from file.\n",
    "    \n",
    "    Args:\n",
    "        glove_path: Path to GloVe file\n",
    "        embedding_dim: Dimension of embeddings (100, 200, 300, etc.)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping words to embedding vectors\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    \n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Configure GloVe path\n",
    "if IN_KAGGLE:\n",
    "    # In Kaggle, add GloVe as a dataset: https://www.kaggle.com/datasets/watts2/glove6b50dtxt\n",
    "    GLOVE_PATH = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt'\n",
    "else:\n",
    "    # For local testing, download from: https://nlp.stanford.edu/projects/glove/\n",
    "    # Place it in a known location\n",
    "    GLOVE_PATH = 'glove.6B.100d.txt'  # Update this path\n",
    "\n",
    "# Load embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "print(f\"Path: {GLOVE_PATH}\")\n",
    "\n",
    "try:\n",
    "    embeddings_dict = load_glove_embeddings(GLOVE_PATH, embedding_dim=100)\n",
    "    EMBEDDING_DIM = 100\n",
    "    print(f\"✓ Loaded {len(embeddings_dict)} word embeddings\")\n",
    "    print(f\"Embedding dimension: {EMBEDDING_DIM}\")\n",
    "    \n",
    "    # Show sample embeddings\n",
    "    sample_words = ['the', 'intelligence', 'artificial', 'human', 'text']\n",
    "    print(\"\\nSample embeddings:\")\n",
    "    for word in sample_words:\n",
    "        if word in embeddings_dict:\n",
    "            print(f\"  {word}: {embeddings_dict[word][:5]}... (showing first 5 dims)\")\n",
    "        else:\n",
    "            print(f\"  {word}: NOT FOUND\")\n",
    "            \n",
    "except FileNotFoundError:\n",
    "    print(\"⚠ GloVe file not found!\")\n",
    "    print(\"In Kaggle: Add 'GloVe' dataset to your notebook\")\n",
    "    print(\"Locally: Download from https://nlp.stanford.edu/projects/glove/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f03df9",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Text to Vector Conversion\n",
    "\n",
    "Convert each paragraph into a fixed-size vector by:\n",
    "1. Tokenizing the text (lowercase, whitespace split)\n",
    "2. Looking up each word in the GloVe embeddings\n",
    "3. Averaging all word vectors\n",
    "4. Handling out-of-vocabulary (OOV) words gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf20412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Simple tokenization: lowercase and split by whitespace.\n",
    "    Remove punctuation and special characters.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation (keep only alphanumeric and spaces)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Split into words\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "\n",
    "def text_to_vector(text, embeddings_dict, embedding_dim):\n",
    "    \"\"\"\n",
    "    Convert text to a single vector by averaging word embeddings.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        embeddings_dict: Dictionary of word embeddings\n",
    "        embedding_dim: Dimension of embeddings\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (embedding_dim,)\n",
    "    \"\"\"\n",
    "    words = tokenize(text)\n",
    "    \n",
    "    # Collect embeddings for known words\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in embeddings_dict:\n",
    "            word_vectors.append(embeddings_dict[word])\n",
    "    \n",
    "    # Average the vectors\n",
    "    if len(word_vectors) > 0:\n",
    "        avg_vector = np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        # No known words - return zero vector\n",
    "        avg_vector = np.zeros(embedding_dim)\n",
    "    \n",
    "    return avg_vector\n",
    "\n",
    "\n",
    "# Convert all texts to vectors\n",
    "print(\"Converting texts to vectors...\")\n",
    "\n",
    "X_train_vecs = np.array([\n",
    "    text_to_vector(text, embeddings_dict, EMBEDDING_DIM) \n",
    "    for text in train_df['text'].values\n",
    "])\n",
    "\n",
    "X_test_vecs = np.array([\n",
    "    text_to_vector(text, embeddings_dict, EMBEDDING_DIM) \n",
    "    for text in test_df['text'].values\n",
    "])\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print(f\"✓ Training vectors shape: {X_train_vecs.shape}\")\n",
    "print(f\"✓ Test vectors shape: {X_test_vecs.shape}\")\n",
    "print(f\"✓ Training labels shape: {y_train.shape}\")\n",
    "print(f\"✓ Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Check for any zero vectors (completely OOV paragraphs)\n",
    "train_zero_vecs = np.sum(np.all(X_train_vecs == 0, axis=1))\n",
    "test_zero_vecs = np.sum(np.all(X_test_vecs == 0, axis=1))\n",
    "print(f\"\\nZero vectors in training set: {train_zero_vecs}\")\n",
    "print(f\"Zero vectors in test set: {test_zero_vecs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3e8df",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. PyTorch Dataset Preparation\n",
    "\n",
    "Create PyTorch Dataset and DataLoader for efficient batch processing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a158ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_vecs)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test_vecs)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(f\"✓ Training batches: {len(train_loader)}\")\n",
    "print(f\"✓ Test batches: {len(test_loader)}\")\n",
    "print(f\"✓ Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb6e57b",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Feedforward Neural Network Model\n",
    "\n",
    "**Architecture:**\n",
    "- Input layer: 100 dimensions (GloVe embedding size)\n",
    "- Hidden layer 1: 64 neurons + ReLU + Dropout(0.3)\n",
    "- Hidden layer 2: 32 neurons + ReLU + Dropout(0.3)\n",
    "- Output layer: 1 neuron + Sigmoid (binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ee9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Feedforward Neural Network for binary classification.\n",
    "    Uses averaged word embeddings as input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim1=64, hidden_dim2=32, dropout_rate=0.3):\n",
    "        super(FeedforwardClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = FeedforwardClassifier(\n",
    "    input_dim=EMBEDDING_DIM,\n",
    "    hidden_dim1=64,\n",
    "    hidden_dim2=32,\n",
    "    dropout_rate=0.3\n",
    ").to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105a0fd",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training\n",
    "\n",
    "**Training Configuration:**\n",
    "- Loss function: Binary Cross Entropy (BCE)\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 0.001\n",
    "- Epochs: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ee484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        predictions = (outputs > 0.5).float()\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1:2d}/{NUM_EPOCHS}] | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a364db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax1.plot(range(1, NUM_EPOCHS + 1), train_losses, marker='o', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss over Epochs')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "ax2.plot(range(1, NUM_EPOCHS + 1), train_accuracies, marker='o', linewidth=2, color='green')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training Accuracy over Epochs')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61009a64",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Evaluation\n",
    "\n",
    "Evaluate the model on the test set and compute comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model and return predictions and probabilities.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs).squeeze()\n",
    "            probabilities = outputs.cpu().numpy()\n",
    "            predictions = (outputs > 0.5).float().cpu().numpy()\n",
    "            \n",
    "            all_predictions.extend(predictions)\n",
    "            all_probabilities.extend(probabilities)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_labels), np.array(all_predictions), np.array(all_probabilities)\n",
    "\n",
    "\n",
    "# Get predictions on test set\n",
    "y_true, y_pred, y_prob = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "roc_auc = roc_auc_score(y_true, y_prob)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST SET EVALUATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Human', 'AI'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d12f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Human', 'AI'], \n",
    "            yticklabels=['Human', 'AI'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics from confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (Human → Human):  {tn}\")\n",
    "print(f\"  False Positives (Human → AI):    {fp}\")\n",
    "print(f\"  False Negatives (AI → Human):    {fn}\")\n",
    "print(f\"  True Positives (AI → AI):        {tp}\")\n",
    "print(f\"\\nSpecificity (True Negative Rate): {tn/(tn+fp):.4f}\")\n",
    "print(f\"Sensitivity (True Positive Rate): {tp/(tp+fn):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c3b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0a8ab",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Analysis & Interpretation\n",
    "\n",
    "### What Semantic Information Do Averaged Embeddings Capture?\n",
    "\n",
    "Averaged word embeddings (like GloVe) capture **distributional semantic information** at the word level. Each word is represented as a vector in a high-dimensional space where words with similar meanings or contexts are positioned closer together. When we average these vectors across all words in a paragraph, we obtain:\n",
    "\n",
    "1. **Topic/Theme Information**: The averaged vector represents the overall semantic \"center of mass\" of the paragraph, capturing the dominant topics and themes discussed.\n",
    "\n",
    "2. **Lexical Composition**: The vocabulary choices are preserved—formal vs. informal language, technical vs. common terminology, abstract vs. concrete concepts.\n",
    "\n",
    "3. **Semantic Similarity**: Paragraphs with similar meanings will have similar averaged vectors, regardless of exact word order or syntax.\n",
    "\n",
    "4. **Word-level Patterns**: Common word associations and collocations are implicitly captured since pre-trained embeddings encode co-occurrence statistics from large corpora.\n",
    "\n",
    "**However, averaged embeddings lose:**\n",
    "- **Word order information**: \"The dog bit the man\" vs. \"The man bit the dog\" would have identical representations.\n",
    "- **Syntactic structure**: Complex grammatical patterns, sentence structure, and dependencies are completely discarded.\n",
    "- **Context-dependent meanings**: Polysemous words (e.g., \"bank\" as financial institution vs. river bank) receive the same embedding regardless of context.\n",
    "- **Long-range dependencies**: Relationships between distant words in the text are not modeled.\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Model May Perform Better Than Surface Statistics But Worse Than Transformers\n",
    "\n",
    "**Better than Surface Statistics (Tier A):**\n",
    "\n",
    "Surface-level features like average word length, punctuation density, or simple n-gram frequencies can be easily mimicked by modern language models. Averaged embeddings go deeper by:\n",
    "\n",
    "- **Semantic Understanding**: Capturing meaning rather than just form. Two texts can use different words but similar concepts, and embeddings will recognize this similarity.\n",
    "- **Vocabulary Richness**: Distinguishing between rich, varied vocabulary (typical of human literature) vs. repetitive or limited word choices (potential AI characteristic).\n",
    "- **Generalization**: Pre-trained embeddings generalize across domains because they're trained on massive corpora, unlike simple statistical features that may be dataset-specific.\n",
    "\n",
    "**Worse than Transformers (Tier C):**\n",
    "\n",
    "Transformer-based models (BERT, GPT, RoBERTa) vastly outperform averaged embeddings because they:\n",
    "\n",
    "1. **Context-Aware Representations**: Transformers generate contextual embeddings where each word's representation depends on the surrounding words, capturing nuanced meanings.\n",
    "\n",
    "2. **Sequential Modeling**: Self-attention mechanisms in transformers model complex relationships between all words in the text, preserving word order and long-range dependencies.\n",
    "\n",
    "3. **Hierarchical Features**: Transformers learn multiple layers of abstraction, from syntactic patterns to high-level semantic understanding, through their deep architecture.\n",
    "\n",
    "4. **Fine-tuning Capability**: Transformers can be fine-tuned on specific tasks, adapting their representations to the nuances of AI vs. human text detection.\n",
    "\n",
    "5. **Stylistic Patterns**: Transformers can detect subtle stylistic signatures like sentence rhythm, coherence patterns, and discourse structure that averaged embeddings completely miss.\n",
    "\n",
    "---\n",
    "\n",
    "### Strengths and Limitations of Averaged Embeddings for AI-Text Detection\n",
    "\n",
    "**Strengths:**\n",
    "\n",
    "1. **Simplicity & Interpretability**: The model is straightforward to understand and debug. We can examine which words contribute most to the classification.\n",
    "\n",
    "2. **Efficiency**: Training is fast and requires minimal computational resources compared to transformers. Inference is nearly instantaneous.\n",
    "\n",
    "3. **Low Data Requirements**: Feedforward networks with averaged embeddings can work reasonably well even with limited training data (thousands vs. millions of examples).\n",
    "\n",
    "4. **No Pre-training Needed**: We leverage pre-trained GloVe embeddings without additional fine-tuning, making the approach accessible and reproducible.\n",
    "\n",
    "5. **Semantic Baseline**: Provides a strong baseline that captures meaningful semantic differences between AI and human text beyond superficial statistics.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "1. **Loss of Syntax**: Cannot detect grammatical errors, unnatural sentence structures, or syntactic patterns that might distinguish AI text.\n",
    "\n",
    "2. **Insensitivity to Order**: \"The enemy destroyed the fortress\" and \"The fortress destroyed the enemy\" are indistinguishable—a critical flaw for coherence detection.\n",
    "\n",
    "3. **Fixed Representations**: Word embeddings are static; the same word always has the same representation regardless of context.\n",
    "\n",
    "4. **Averaging Artifacts**: Important words (like negations \"not\") can be drowned out when averaging hundreds of words, potentially reversing the intended meaning.\n",
    "\n",
    "5. **Brittleness to Adversarial Attacks**: AI text generators could easily evade detection by simply adjusting vocabulary distributions to mimic human word choices while maintaining their characteristic syntactic patterns.\n",
    "\n",
    "6. **Domain Dependence**: Performance may degrade when applied to domains different from both the GloVe training corpus and our novel-based dataset.\n",
    "\n",
    "7. **No Stylistic Modeling**: Cannot capture coherence, logical flow, rhetorical devices, or the \"human touch\" in creative writing that transformers might detect through attention patterns.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Averaged embeddings with a feedforward neural network represent a reasonable middle ground for AI text detection—better than purely surface-level features but fundamentally limited by the loss of sequential and contextual information. This approach serves as a strong baseline and is useful for scenarios where computational resources are constrained or interpretability is prioritized over maximum accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
