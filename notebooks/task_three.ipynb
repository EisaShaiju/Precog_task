{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f63f4c",
   "metadata": {},
   "source": [
    "# Task 3: Saliency Mapping & Model Interpretability\n",
    "## Interpreting DistilBERT + LoRA AI Text Classifier\n",
    "\n",
    "**Objective**: Perform post-hoc interpretability analysis on the fine-tuned DistilBERT model to understand which tokens/features drive AI vs Human classification decisions.\n",
    "\n",
    "**Methods**:\n",
    "- **Captum Integrated Gradients** - Token-level attribution analysis\n",
    "- **HTML Heatmap Visualization** - Color-coded importance scores\n",
    "- **Error Analysis** - Examining misclassified or borderline cases\n",
    "\n",
    "**Key Questions**:\n",
    "1. Does the model rely on specific lexical markers (\"AI-isms\")?\n",
    "2. Does it capture broader structural patterns (rhythm, coherence)?\n",
    "3. What causes false positives (Human → AI)?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8baa5",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c423bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers & PEFT\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "# Captum for interpretability\n",
    "from captum.attr import IntegratedGradients, LayerIntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e2361",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Fine-Tuned DistilBERT + LoRA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "MODEL_PATH = Path('../output/tier_c_models/tier_c_lora_model')\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Load model (handles LoRA automatically if saved with PEFT)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=2\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"✓ Model loaded successfully\")\n",
    "print(f\"✓ Tokenizer vocab size: {len(tokenizer)}\")\n",
    "print(f\"✓ Model has {model.config.num_labels} classes (0=Human, 1=AI)\")\n",
    "print(f\"✓ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db85124",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Test Samples\n",
    "\n",
    "Load some AI and Human samples from the test set for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49556ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample AI-generated texts\n",
    "AI_SAMPLES_PATH = Path('../output/class2')\n",
    "\n",
    "sample_ai_texts = []\n",
    "for jsonl_file in AI_SAMPLES_PATH.glob('*.jsonl'):\n",
    "    with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                text = entry.get('text') or entry.get('paragraph') or entry.get('content', '')\n",
    "                if text and len(text.split()) >= 50:\n",
    "                    sample_ai_texts.append(text)\n",
    "                    if len(sample_ai_texts) >= 10:\n",
    "                        break\n",
    "            except:\n",
    "                continue\n",
    "    if len(sample_ai_texts) >= 10:\n",
    "        break\n",
    "\n",
    "# Load sample Human-written texts\n",
    "HUMAN_SAMPLES_PATH = Path('../output/class1')\n",
    "\n",
    "sample_human_texts = []\n",
    "for txt_file in HUMAN_SAMPLES_PATH.glob('*_cleaned.txt'):\n",
    "    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        # Chunk into paragraphs\n",
    "        words = text.split()\n",
    "        for i in range(0, min(len(words), 2000), 200):  # Sample first 10 chunks\n",
    "            chunk = ' '.join(words[i:i+200])\n",
    "            if len(chunk.split()) >= 50:\n",
    "                sample_human_texts.append(chunk)\n",
    "                if len(sample_human_texts) >= 10:\n",
    "                    break\n",
    "    if len(sample_human_texts) >= 10:\n",
    "        break\n",
    "\n",
    "print(f\"✓ Loaded {len(sample_ai_texts)} AI-generated samples\")\n",
    "print(f\"✓ Loaded {len(sample_human_texts)} Human-written samples\")\n",
    "\n",
    "# Preview samples\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SAMPLE AI TEXT (first 300 chars):\")\n",
    "print(sample_ai_texts[0][:300] + \"...\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SAMPLE HUMAN TEXT (first 300 chars):\")\n",
    "print(sample_human_texts[0][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540804d",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Integrated Gradients Attribution Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_confidence(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Get model prediction and confidence for a text sample.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (prediction_class, confidence, probabilities)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=MAX_LENGTH, padding='max_length')\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        prediction = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0, prediction].item()\n",
    "    \n",
    "    return prediction, confidence, probs[0].cpu().numpy()\n",
    "\n",
    "\n",
    "def compute_attributions(text, model, tokenizer, device, target_class=1, n_steps=50):\n",
    "    \"\"\"\n",
    "    Compute token-level attributions using Integrated Gradients.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to analyze\n",
    "        model: Fine-tuned model\n",
    "        tokenizer: Tokenizer\n",
    "        device: torch device\n",
    "        target_class: Class to explain (0=Human, 1=AI)\n",
    "        n_steps: Number of integration steps\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (tokens, attributions, prediction, confidence)\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=MAX_LENGTH, padding='max_length')\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    prediction, confidence, probs = predict_with_confidence(text, model, tokenizer, device)\n",
    "    \n",
    "    # Define forward function that returns target class logit\n",
    "    def forward_func(input_ids, attention_mask):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Return logit for target class\n",
    "        return outputs.logits[:, target_class]\n",
    "    \n",
    "    # Initialize Integrated Gradients\n",
    "    ig = IntegratedGradients(forward_func)\n",
    "    \n",
    "    # Baseline: all PAD tokens (ID 0 in DistilBERT)\n",
    "    baseline_ids = torch.zeros_like(input_ids)\n",
    "    \n",
    "    # Compute attributions\n",
    "    attributions, delta = ig.attribute(\n",
    "        inputs=input_ids,\n",
    "        baselines=baseline_ids,\n",
    "        additional_forward_args=(attention_mask,),\n",
    "        return_convergence_delta=True,\n",
    "        n_steps=n_steps\n",
    "    )\n",
    "    \n",
    "    # Sum attributions (they are per-embedding dimension)\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "    \n",
    "    return tokens, attributions, prediction, confidence, delta\n",
    "\n",
    "\n",
    "def aggregate_subword_attributions(tokens, attributions):\n",
    "    \"\"\"\n",
    "    Aggregate WordPiece subword tokens to word-level attributions.\n",
    "    \n",
    "    DistilBERT uses ## prefix for subword continuations.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: [(word, attribution_score), ...]\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    word_scores = []\n",
    "    current_word = \"\"\n",
    "    current_score = 0\n",
    "    \n",
    "    for token, score in zip(tokens, attributions):\n",
    "        # Skip special tokens\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "        \n",
    "        # Check if subword continuation\n",
    "        if token.startswith('##'):\n",
    "            current_word += token[2:]\n",
    "            current_score += score\n",
    "        else:\n",
    "            # Save previous word\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "                word_scores.append(current_score)\n",
    "            # Start new word\n",
    "            current_word = token\n",
    "            current_score = score\n",
    "    \n",
    "    # Add final word\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "        word_scores.append(current_score)\n",
    "    \n",
    "    return list(zip(words, word_scores))\n",
    "\n",
    "\n",
    "print(\"✓ Attribution functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f27a3",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. HTML Heatmap Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b9880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_heatmap(words, scores, prediction, confidence, title=\"Saliency Map\"):\n",
    "    \"\"\"\n",
    "    Create an HTML heatmap visualization.\n",
    "    \n",
    "    Positive scores (red) = push toward AI\n",
    "    Negative scores (blue) = push toward Human\n",
    "    \"\"\"\n",
    "    # Normalize scores for coloring\n",
    "    max_abs_score = max(abs(min(scores)), abs(max(scores)))\n",
    "    if max_abs_score == 0:\n",
    "        max_abs_score = 1\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; padding: 20px; background-color: #f5f5f5; border-radius: 10px;\">\n",
    "        <h3 style=\"color: #333;\">{title}</h3>\n",
    "        <p style=\"margin: 10px 0;\">\n",
    "            <strong>Prediction:</strong> <span style=\"color: {'#d32f2f' if prediction == 1 else '#1976d2'};\">\n",
    "                {'AI-generated' if prediction == 1 else 'Human-written'}\n",
    "            </span> \n",
    "            ({confidence:.1%} confidence)\n",
    "        </p>\n",
    "        <div style=\"margin-top: 15px; line-height: 2.2; font-size: 16px;\">\n",
    "    \"\"\"\n",
    "    \n",
    "    for word, score in zip(words, scores):\n",
    "        # Normalize score to [-1, 1]\n",
    "        norm_score = score / max_abs_score\n",
    "        \n",
    "        # Color mapping\n",
    "        if norm_score > 0:\n",
    "            # Red for AI\n",
    "            intensity = min(int(norm_score * 200 + 55), 255)\n",
    "            bg_color = f\"rgb({intensity}, {255-intensity}, {255-intensity})\"\n",
    "        else:\n",
    "            # Blue for Human\n",
    "            intensity = min(int(abs(norm_score) * 200 + 55), 255)\n",
    "            bg_color = f\"rgb({255-intensity}, {255-intensity}, {intensity})\"\n",
    "        \n",
    "        # Determine text color for readability\n",
    "        text_color = \"#000\" if abs(norm_score) < 0.5 else \"#fff\"\n",
    "        \n",
    "        html += f'''\n",
    "            <span style=\"\n",
    "                background-color: {bg_color};\n",
    "                color: {text_color};\n",
    "                padding: 3px 6px;\n",
    "                margin: 2px;\n",
    "                border-radius: 4px;\n",
    "                display: inline-block;\n",
    "                font-weight: {'bold' if abs(norm_score) > 0.3 else 'normal'};\n",
    "            \" title=\"Score: {score:.4f}\">\n",
    "                {word}\n",
    "            </span>\n",
    "        '''\n",
    "    \n",
    "    html += \"\"\"\n",
    "        </div>\n",
    "        <div style=\"margin-top: 20px; font-size: 12px; color: #666;\">\n",
    "            <strong>Legend:</strong>\n",
    "            <span style=\"background-color: #ffcccc; padding: 2px 8px; margin: 0 5px; border-radius: 3px;\">Red = AI signal</span>\n",
    "            <span style=\"background-color: #ccccff; padding: 2px 8px; margin: 0 5px; border-radius: 3px;\">Blue = Human signal</span>\n",
    "            <span style=\"color: #999; margin-left: 10px;\">(Hover over words for exact scores)</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "\n",
    "print(\"✓ Visualization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9d8d3",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Analysis: AI-Generated Text Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f191228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze AI-generated sample\n",
    "ai_text = sample_ai_texts[0]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANALYZING AI-GENERATED TEXT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nText:\\n{ai_text[:400]}...\\n\")\n",
    "\n",
    "# Compute attributions\n",
    "tokens, attrs, pred, conf, delta = compute_attributions(\n",
    "    ai_text, model, tokenizer, device, target_class=1\n",
    ")\n",
    "\n",
    "# Aggregate to words\n",
    "word_attrs = aggregate_subword_attributions(tokens, attrs)\n",
    "words, scores = zip(*word_attrs)\n",
    "\n",
    "print(f\"Prediction: {'AI' if pred == 1 else 'Human'} ({conf:.2%} confidence)\")\n",
    "print(f\"Convergence delta: {delta.item():.6f}\")\n",
    "print(f\"\\nTop 10 words pushing toward AI:\")\n",
    "sorted_words = sorted(word_attrs, key=lambda x: x[1], reverse=True)\n",
    "for i, (w, s) in enumerate(sorted_words[:10], 1):\n",
    "    print(f\"  {i:2d}. {w:20s} {s:+.4f}\")\n",
    "\n",
    "# Visualize\n",
    "html_viz = create_html_heatmap(\n",
    "    words, scores, pred, conf,\n",
    "    title=\"Saliency Map: AI-Generated Text\"\n",
    ")\n",
    "display(HTML(html_viz))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a718805",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Analysis: Human-Written Text Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Human-written sample\n",
    "human_text = sample_human_texts[0]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANALYZING HUMAN-WRITTEN TEXT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nText:\\n{human_text[:400]}...\\n\")\n",
    "\n",
    "# Compute attributions (still target AI class to see what prevents AI classification)\n",
    "tokens, attrs, pred, conf, delta = compute_attributions(\n",
    "    human_text, model, tokenizer, device, target_class=1\n",
    ")\n",
    "\n",
    "# Aggregate to words\n",
    "word_attrs = aggregate_subword_attributions(tokens, attrs)\n",
    "words, scores = zip(*word_attrs)\n",
    "\n",
    "print(f\"Prediction: {'AI' if pred == 1 else 'Human'} ({conf:.2%} confidence)\")\n",
    "print(f\"Convergence delta: {delta.item():.6f}\")\n",
    "print(f\"\\nTop 10 words pushing AWAY from AI (toward Human):\")\n",
    "sorted_words = sorted(word_attrs, key=lambda x: x[1])\n",
    "for i, (w, s) in enumerate(sorted_words[:10], 1):\n",
    "    print(f\"  {i:2d}. {w:20s} {s:+.4f}\")\n",
    "\n",
    "# Visualize\n",
    "html_viz = create_html_heatmap(\n",
    "    words, scores, pred, conf,\n",
    "    title=\"Saliency Map: Human-Written Text\"\n",
    ")\n",
    "display(HTML(html_viz))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c26e5",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Find Borderline/Misclassified Cases\n",
    "\n",
    "Let's find human texts that are classified as AI or have low confidence, for error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find borderline or misclassified human samples\n",
    "print(\"Searching for borderline Human samples (predicted as AI or low confidence)...\\n\")\n",
    "\n",
    "borderline_cases = []\n",
    "\n",
    "for i, text in enumerate(sample_human_texts):\n",
    "    pred, conf, probs = predict_with_confidence(text, model, tokenizer, device)\n",
    "    \n",
    "    # Look for:\n",
    "    # 1. Misclassified (Human predicted as AI)\n",
    "    # 2. Low confidence correct predictions\n",
    "    # 3. High AI probability even if correctly classified\n",
    "    \n",
    "    ai_prob = probs[1]  # Probability of AI class\n",
    "    \n",
    "    if pred == 1 or conf < 0.7 or ai_prob > 0.3:\n",
    "        borderline_cases.append({\n",
    "            'text': text,\n",
    "            'prediction': pred,\n",
    "            'confidence': conf,\n",
    "            'ai_probability': ai_prob,\n",
    "            'type': 'MISCLASSIFIED' if pred == 1 else 'BORDERLINE'\n",
    "        })\n",
    "    \n",
    "    if len(borderline_cases) >= 5:\n",
    "        break\n",
    "\n",
    "print(f\"Found {len(borderline_cases)} borderline/misclassified cases\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, case in enumerate(borderline_cases[:3], 1):  # Show top 3\n",
    "    print(f\"\\nCase {i} - {case['type']}\")\n",
    "    print(f\"Prediction: {'AI' if case['prediction'] == 1 else 'Human'}\")\n",
    "    print(f\"Confidence: {case['confidence']:.2%}\")\n",
    "    print(f\"AI Probability: {case['ai_probability']:.2%}\")\n",
    "    print(f\"Text preview: {case['text'][:200]}...\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741025aa",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Error Analysis: Detailed Investigation of 3 Cases\n",
    "\n",
    "Now we'll perform detailed saliency analysis on 3 borderline/misclassified cases, looking for:\n",
    "1. **Lexical repetition** - Repeated words or phrases\n",
    "2. **Stylistic regularity** - Overly formal or structured language\n",
    "3. **AI-isms** - Words like \"tapestry\", \"delve\", \"testament\", \"furthermore\", \"consequently\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
