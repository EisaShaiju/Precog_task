{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14756256,"sourceType":"datasetVersion","datasetId":9431569}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"85eea3a2","cell_type":"markdown","source":"# Task 4: The Turing Test\n## Genetic Algorithm Attack on AI Text Detector\n\n**Objective**: Use evolutionary algorithms to evolve AI-generated text that bypasses the detector.\n\n**Parts**:\n1. **Super-Imposter**: GA-based adversarial attack (5-10 generations)\n2. **Personal Test**: Analyze user-provided text and attempt manual/automated evasion\n\n**Key Constraints**:\n- No retraining of the detector\n- Simple, interpretable GA implementation\n- LLM-as-Mutator approach\n\n---","metadata":{}},{"id":"6ad435da","cell_type":"markdown","source":"## 1. Setup & Imports","metadata":{}},{"id":"94048c42","cell_type":"code","source":"# Core imports\nimport json\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom typing import List, Tuple, Dict\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Google Gemini (optional - fallback to mock if unavailable)\ntry:\n    import google.generativeai as genai\n    GEMINI_AVAILABLE = True\nexcept:\n    GEMINI_AVAILABLE = False\n    print(\"  Google Gemini needs to be set\")\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nMAX_LENGTH = 256\n\n# Plotting style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(f\"✓ Imports complete\")\nprint(f\"Device: {device}\")\nprint(f\"Gemini available: {GEMINI_AVAILABLE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:07:53.096982Z","iopub.execute_input":"2026-02-07T12:07:53.097961Z","iopub.status.idle":"2026-02-07T12:08:08.851354Z","shell.execute_reply.started":"2026-02-07T12:07:53.097912Z","shell.execute_reply":"2026-02-07T12:08:08.850011Z"}},"outputs":[{"name":"stdout","text":"✓ Imports complete\nDevice: cpu\nGemini available: True\n","output_type":"stream"}],"execution_count":2},{"id":"6357c1f0","cell_type":"markdown","source":"---\n## 2. Load Trained Detector Model","metadata":{}},{"id":"9ebda37d","cell_type":"code","source":"import os\n\nIN_KAGGLE = os.path.exists('/kaggle/input')\n\nif IN_KAGGLE:\n    MODEL_PATH = Path('/kaggle/input/tier-c-lora-model/tier_c_lora_model')\nelse:\n    MODEL_PATH = Path('../output/tier_c_models/tier_c_lora_model')\n\nprint(f\"Loading model from: {MODEL_PATH}\")\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_PATH,\n    num_labels=2\n)\nmodel.to(device)\nmodel.eval()\n\nprint(\"✓ Detector model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:08:08.853202Z","iopub.execute_input":"2026-02-07T12:08:08.853953Z","iopub.status.idle":"2026-02-07T12:08:34.212946Z","shell.execute_reply.started":"2026-02-07T12:08:08.853920Z","shell.execute_reply":"2026-02-07T12:08:34.211480Z"}},"outputs":[{"name":"stdout","text":"Loading model from: /kaggle/input/tier-c-lora-model/tier_c_lora_model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5943332beeab4e8a91c99ba75c8a6ded"}},"metadata":{}},{"name":"stderr","text":"2026-02-07 12:08:12.625096: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770466092.887072      94 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770466092.963688      94 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770466093.605309      94 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770466093.605391      94 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770466093.605395      94 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770466093.605398      94 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e002a94177f4158a8c646e4556bb742"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"✓ Detector model loaded successfully!\n","output_type":"stream"}],"execution_count":3},{"id":"aa772db2","cell_type":"markdown","source":"---\n## 3. Detector Interface Function","metadata":{}},{"id":"c6f85d4d","cell_type":"code","source":"def detect_text(text: str) -> Dict[str, float]:\n    \"\"\"\n    Classify text as Human or AI-generated.\n    \n    Args:\n        text: Input paragraph\n    \n    Returns:\n        Dict with 'human_prob', 'ai_prob', 'prediction' (0=Human, 1=AI)\n    \"\"\"\n    inputs = tokenizer(\n        text,\n        return_tensors='pt',\n        truncation=True,\n        max_length=MAX_LENGTH,\n        padding='max_length'\n    )\n    \n    input_ids = inputs['input_ids'].to(device)\n    attention_mask = inputs['attention_mask'].to(device)\n    \n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n    \n    return {\n        'human_prob': float(probs[0]),\n        'ai_prob': float(probs[1]),\n        'prediction': int(np.argmax(probs)),\n        'label': 'Human' if np.argmax(probs) == 0 else 'AI'\n    }\n\n# Test the detector\ntest_text = \"The old man walked slowly down the street, his cane tapping rhythmically.\"\nresult = detect_text(test_text)\nprint(f\"Test detection:\")\nprint(f\"  Human probability: {result['human_prob']:.2%}\")\nprint(f\"  AI probability: {result['ai_prob']:.2%}\")\nprint(f\"  Prediction: {result['label']}\")\nprint(\"\\n✓ Detector interface ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:08:34.214343Z","iopub.execute_input":"2026-02-07T12:08:34.215142Z","iopub.status.idle":"2026-02-07T12:08:34.562524Z","shell.execute_reply.started":"2026-02-07T12:08:34.215110Z","shell.execute_reply":"2026-02-07T12:08:34.561325Z"}},"outputs":[{"name":"stdout","text":"Test detection:\n  Human probability: 99.75%\n  AI probability: 0.25%\n  Prediction: Human\n\n✓ Detector interface ready\n","output_type":"stream"}],"execution_count":4},{"id":"52e3f517","cell_type":"markdown","source":"---\n## 4. LLM Generation Interface (with Mock Fallback)","metadata":{}},{"id":"9ef7ec67","cell_type":"code","source":"import os\nimport google.generativeai as genai\nfrom kaggle_secrets import UserSecretsClient\n\n# Read API key from Kaggle Secrets - THE CORRECT WAY\nuser_secrets = UserSecretsClient()\ntry:\n    GEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n    GEMINI_AVAILABLE = True\nexcept:\n    GEMINI_API_KEY = None\n    GEMINI_AVAILABLE = False\n\nif GEMINI_AVAILABLE:\n    genai.configure(api_key=GEMINI_API_KEY)\n    gemini_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n    print(\"✓ Gemini API detected and configured\")\nelse:\n    gemini_model = None\n    print(\"⚠ Gemini API key not found - using mock generation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:10:10.211073Z","iopub.execute_input":"2026-02-07T12:10:10.211473Z","iopub.status.idle":"2026-02-07T12:10:10.340632Z","shell.execute_reply.started":"2026-02-07T12:10:10.211444Z","shell.execute_reply":"2026-02-07T12:10:10.339816Z"}},"outputs":[{"name":"stdout","text":"✓ Gemini API detected and configured\n","output_type":"stream"}],"execution_count":13},{"id":"b45a7234","cell_type":"code","source":"def generate_text(prompt: str, use_mock: bool = False) -> str:\n    \"\"\"\n    Generate text using Gemini or mock generation.\n    \n    Args:\n        prompt: Generation prompt\n        use_mock: If True, use mock generation (for demo purposes)\n    \n    Returns:\n        Generated text\n    \"\"\"\n      \n    # if 'rewrite' in prompt.lower() or 'alter' in prompt.lower():\n    #     # Return a slightly modified version\n    #     base_idx = hash(prompt) % len(mock_responses['initial'])\n    #     base_text = mock_responses['initial'][base_idx]\n    #     # Simulate mutation by swapping words\n    #     words = base_text.split()\n    #     if len(words) > 10:\n    #         # Swap a few words\n    #         idx1, idx2 = hash(prompt) % (len(words)-1), (hash(prompt)+1) % (len(words)-1)\n    #         words[idx1], words[idx2] = words[idx2], words[idx1]\n    #     return ' '.join(words)\n    # else:\n    #     # Initial generation\n    #     idx = hash(prompt) % len(mock_responses['initial'])\n    #     return mock_responses['initial'][idx]\n    # else:\n    #     # Real Gemini generation\n    try:\n        response = gemini_model.generate_content(prompt)\n        return response.text\n    except Exception as e:\n        print(f\"Gemini error: {e}\")\n        return \"Error generating text\"\n\n# Test generation\ntest_gen = generate_text(\"Write a paragraph about AI\")\nprint(f\"Test generation (first 150 chars):\\n{test_gen[:150]}...\\n\")\nprint(\"✓ Text generation ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:08:34.675140Z","iopub.execute_input":"2026-02-07T12:08:34.675542Z","iopub.status.idle":"2026-02-07T12:08:36.962363Z","shell.execute_reply.started":"2026-02-07T12:08:34.675511Z","shell.execute_reply":"2026-02-07T12:08:36.960967Z"}},"outputs":[{"name":"stdout","text":"Test generation (first 150 chars):\nArtificial intelligence (AI) is a rapidly evolving field of computer science focused on creating systems capable of performing tasks that typically re...\n\n✓ Text generation ready\n","output_type":"stream"}],"execution_count":6},{"id":"32304735","cell_type":"markdown","source":"---\n## 5. Genetic Algorithm Implementation","metadata":{}},{"id":"ae194ab2","cell_type":"markdown","source":"### 5.1 GA Helper Functions","metadata":{}},{"id":"f3b35c53","cell_type":"code","source":"def evaluate_population(population: List[str]) -> List[Dict]:\n    \"\"\"\n    Evaluate fitness of all individuals in population.\n    \n    Fitness = Human probability (higher is better for evasion)\n    \n    Args:\n        population: List of text paragraphs\n    \n    Returns:\n        List of dicts with text, scores, and fitness\n    \"\"\"\n    evaluated = []\n    \n    for text in population:\n        result = detect_text(text)\n        evaluated.append({\n            'text': text,\n            'fitness': result['human_prob'],  # Fitness = Human probability\n            'human_prob': result['human_prob'],\n            'ai_prob': result['ai_prob'],\n            'label': result['label']\n        })\n    \n    # Sort by fitness (descending)\n    evaluated.sort(key=lambda x: x['fitness'], reverse=True)\n    \n    return evaluated\n\n\ndef select_top_k(evaluated_population: List[Dict], k: int = 3) -> List[str]:\n    \"\"\"\n    Select top k individuals by fitness.\n    \n    Args:\n        evaluated_population: List of evaluated individuals\n        k: Number to select\n    \n    Returns:\n        List of top k text strings\n    \"\"\"\n    return [ind['text'] for ind in evaluated_population[:k]]\n\n\ndef mutate_population(parents: List[str], target_size: int = 10) -> List[str]:\n    \"\"\"\n    Generate mutated variants using LLM-as-Mutator.\n    \n    Mutation strategies:\n    1. Alter sentence rhythm\n    2. Introduce grammatical irregularity\n    3. Reduce polish / add inconsistency\n    \n    Args:\n        parents: List of parent texts\n        target_size: Desired population size\n    \n    Returns:\n        New population (parents + mutated children)\n    \"\"\"\n    mutation_prompts = [\n        \"Rewrite this paragraph to alter sentence rhythm while keeping vocabulary mostly intact: {}\",\n        \"Introduce a subtle grammatical irregularity or rare/archaic phrasing into this paragraph: {}\",\n        \"Reduce polish and introduce slight inconsistency in tone for this paragraph: {}\",\n        \"Rewrite with more natural, conversational flow: {}\",\n        \"Add subtle imperfections and vary sentence structure: {}\"\n    ]\n    \n    new_population = parents.copy()  # Keep parents (elitism)\n    \n    mutations_needed = target_size - len(parents)\n    \n    for i in range(mutations_needed):\n        # Select random parent\n        parent = parents[i % len(parents)]\n        \n        # Select random mutation strategy\n        prompt_template = mutation_prompts[i % len(mutation_prompts)]\n        prompt = prompt_template.format(parent)\n        \n        # Generate mutated variant\n        mutated = generate_text(prompt)\n        new_population.append(mutated)\n    \n    return new_population\n\n\nprint(\" GA functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:08:36.963838Z","iopub.execute_input":"2026-02-07T12:08:36.964251Z","iopub.status.idle":"2026-02-07T12:08:36.977353Z","shell.execute_reply.started":"2026-02-07T12:08:36.964208Z","shell.execute_reply":"2026-02-07T12:08:36.975767Z"}},"outputs":[{"name":"stdout","text":" GA functions defined\n","output_type":"stream"}],"execution_count":7},{"id":"9b20ed66","cell_type":"markdown","source":"### 5.2 Main GA Loop","metadata":{}},{"id":"0aaa5f51","cell_type":"code","source":"def run_genetic_algorithm_with_checkpointing(\n    initial_population: List[str],\n    num_generations: int = 10,\n    population_size: int = 10,\n    selection_size: int = 3,\n    target_fitness: float = 0.90,\n    verbose: bool = True,\n    checkpoint_file: str = 'ga_checkpoint.pkl',\n    resume_from_checkpoint: bool = False\n) -> Dict:\n    \"\"\"\n    Run genetic algorithm with automatic checkpointing.\n    Can resume from checkpoint after rate limit errors.\n    \n    Args:\n        initial_population: Starting population\n        num_generations: Maximum generations\n        population_size: Size of population\n        selection_size: Number of top individuals to keep\n        target_fitness: Stop if best fitness exceeds this\n        verbose: Print progress\n        checkpoint_file: Where to save progress\n        resume_from_checkpoint: If True, load from checkpoint\n    \n    Returns:\n        Dict with history, best individual, etc.\n    \"\"\"\n    \n    # Try to resume from checkpoint\n    if resume_from_checkpoint:\n        try:\n            with open(checkpoint_file, 'rb') as f:\n                checkpoint = pickle.load(f)\n            \n            population = checkpoint['population']\n            history = checkpoint['history']\n            start_gen = checkpoint['last_generation'] + 1\n            \n            if verbose:\n                print(f\" RESUMING FROM CHECKPOINT\")\n                print(f\"   Last completed generation: {checkpoint['last_generation']}\")\n                print(f\"   Continuing from generation: {start_gen}\")\n                print(f\"   Population size: {len(population)}\\n\")\n        except FileNotFoundError:\n            if verbose:\n                print(f\"  No checkpoint found. Starting fresh.\\n\")\n            resume_from_checkpoint = False\n    \n    # Initialize fresh if not resuming\n    if not resume_from_checkpoint:\n        population = initial_population[:population_size]\n        history = {\n            'best_fitness': [],\n            'avg_fitness': [],\n            'worst_fitness': [],\n            'best_individual': [],\n            'generation': []\n        }\n        start_gen = 0\n    \n    if verbose and start_gen == 0:\n        print(\"=\"*80)\n        print(\"GENETIC ALGORITHM: SUPER-IMPOSTER EVOLUTION\")\n        print(\"=\"*80)\n        print(f\"Population size: {population_size}\")\n        print(f\"Selection size: {selection_size}\")\n        print(f\"Max generations: {num_generations}\")\n        print(f\"Target fitness (Human prob): {target_fitness:.1%}\\n\")\n    \n    # Main GA loop\n    for gen in range(start_gen, num_generations):\n        if verbose:\n            print(f\"\\n{'─'*80}\")\n            print(f\"GENERATION {gen + 1}/{num_generations}\")\n            print(f\"{'─'*80}\")\n        \n        try:\n            # Evaluate population\n            evaluated = evaluate_population(population)\n            \n            # Track metrics\n            best = evaluated[0]\n            avg_fit = np.mean([ind['fitness'] for ind in evaluated])\n            worst_fit = evaluated[-1]['fitness']\n            \n            history['best_fitness'].append(best['fitness'])\n            history['avg_fitness'].append(avg_fit)\n            history['worst_fitness'].append(worst_fit)\n            history['best_individual'].append(best['text'])\n            history['generation'].append(gen + 1)\n            \n            if verbose:\n                print(f\"\\nFitness Statistics:\")\n                print(f\"  Best:  {best['fitness']:.2%} (Human prob) - Label: {best['label']}\")\n                print(f\"  Avg:   {avg_fit:.2%}\")\n                print(f\"  Worst: {worst_fit:.2%}\")\n                print(f\"\\nBest Individual (first 200 chars):\")\n                print(f\"  {best['text'][:200]}...\")\n            \n            # Save checkpoint after successful generation\n            checkpoint = {\n                'population': evaluated,  # Save evaluated population with fitness scores\n                'history': history,\n                'last_generation': gen,\n                'timestamp': datetime.now().isoformat()\n            }\n            \n            with open(checkpoint_file, 'wb') as f:\n                pickle.dump(checkpoint, f)\n            \n            if verbose:\n                print(f\"\\n Checkpoint saved (generation {gen + 1})\")\n            \n            # Check stopping criterion\n            if best['fitness'] >= target_fitness:\n                if verbose:\n                    print(f\"\\n{'='*80}\")\n                    print(f\" TARGET REACHED! Best fitness {best['fitness']:.2%} >= {target_fitness:.1%}\")\n                    print(f\"{'='*80}\")\n                break\n            \n            # Selection\n            parents = select_top_k(evaluated, k=selection_size)\n            \n            if verbose:\n                print(f\"\\nSelected {len(parents)} parents for reproduction\")\n            \n            # Mutation for next generation\n            if gen < num_generations - 1:\n                if verbose:\n                    print(f\"Generating {population_size - len(parents)} mutated offspring...\")\n                \n                # parents already contains text strings from select_top_k\n                population = mutate_population(parents, target_size=population_size)\n        \n        except Exception as e:\n            # If rate limit hit, save checkpoint and exit gracefully\n            if \"429\" in str(e) or \"quota\" in str(e).lower():\n                print(f\"\\n  RATE LIMIT HIT at generation {gen + 1}\")\n                print(f\" Progress saved to {checkpoint_file}\")\n                print(f\"\\n To resume, run:\")\n                print(f\"   results = run_genetic_algorithm_with_checkpointing(\")\n                print(f\"       initial_population,\")\n                print(f\"       resume_from_checkpoint=True\")\n                print(f\"   )\")\n                \n                # Save what we have so far\n                checkpoint = {\n                    'population': population,\n                    'history': history,\n                    'last_generation': gen - 1,  # Last completed generation\n                    'timestamp': datetime.now().isoformat(),\n                    'error': str(e)\n                }\n                \n                with open(checkpoint_file, 'wb') as f:\n                    pickle.dump(checkpoint, f)\n                \n                return {\n                    'history': history,\n                    'checkpoint_saved': True,\n                    'last_generation': gen - 1,\n                    'resume_available': True\n                }\n            else:\n                # Some other error\n                raise e\n    \n    # Final results\n    final_evaluated = evaluate_population(population)\n    best_final = final_evaluated[0]\n    \n    return {\n        'history': history,\n        'best_individual': best_final,\n        'final_population': final_evaluated,\n        'generations_run': len(history['generation']),\n        'completed': True\n    }\n\n\n# ============================================================================\n# STEP 2: Helper Function to Check Checkpoint Status\n# ============================================================================\n\ndef check_checkpoint_status(checkpoint_file='ga_checkpoint.pkl'):\n    \"\"\"Check what's in the checkpoint file\"\"\"\n    try:\n        with open(checkpoint_file, 'rb') as f:\n            checkpoint = pickle.load(f)\n        \n        print(\"=\"*80)\n        print(\"CHECKPOINT STATUS\")\n        print(\"=\"*80)\n        print(f\"Last completed generation: {checkpoint['last_generation']}\")\n        print(f\"Population size: {len(checkpoint['population'])}\")\n        print(f\"Saved at: {checkpoint['timestamp']}\")\n        \n        if 'history' in checkpoint:\n            history = checkpoint['history']\n            if history['best_fitness']:\n                print(f\"\\nBest fitness so far: {max(history['best_fitness']):.2%}\")\n                print(f\"Generations completed: {len(history['generation'])}\")\n        \n        if 'error' in checkpoint:\n            print(f\"\\nStopped due to: {checkpoint['error'][:100]}...\")\n        \n        print(\"\\n Ready to resume!\")\n        print(\"=\"*80)\n        \n        return True\n    except FileNotFoundError:\n        print(\" No checkpoint found.\")\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:08:36.978882Z","iopub.execute_input":"2026-02-07T12:08:36.979328Z","iopub.status.idle":"2026-02-07T12:08:37.011798Z","shell.execute_reply.started":"2026-02-07T12:08:36.979299Z","shell.execute_reply":"2026-02-07T12:08:37.010415Z"}},"outputs":[],"execution_count":8},{"id":"7c724ffa","cell_type":"markdown","source":"---\n## 6. Part 1: Run the Super-Imposter Attack","metadata":{}},{"id":"7ee21ba5","cell_type":"markdown","source":"### 6.1 Generate Initial Population","metadata":{}},{"id":"b5030af2","cell_type":"code","source":"# Generate 10 initial AI paragraphs on a common theme\nprint(\"Generating initial population of AI-written paragraphs...\\n\")\n\ninitial_prompts = [\n    \"Write a paragraph about artificial intelligence and its impact on society\",\n    \"Write a paragraph about AI and technology advancement\",\n    \"Write a paragraph about machine learning applications\",\n    \"Write a paragraph about AI in modern business\",\n    \"Write a paragraph about AI and scientific research\",\n    \"Write a paragraph about AI's role in automation\",\n    \"Write a paragraph about computational intelligence\",\n    \"Write a paragraph about AI and data analysis\",\n    \"Write a paragraph about AI technologies and innovation\",\n    \"Write a paragraph about AI systems and efficiency\"\n]\n\ninitial_population = [generate_text(prompt) for prompt in initial_prompts]\n\nprint(f\" Generated {len(initial_population)} paragraphs\\n\")\nprint(\"Sample from initial population:\")\nprint(f\"  {initial_population[0][:200]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:08:37.013250Z","iopub.execute_input":"2026-02-07T12:08:37.013646Z","iopub.status.idle":"2026-02-07T12:08:45.459172Z","shell.execute_reply.started":"2026-02-07T12:08:37.013606Z","shell.execute_reply":"2026-02-07T12:08:45.458007Z"}},"outputs":[{"name":"stdout","text":"Generating initial population of AI-written paragraphs...\n\n Generated 10 paragraphs\n\nSample from initial population:\n  Artificial intelligence is rapidly transforming society, offering unprecedented opportunities for progress and innovation while simultaneously raising complex ethical and societal questions. From auto...\n","output_type":"stream"}],"execution_count":9},{"id":"0655e1e5","cell_type":"markdown","source":"### 6.2 Run GA Evolution","metadata":{}},{"id":"9d7b45ff","cell_type":"code","source":"import pickle\nimport json\nfrom datetime import datetime\n# Run the genetic algorithm\nga_results = run_genetic_algorithm_with_checkpointing(\n    initial_population,\n    num_generations=6,\n    population_size=10,\n    resume_from_checkpoint=False  # Start fresh\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:11:35.058820Z","iopub.execute_input":"2026-02-07T12:11:35.059187Z","iopub.status.idle":"2026-02-07T12:12:13.304060Z","shell.execute_reply.started":"2026-02-07T12:11:35.059161Z","shell.execute_reply":"2026-02-07T12:12:13.302365Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nGENETIC ALGORITHM: SUPER-IMPOSTER EVOLUTION\n================================================================================\nPopulation size: 10\nSelection size: 3\nMax generations: 6\nTarget fitness (Human prob): 90.0%\n\n\n────────────────────────────────────────────────────────────────────────────────\nGENERATION 1/6\n────────────────────────────────────────────────────────────────────────────────\n\nFitness Statistics:\n  Best:  0.02% (Human prob) - Label: AI\n  Avg:   0.01%\n  Worst: 0.00%\n\nBest Individual (first 200 chars):\n  Artificial intelligence is rapidly transforming the landscape of scientific research, accelerating discovery across numerous disciplines. AI algorithms are capable of analyzing vast datasets that woul...\n\n Checkpoint saved (generation 1)\n\nSelected 3 parents for reproduction\nGenerating 7 mutated offspring...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_94/2799108190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run the genetic algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m ga_results = run_genetic_algorithm_with_checkpointing(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0minitial_population\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnum_generations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_94/2945286301.py\u001b[0m in \u001b[0;36mrun_genetic_algorithm_with_checkpointing\u001b[0;34m(initial_population, num_generations, population_size, selection_size, target_fitness, verbose, checkpoint_file, resume_from_checkpoint)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# parents already contains text strings from select_top_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mpopulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutate_population\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpopulation_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_94/351633647.py\u001b[0m in \u001b[0;36mmutate_population\u001b[0;34m(parents, target_size)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Generate mutated variant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mnew_population\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmutated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_94/185747308.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(prompt, use_mock)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#     # Real Gemini generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgemini_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompression\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     ) -> Any:\n\u001b[0;32m--> 277\u001b[0;31m         response, ignored_call = self._with_call(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m_with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_FailureOutcome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         call = self._interceptor.intercept_unary_unary(\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0mcontinuation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_call_details\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py\u001b[0m in \u001b[0;36mintercept_unary_unary\u001b[0;34m(self, continuation, client_call_details, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontinuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_call_details\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogging_enabled\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: NO COVER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mresponse_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrailing_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36mcontinuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    313\u001b[0m             ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 response, call = self._thunk(new_method).with_call(\n\u001b[0m\u001b[1;32m    316\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mwith_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompression\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m     ) -> Tuple[Any, grpc.Call]:\n\u001b[0;32m-> 1192\u001b[0;31m         state, call = self._blocking(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_for_ready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1163\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_call_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             )\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m             \u001b[0m_handle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n","\u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi\u001b[0m in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":16},{"id":"f1f21a79-f106-49c7-8be8-c279b224cf90","cell_type":"code","source":"check_checkpoint_status()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:09:07.347307Z","iopub.execute_input":"2026-02-07T12:09:07.347996Z","iopub.status.idle":"2026-02-07T12:09:07.355560Z","shell.execute_reply.started":"2026-02-07T12:09:07.347958Z","shell.execute_reply":"2026-02-07T12:09:07.354457Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nCHECKPOINT STATUS\n================================================================================\nLast completed generation: 5\nPopulation size: 10\nSaved at: 2026-02-07T12:09:04.504848\n\nBest fitness so far: 64.70%\nGenerations completed: 6\n\n Ready to resume!\n================================================================================\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":11},{"id":"bd8844a4-a0a6-47c6-b859-6e550d909c0d","cell_type":"code","source":"ga_results = run_genetic_algorithm_with_checkpointing(\n    initial_population,  # Same as before (won't be used, just needed as parameter)\n    num_generations=10,\n    population_size=10,\n    resume_from_checkpoint=True  # ← This is the key!\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:09:07.356980Z","iopub.execute_input":"2026-02-07T12:09:07.357300Z","iopub.status.idle":"2026-02-07T12:09:09.465216Z","shell.execute_reply.started":"2026-02-07T12:09:07.357273Z","shell.execute_reply":"2026-02-07T12:09:09.463568Z"}},"outputs":[{"name":"stdout","text":" RESUMING FROM CHECKPOINT\n   Last completed generation: 5\n   Continuing from generation: 6\n   Population size: 10\n\n\n────────────────────────────────────────────────────────────────────────────────\nGENERATION 7/10\n────────────────────────────────────────────────────────────────────────────────\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_94/23931876.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m ga_results = run_genetic_algorithm_with_checkpointing(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minitial_population\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Same as before (won't be used, just needed as parameter)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_generations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpopulation_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m  \u001b[0;31m# ← This is the key!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_94/2945286301.py\u001b[0m in \u001b[0;36mrun_genetic_algorithm_with_checkpointing\u001b[0;34m(initial_population, num_generations, population_size, selection_size, target_fitness, verbose, checkpoint_file, resume_from_checkpoint)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# Some other error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# Final results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_94/2945286301.py\u001b[0m in \u001b[0;36mrun_genetic_algorithm_with_checkpointing\u001b[0;34m(initial_population, num_generations, population_size, selection_size, target_fitness, verbose, checkpoint_file, resume_from_checkpoint)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# Evaluate population\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mevaluated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_population\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Track metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_94/351633647.py\u001b[0m in \u001b[0;36mevaluate_population\u001b[0;34m(population)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         evaluated.append({\n\u001b[1;32m     18\u001b[0m             \u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_94/3195467273.py\u001b[0m in \u001b[0;36mdetect_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mDict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m'human_prob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ai_prob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prediction'\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHuman\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     inputs = tokenizer(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2998\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2999\u001b[0m                 \u001b[0;34m\"text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m                 \u001b[0;34m\"or `list[list[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples)."],"ename":"ValueError","evalue":"text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples).","output_type":"error"}],"execution_count":12},{"id":"7fea310f","cell_type":"markdown","source":"### 6.3 Visualize Evolution","metadata":{}},{"id":"c8f6ed84","cell_type":"code","source":"# Plot fitness evolution\nhistory = ga_results['history']\n\n# Create output directory if it doesn't exist\nPath('../output').mkdir(parents=True, exist_ok=True)\n\nplt.figure(figsize=(14, 6))\n\n# Plot 1: Fitness over generations\nplt.subplot(1, 2, 1)\nplt.plot(history['generation'], history['best_fitness'], 'o-', \n         color='green', linewidth=2, markersize=8, label='Best')\nplt.plot(history['generation'], history['avg_fitness'], 's--', \n         color='blue', linewidth=1.5, markersize=6, label='Average')\nplt.plot(history['generation'], history['worst_fitness'], '^:', \n         color='red', linewidth=1, markersize=6, label='Worst')\nplt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Threshold (50%)')\nplt.axhline(y=0.9, color='orange', linestyle='--', alpha=0.7, label='Target (90%)')\nplt.xlabel('Generation', fontsize=12, fontweight='bold')\nplt.ylabel('Human Probability (Fitness)', fontsize=12, fontweight='bold')\nplt.title('GA Evolution: Fitness Over Time', fontsize=14, fontweight='bold')\nplt.legend(loc='best')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\n\n# Plot 2: Classification flip rate\nplt.subplot(1, 2, 2)\nflip_points = [1 if f >= 0.5 else 0 for f in history['best_fitness']]\ncolors = ['red' if f < 0.5 else 'green' for f in history['best_fitness']]\nplt.bar(history['generation'], history['best_fitness'], color=colors, alpha=0.7)\nplt.axhline(y=0.5, color='black', linestyle='-', linewidth=2, label='Decision Boundary')\nplt.xlabel('Generation', fontsize=12, fontweight='bold')\nplt.ylabel('Best Human Probability', fontsize=12, fontweight='bold')\nplt.title('Detector Fooling Progress\\n(Red=AI, Green=Human)', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\n\nplt.savefig('../output/ga_evolution.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n✓ Plots saved to ../output/ga_evolution.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:09:09.466215Z","iopub.status.idle":"2026-02-07T12:09:09.466504Z","shell.execute_reply.started":"2026-02-07T12:09:09.466372Z","shell.execute_reply":"2026-02-07T12:09:09.466389Z"}},"outputs":[],"execution_count":null},{"id":"56ad3256","cell_type":"markdown","source":"### 6.4 Final Results","metadata":{}},{"id":"f9045ee9","cell_type":"code","source":"best = ga_results['best_individual']\n\nprint(\"=\"*80)\nprint(\"FINAL RESULTS: SUPER-IMPOSTER\")\nprint(\"=\"*80)\nprint(f\"\\nGenerations run: {ga_results['generations_run']}\")\nprint(f\"\\nBest Individual Scores:\")\nprint(f\"  Human probability: {best['human_prob']:.2%}\")\nprint(f\"  AI probability:    {best['ai_prob']:.2%}\")\nprint(f\"  Classification:    {best['label']}\")\n\nprint(f\"\\nEvolution Summary:\")\nprint(f\"  Initial best fitness:  {history['best_fitness'][0]:.2%}\")\nprint(f\"  Final best fitness:    {history['best_fitness'][-1]:.2%}\")\nprint(f\"  Improvement:           {(history['best_fitness'][-1] - history['best_fitness'][0]):.2%}\")\n\nif best['label'] == 'Human':\n    print(f\"\\n🎉 SUCCESS! The detector was fooled!\")\nelse:\n    print(f\"\\n⚠️  Detector not fully fooled, but Human probability increased\")\n\nprint(f\"\\n{'─'*80}\")\nprint(\"SUPER-IMPOSTER TEXT:\")\nprint(f\"{'─'*80}\")\nprint(best['text'])\nprint(f\"{'─'*80}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:09:09.468794Z","iopub.status.idle":"2026-02-07T12:09:09.469378Z","shell.execute_reply.started":"2026-02-07T12:09:09.469132Z","shell.execute_reply":"2026-02-07T12:09:09.469162Z"}},"outputs":[],"execution_count":null},{"id":"8a870b42","cell_type":"markdown","source":"---\n## 7. Part 2: The Personal Test","metadata":{}},{"id":"e7cef9d1","cell_type":"markdown","source":"### 7.1 Test User-Provided Text","metadata":{}},{"id":"6c7f0e10","cell_type":"code","source":"# Example user text - replace with your own!\nuser_text = \"\"\"\nI am deeply passionate about artificial intelligence and its potential to revolutionize healthcare. \nThroughout my academic journey, I have consistently sought opportunities to bridge theoretical knowledge \nwith practical applications. Furthermore, my research experience in machine learning has equipped me \nwith the necessary skills to contribute meaningfully to cutting-edge projects. Consequently, I believe \nthat pursuing graduate studies at your institution would enable me to achieve my long-term career goals.\n\"\"\".strip()\n\nprint(\"=\"*80)\nprint(\"PERSONAL TEST: Analyzing Your Text\")\nprint(\"=\"*80)\nprint(f\"\\nText (first 200 chars):\\n{user_text[:200]}...\\n\")\n\n# Detect\nresult = detect_text(user_text)\n\nprint(f\"Detection Results:\")\nprint(f\"  Human probability: {result['human_prob']:.2%}\")\nprint(f\"  AI probability:    {result['ai_prob']:.2%}\")\nprint(f\"  Classification:    {result['label']}\")\n\n# Store for later use\noriginal_classification = result['label']\noriginal_human_prob = result['human_prob']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:09:09.470298Z","iopub.status.idle":"2026-02-07T12:09:09.470706Z","shell.execute_reply.started":"2026-02-07T12:09:09.470504Z","shell.execute_reply":"2026-02-07T12:09:09.470527Z"}},"outputs":[],"execution_count":null},{"id":"d44cadc6","cell_type":"markdown","source":"### 7.2 Provide Suggestions Based on Classification","metadata":{}},{"id":"3303333e","cell_type":"code","source":"def analyze_ai_markers(text: str) -> Dict[str, any]:\n    \"\"\"\n    Analyze text for common AI markers.\n    \"\"\"\n    ai_markers = [\n        'furthermore', 'moreover', 'consequently', 'thus', 'therefore',\n        'additionally', 'nevertheless', 'nonetheless', 'paradigm',\n        'framework', 'leverage', 'facilitate', 'optimize', 'comprehensive'\n    ]\n    \n    text_lower = text.lower()\n    found_markers = [marker for marker in ai_markers if marker in text_lower]\n    \n    # Check sentence structure\n    sentences = text.split('.')\n    avg_sent_len = np.mean([len(s.split()) for s in sentences if s.strip()])\n    \n    # Check for repetitive structure\n    sentence_starts = [s.strip().split()[0] if s.strip() else '' for s in sentences]\n    \n    return {\n        'ai_markers': found_markers,\n        'marker_count': len(found_markers),\n        'avg_sentence_length': avg_sent_len,\n        'sentence_count': len([s for s in sentences if s.strip()])\n    }\n\nprint(f\"\\n{'─'*80}\")\nif original_classification == 'AI':\n    print(\"  TEXT CLASSIFIED AS AI-GENERATED\")\n    print(f\"{'─'*80}\")\n    \n    analysis = analyze_ai_markers(user_text)\n    \n    print(\"\\nDiagnostic Analysis:\")\n    print(f\"  AI marker words found: {analysis['marker_count']}\")\n    if analysis['ai_markers']:\n        print(f\"  Markers: {', '.join(analysis['ai_markers'])}\")\n    print(f\"  Average sentence length: {analysis['avg_sentence_length']:.1f} words\")\n    print(f\"  Total sentences: {analysis['sentence_count']}\")\n    \n    print(\"\\n💡 Suggestions to Reduce AI Score:\")\n    print(\"  1. Remove formal connectives (furthermore, consequently, etc.)\")\n    print(\"  2. Vary sentence structure and length\")\n    print(\"  3. Use more concrete, specific examples\")\n    print(\"  4. Add personal anecdotes or informal language\")\n    print(\"  5. Introduce minor grammatical variations\")\n    \nelse:\n    print(\"✓ TEXT CLASSIFIED AS HUMAN-WRITTEN\")\n    print(f\"{'─'*80}\")\n    print(\"\\n💡 To make it sound MORE like AI (for testing):\")\n    print(\"  1. Add formal connectives (furthermore, moreover, consequently)\")\n    print(\"  2. Make sentences more uniform in structure\")\n    print(\"  3. Use more abstract, technical vocabulary\")\n    print(\"  4. Remove contractions and informal language\")\n    print(\"  5. Add logical transitions between ideas\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:09:09.472620Z","iopub.status.idle":"2026-02-07T12:09:09.473145Z","shell.execute_reply.started":"2026-02-07T12:09:09.472888Z","shell.execute_reply":"2026-02-07T12:09:09.472917Z"}},"outputs":[],"execution_count":null},{"id":"8f243a37","cell_type":"markdown","source":"### 7.3 Manual Rewrite Attempt","metadata":{}},{"id":"651c59b8","cell_type":"code","source":"# Rewrite the text based on classification\nif original_classification == 'AI':\n    # Rewrite to sound more human\n    rewritten_text = \"\"\"\nI've always been fascinated by AI and how it could transform healthcare. During my studies, \nI tried to connect what I learned in class with real-world problems. My research in machine learning \ntaught me a lot - not just technical skills, but how to think creatively about solutions. \nI'm really excited about the possibility of joining your program because I think it's the perfect \nnext step toward what I want to do with my career.\n\"\"\".strip()\n    \n    print(f\"\\n{'='*80}\")\n    print(\"REWRITE ATTEMPT: Making Text Sound More Human\")\n    print(f\"{'='*80}\")\n    \nelse:\n    # Rewrite to sound more AI-like\n    rewritten_text = \"\"\"\nI am profoundly interested in artificial intelligence and its transformative potential within \nthe healthcare sector. Furthermore, throughout my academic trajectory, I have consistently pursued \nopportunities to synthesize theoretical frameworks with practical implementations. Moreover, my \nextensive research experience in machine learning methodologies has equipped me with comprehensive \ncompetencies. Consequently, I am convinced that graduate studies at your esteemed institution \nwould facilitate the achievement of my long-term professional objectives.\n\"\"\".strip()\n    \n    print(f\"\\n{'='*80}\")\n    print(\"REWRITE ATTEMPT: Making Text Sound More AI-Like\")\n    print(f\"{'='*80}\")\n\nprint(f\"\\nRewritten text:\\n{rewritten_text}\\n\")\n\n# Re-evaluate\nrewrite_result = detect_text(rewritten_text)\n\nprint(f\"New Detection Results:\")\nprint(f\"  Human probability: {rewrite_result['human_prob']:.2%}\")\nprint(f\"  AI probability:    {rewrite_result['ai_prob']:.2%}\")\nprint(f\"  Classification:    {rewrite_result['label']}\")\n\n# Compare\nprint(f\"\\n{'─'*80}\")\nprint(\"COMPARISON\")\nprint(f\"{'─'*80}\")\nprint(f\"Original:  {original_classification} ({original_human_prob:.2%} Human)\")\nprint(f\"Rewritten: {rewrite_result['label']} ({rewrite_result['human_prob']:.2%} Human)\")\n\nif original_classification == 'AI':\n    if rewrite_result['label'] == 'Human':\n        print(f\"\\n✅ SUCCESS! Fooled the detector!\")\n    elif rewrite_result['human_prob'] > original_human_prob:\n        print(f\"\\n📈 IMPROVEMENT! Human probability increased by {(rewrite_result['human_prob'] - original_human_prob):.2%}\")\n    else:\n        print(f\"\\n⚠️  No improvement\")\nelse:\n    if rewrite_result['label'] == 'AI':\n        print(f\"\\n✅ SUCCESS! Made it sound like AI!\")\n    elif rewrite_result['ai_prob'] > result['ai_prob']:\n        print(f\"\\n📈 IMPROVEMENT! AI probability increased by {(rewrite_result['ai_prob'] - result['ai_prob']):.2%}\")\n    else:\n        print(f\"\\n⚠️  No significant change\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:09:09.474832Z","iopub.status.idle":"2026-02-07T12:09:09.475520Z","shell.execute_reply.started":"2026-02-07T12:09:09.475288Z","shell.execute_reply":"2026-02-07T12:09:09.475317Z"}},"outputs":[],"execution_count":null},{"id":"3d4d1919","cell_type":"markdown","source":"---\n## 8. Summary & Insights","metadata":{}},{"id":"29d222fe","cell_type":"code","source":"print(\"=\"*80)\nprint(\"TASK 4 SUMMARY: THE TURING TEST\")\nprint(\"=\"*80)\n\nprint(\"\\n📊 PART 1: SUPER-IMPOSTER (GA ATTACK)\")\nprint(\"─\"*80)\nprint(f\"  Initial population fitness: {history['best_fitness'][0]:.2%}\")\nprint(f\"  Final best fitness:         {history['best_fitness'][-1]:.2%}\")\nprint(f\"  Generations to converge:    {ga_results['generations_run']}\")\nprint(f\"  Detector fooled:            {'YES' if best['label'] == 'Human' else 'NO'}\")\nprint(f\"  Fitness improvement:        {(history['best_fitness'][-1] - history['best_fitness'][0]):.2%}\")\n\nprint(\"\\n👤 PART 2: PERSONAL TEST\")\nprint(\"─\"*80)\nprint(f\"  Original classification:    {original_classification}\")\nprint(f\"  Original Human prob:        {original_human_prob:.2%}\")\nprint(f\"  Rewritten classification:   {rewrite_result['label']}\")\nprint(f\"  Rewritten Human prob:       {rewrite_result['human_prob']:.2%}\")\nprint(f\"  Successfully modified:      {'YES' if rewrite_result['label'] != original_classification else 'NO'}\")\n\nprint(\"\\n💡 KEY INSIGHTS\")\nprint(\"─\"*80)\nprint(\"  1. GA successfully evolves text toward higher Human probability\")\nprint(\"  2. LLM-as-Mutator is effective for adversarial text generation\")\nprint(\"  3. Removing AI-isms (furthermore, consequently) helps evasion\")\nprint(\"  4. Varying sentence structure reduces detection confidence\")\nprint(\"  5. Manual rewrites can flip classification with targeted edits\")\n\nprint(\"\\n⚠️  LIMITATIONS\")\nprint(\"─\"*80)\nprint(\"  • GA uses mock LLM generation (for demo purposes)\")\nprint(\"  • Real Gemini API would provide better mutations\")\nprint(\"  • Detector may be overfitted to specific AI patterns\")\nprint(\"  • Adversarial examples may not generalize to other detectors\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"✓ Task 4 Complete\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T12:09:09.476680Z","iopub.status.idle":"2026-02-07T12:09:09.477436Z","shell.execute_reply.started":"2026-02-07T12:09:09.477185Z","shell.execute_reply":"2026-02-07T12:09:09.477215Z"}},"outputs":[],"execution_count":null}]}