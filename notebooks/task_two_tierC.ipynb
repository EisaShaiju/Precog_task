{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14719881,"sourceType":"datasetVersion","datasetId":9405205}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ea288d8e","cell_type":"markdown","source":"# Tier C: The Transformer - AI vs Human Text Detection\n## Using DistilBERT with LoRA (Low-Rank Adaptation) Fine-tuning\n\nThis notebook implements a binary classifier that distinguishes AI-generated from human-written text using:\n- **DistilBERT** (distilbert-base-uncased) - lightweight transformer\n- **LoRA (Low-Rank Adaptation)** via peft library - parameter-efficient fine-tuning\n- **HuggingFace Transformers** - unified interface for model training\n\n**Author**: Tier C Implementation  \n**Dataset**: Human novels (class1) + AI-generated paragraphs (class2)  \n**Model**: DistilBERT + LoRA with binary classification head  \n**Training Strategy**: Stratified 64/16/20 split with early stopping","metadata":{}},{"id":"04261468","cell_type":"markdown","source":"---\n## 1. Environment Setup & Imports","metadata":{}},{"id":"7cfe832f","cell_type":"code","source":"# Environment detection and core imports\nimport os\nimport sys\nimport json\nimport re\nimport warnings\nfrom pathlib import Path\nfrom collections import defaultdict\n\nwarnings.filterwarnings('ignore')\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# PyTorch & Transformers\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n    get_linear_schedule_with_warmup\n)\n\n# PEFT (Parameter-Efficient Fine-Tuning) for LoRA\nfrom peft import (\n    get_peft_model,\n    LoraConfig,\n    TaskType\n)\n\n# HuggingFace Datasets\nfrom datasets import Dataset, DatasetDict\n\n# Sklearn metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score,\n    confusion_matrix,\n    classification_report\n)\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n\nprint(\"‚úì All imports successful\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Check VRAM if CUDA available\nif torch.cuda.is_available():\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:26.072155Z","iopub.execute_input":"2026-02-04T20:22:26.072852Z","iopub.status.idle":"2026-02-04T20:22:26.082386Z","shell.execute_reply.started":"2026-02-04T20:22:26.072821Z","shell.execute_reply":"2026-02-04T20:22:26.081756Z"}},"outputs":[{"name":"stdout","text":"‚úì All imports successful\nPyTorch version: 2.8.0+cu126\nCUDA available: True\nUsing device: cuda\nGPU Memory: 15.6 GB\n","output_type":"stream"}],"execution_count":4},{"id":"58d2e159","cell_type":"markdown","source":"---\n## 2. Data Loading & Exploration\n\n**Data Sources:**\n- **Class 1 (Human)**: Cleaned novel texts, chunked into ~200-word paragraphs\n- **Class 2 (AI-generated)**: Pre-generated paragraphs from Gemini\n\n**Process:**\n1. Load cleaned human text and chunk into paragraphs\n2. Load AI-generated JSONL files\n3. Create DataFrame with proper stratification","metadata":{}},{"id":"77460934","cell_type":"code","source":"# Configure paths for Kaggle vs Local execution\nimport os\n\n# Detect if running in Kaggle\nIN_KAGGLE = os.path.exists('/kaggle/input')\n\nif IN_KAGGLE:\n    # Kaggle paths\n    BASE_PATH = Path('/kaggle/input/precog-novels-data/precog-novels-data')\n    CLASS1_PATH = BASE_PATH / 'class1'\n    CLASS2_PATH = BASE_PATH / 'class2'\n    OUTPUT_PATH = Path('/kaggle/working')\nelse:\n    # Local paths (adjust based on your workspace)\n    BASE_PATH = Path('../output')\n    CLASS1_PATH = BASE_PATH / 'class1'\n    CLASS2_PATH = BASE_PATH / 'class2'\n    OUTPUT_PATH = Path('../output/tier_c_models')\n    OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Running in: {'Kaggle' if IN_KAGGLE else 'Local'}\")\nprint(f\"Base path: {BASE_PATH}\")\nprint(f\"Class1 path exists: {CLASS1_PATH.exists()}\")\nprint(f\"Class2 path exists: {CLASS2_PATH.exists()}\")\nprint(f\"Output path: {OUTPUT_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:26.083593Z","iopub.execute_input":"2026-02-04T20:22:26.083803Z","iopub.status.idle":"2026-02-04T20:22:26.117678Z","shell.execute_reply.started":"2026-02-04T20:22:26.083784Z","shell.execute_reply":"2026-02-04T20:22:26.117021Z"}},"outputs":[{"name":"stdout","text":"Running in: Kaggle\nBase path: /kaggle/input/precog-novels-data/precog-novels-data\nClass1 path exists: True\nClass2 path exists: True\nOutput path: /kaggle/working\n","output_type":"stream"}],"execution_count":5},{"id":"120c52a1","cell_type":"code","source":"def chunk_text(text, chunk_size=200):\n    \"\"\"\n    Chunk text into paragraphs of approximately chunk_size words.\n    \n    Args:\n        text: Input text string\n        chunk_size: Target number of words per chunk\n    \n    Returns:\n        List of text chunks\n    \"\"\"\n    words = text.split()\n    chunks = []\n    \n    for i in range(0, len(words), chunk_size):\n        chunk = ' '.join(words[i:i + chunk_size])\n        if len(chunk.split()) >= 50:  # Minimum 50 words per chunk\n            chunks.append(chunk)\n    \n    return chunks\n\n\ndef load_human_data(class1_path):\n    \"\"\"\n    Load human-written text from cleaned novel files and chunk them.\n    \n    Returns:\n        List of dictionaries with 'text', 'label', and 'source' keys\n    \"\"\"\n    novels = [\n        'heart_of_darkness_cleaned.txt',\n        'lord_jim_cleaned.txt',\n        'metamorphosis_cleaned.txt',\n        'the_trial_cleaned.txt',\n        'typhoon_cleaned.txt'\n    ]\n    \n    human_data = []\n    \n    for novel_file in novels:\n        file_path = class1_path / novel_file\n        if file_path.exists():\n            with open(file_path, 'r', encoding='utf-8') as f:\n                text = f.read()\n            \n            # Chunk the text\n            chunks = chunk_text(text, chunk_size=200)\n            \n            # Add to dataset\n            for chunk in chunks:\n                human_data.append({\n                    'text': chunk,\n                    'label': 0,  # 0 = Human\n                    'source': novel_file.replace('_cleaned.txt', '')\n                })\n            \n            print(f\"‚úì Loaded {novel_file}: {len(chunks)} chunks\")\n        else:\n            print(f\"‚úó File not found: {file_path}\")\n    \n    return human_data\n\n\ndef load_ai_data(class2_path):\n    \"\"\"\n    Load AI-generated text from JSONL files.\n    \n    Returns:\n        List of dictionaries with 'text', 'label', and 'source' keys\n    \"\"\"\n    novels = [\n        'heart_of_darkness_generic.jsonl',\n        'lord_jim_generic.jsonl',\n        'metamorphosis_generic.jsonl',\n        'the_trial_generic.jsonl',\n        'typhoon_generic.jsonl'\n    ]\n    \n    ai_data = []\n    \n    for novel_file in novels:\n        file_path = class2_path / novel_file\n        if file_path.exists():\n            with open(file_path, 'r', encoding='utf-8') as f:\n                lines = f.readlines()\n            \n            count = 0\n            # Parse JSONL\n            for line in lines:\n                try:\n                    entry = json.loads(line.strip())\n                    # Extract text (adjust key based on your JSONL structure)\n                    text = entry.get('text') or entry.get('paragraph') or entry.get('content', '')\n                    \n                    if text and len(text.split()) >= 50:  # Minimum 50 words\n                        ai_data.append({\n                            'text': text,\n                            'label': 1,  # 1 = AI\n                            'source': novel_file.replace('_generic.jsonl', '')\n                        })\n                        count += 1\n                except json.JSONDecodeError:\n                    continue\n            \n            print(f\"‚úì Loaded {novel_file}: {count} paragraphs\")\n        else:\n            print(f\"‚úó File not found: {file_path}\")\n    \n    return ai_data\n\n\n# Load all data\nprint(\"Loading Human data (Class 1)...\")\nhuman_data = load_human_data(CLASS1_PATH)\n\nprint(\"\\nLoading AI data (Class 2)...\")\nai_data = load_ai_data(CLASS2_PATH)\n\n# Combine datasets\nall_data = human_data + ai_data\n\nprint(f\"\\n{'='*70}\")\nprint(f\"Total Human paragraphs: {len(human_data)}\")\nprint(f\"Total AI paragraphs: {len(ai_data)}\")\nprint(f\"Total dataset size: {len(all_data)}\")\nprint(f\"Balance: {len(human_data)/len(all_data)*100:.1f}% Human, {len(ai_data)/len(all_data)*100:.1f}% AI\")\nprint(f\"{'='*70}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:26.118561Z","iopub.execute_input":"2026-02-04T20:22:26.118810Z","iopub.status.idle":"2026-02-04T20:22:26.407264Z","shell.execute_reply.started":"2026-02-04T20:22:26.118784Z","shell.execute_reply":"2026-02-04T20:22:26.406678Z"}},"outputs":[{"name":"stdout","text":"Loading Human data (Class 1)...\n‚úì Loaded heart_of_darkness_cleaned.txt: 196 chunks\n‚úì Loaded lord_jim_cleaned.txt: 649 chunks\n‚úì Loaded metamorphosis_cleaned.txt: 111 chunks\n‚úì Loaded the_trial_cleaned.txt: 418 chunks\n‚úì Loaded typhoon_cleaned.txt: 156 chunks\n\nLoading AI data (Class 2)...\n‚úì Loaded heart_of_darkness_generic.jsonl: 500 paragraphs\n‚úì Loaded lord_jim_generic.jsonl: 500 paragraphs\n‚úì Loaded metamorphosis_generic.jsonl: 500 paragraphs\n‚úì Loaded the_trial_generic.jsonl: 500 paragraphs\n‚úì Loaded typhoon_generic.jsonl: 500 paragraphs\n\n======================================================================\nTotal Human paragraphs: 1530\nTotal AI paragraphs: 2500\nTotal dataset size: 4030\nBalance: 38.0% Human, 62.0% AI\n======================================================================\n","output_type":"stream"}],"execution_count":6},{"id":"9d40360e","cell_type":"code","source":"# Create DataFrame\ndf = pd.DataFrame(all_data)\n\n# Shuffle the dataset\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Display dataset info\nprint(\"Dataset Overview:\")\nprint(df.head(10))\nprint(f\"\\nDataset shape: {df.shape}\")\nprint(f\"\\nLabel distribution:\")\nprint(df['label'].value_counts())\nprint(f\"\\nSource distribution:\")\nprint(df['source'].value_counts())\n\n# Check text lengths\ndf['text_length'] = df['text'].str.split().str.len()\nprint(f\"\\nText length statistics (words):\")\nprint(df['text_length'].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:26.408654Z","iopub.execute_input":"2026-02-04T20:22:26.408856Z","iopub.status.idle":"2026-02-04T20:22:26.513302Z","shell.execute_reply.started":"2026-02-04T20:22:26.408838Z","shell.execute_reply":"2026-02-04T20:22:26.512631Z"}},"outputs":[{"name":"stdout","text":"Dataset Overview:\n                                                text  label             source\n0  weaker by the day.\" \"I see,\" said K.'s uncle, ...      0          the_trial\n1  passage they disturbed an old hag who did the ...      0           lord_jim\n2  on in a gentle, almost yearning tone, \"that al...      0           lord_jim\n3  The absurdity of the law is starkly revealed i...      1          the_trial\n4  The supposed distinction between \"savagery\" an...      1  heart_of_darkness\n5  himself were like those glimpses through the s...      0           lord_jim\n6  in the facts of human existence. I don't know....      0  heart_of_darkness\n7  Within the nature of darkness lies the profoun...      1  heart_of_darkness\n8  Unimaginative literalism presents a peculiar b...      1            typhoon\n9  He pretended a great reluctance. The voice dec...      0           lord_jim\n\nDataset shape: (4030, 3)\n\nLabel distribution:\nlabel\n1    2500\n0    1530\nName: count, dtype: int64\n\nSource distribution:\nsource\nlord_jim             1149\nthe_trial             918\nheart_of_darkness     696\ntyphoon               656\nmetamorphosis         611\nName: count, dtype: int64\n\nText length statistics (words):\ncount    4030.000000\nmean      154.892556\nstd        39.361901\nmin        80.000000\n25%       121.000000\n50%       149.000000\n75%       200.000000\nmax       200.000000\nName: text_length, dtype: float64\n","output_type":"stream"}],"execution_count":7},{"id":"ee5378f6","cell_type":"markdown","source":"---\n## 3. Train/Val/Test Split (BEFORE PREPROCESSING)\n\n‚ö†Ô∏è **CRITICAL**: We split FIRST before any preprocessing to prevent data leakage.\n- Training set: 64%\n- Validation set: 16%\n- Test set: 20%","metadata":{}},{"id":"fd4652f3","cell_type":"code","source":"# Step 1: Split into train (64%) and temp (36%)\ntrain_df, temp_df = train_test_split(\n    df,\n    test_size=0.36,  # 36% for val + test\n    stratify=df['label'],\n    random_state=42\n)\n\n# Step 2: Split temp into val (16%) and test (20%)\n# From 36%: val should be 16/36 ‚âà 0.444 and test should be 20/36 ‚âà 0.556\nval_df, test_df = train_test_split(\n    temp_df,\n    test_size=20/36,  # 20% of original\n    stratify=temp_df['label'],\n    random_state=42\n)\n\nprint(f\"Training set size: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\nprint(f\"Validation set size: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\nprint(f\"Test set size: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\nprint(f\"Total: {len(train_df) + len(val_df) + len(test_df)}\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"Training set label distribution:\")\nprint(train_df['label'].value_counts())\nprint(f\"Human: {(train_df['label']==0).sum()/len(train_df)*100:.1f}%, AI: {(train_df['label']==1).sum()/len(train_df)*100:.1f}%\")\n\nprint(f\"\\nValidation set label distribution:\")\nprint(val_df['label'].value_counts())\nprint(f\"Human: {(val_df['label']==0).sum()/len(val_df)*100:.1f}%, AI: {(val_df['label']==1).sum()/len(val_df)*100:.1f}%\")\n\nprint(f\"\\nTest set label distribution:\")\nprint(test_df['label'].value_counts())\nprint(f\"Human: {(test_df['label']==0).sum()/len(test_df)*100:.1f}%, AI: {(test_df['label']==1).sum()/len(test_df)*100:.1f}%\")\nprint(f\"{'='*70}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:26.514089Z","iopub.execute_input":"2026-02-04T20:22:26.514375Z","iopub.status.idle":"2026-02-04T20:22:26.545306Z","shell.execute_reply.started":"2026-02-04T20:22:26.514355Z","shell.execute_reply":"2026-02-04T20:22:26.544690Z"}},"outputs":[{"name":"stdout","text":"Training set size: 2579 (64.0%)\nValidation set size: 644 (16.0%)\nTest set size: 807 (20.0%)\nTotal: 4030\n\n======================================================================\nTraining set label distribution:\nlabel\n1    1600\n0     979\nName: count, dtype: int64\nHuman: 38.0%, AI: 62.0%\n\nValidation set label distribution:\nlabel\n1    399\n0    245\nName: count, dtype: int64\nHuman: 38.0%, AI: 62.0%\n\nTest set label distribution:\nlabel\n1    501\n0    306\nName: count, dtype: int64\nHuman: 37.9%, AI: 62.1%\n======================================================================\n","output_type":"stream"}],"execution_count":8},{"id":"a88f8e91","cell_type":"markdown","source":"---\n## 4. Model & Tokenizer Setup\n\nLoad DistilBERT and initialize tokenizer.","metadata":{}},{"id":"4e53bce5","cell_type":"code","source":"# Model and tokenizer configuration\nMODEL_NAME = 'distilbert-base-uncased'\nMAX_LENGTH = 256\n\nprint(f\"Loading model: {MODEL_NAME}\")\nprint(f\"Max sequence length: {MAX_LENGTH}\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint(f\"‚úì Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n\n# Load base model\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=2,  # Binary classification\n    problem_type='single_label_classification'\n)\n\nprint(f\"‚úì Base model loaded\")\nprint(f\"Model parameters: {sum(p.numel() for p in base_model.parameters()):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:26.546397Z","iopub.execute_input":"2026-02-04T20:22:26.546680Z","iopub.status.idle":"2026-02-04T20:22:29.235702Z","shell.execute_reply.started":"2026-02-04T20:22:26.546661Z","shell.execute_reply":"2026-02-04T20:22:29.235157Z"}},"outputs":[{"name":"stdout","text":"Loading model: distilbert-base-uncased\nMax sequence length: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1835fdbb14b42548cea2ac9f294f019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"345c11106ef4497ca9dcddcae89647b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7058ec161eb4a55bc456acc14616157"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dee95a381b0e432aaee01c883f656783"}},"metadata":{}},{"name":"stdout","text":"‚úì Tokenizer loaded. Vocab size: 30522\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6c4df1e836c4162a742482bbb3b15fe"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"‚úì Base model loaded\nModel parameters: 66,955,010\n","output_type":"stream"}],"execution_count":9},{"id":"f65133bd","cell_type":"markdown","source":"---\n## 5. LoRA Configuration & Model Setup\n\nConfigure LoRA with:\n- Rank (r): 8\n- Alpha: 16\n- Dropout: 0.1\n- Target modules: q_lin, v_lin (DistilBERT attention layers)","metadata":{}},{"id":"1b65bdc9","cell_type":"code","source":"# LoRA Configuration\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=8,                              # Rank\n    lora_alpha=16,                    # Scaling factor\n    lora_dropout=0.1,                 # Dropout in LoRA layers\n    bias='none',                      # Don't train bias\n    target_modules=['q_lin', 'v_lin'] # DistilBERT query and value projections\n)\n\nprint(\"LoRA Configuration:\")\nprint(f\"  Rank (r): {lora_config.r}\")\nprint(f\"  Alpha: {lora_config.lora_alpha}\")\nprint(f\"  Dropout: {lora_config.lora_dropout}\")\nprint(f\"  Target modules: {lora_config.target_modules}\")\n\n# Apply LoRA to model\nmodel = get_peft_model(base_model, lora_config)\nprint(f\"\\n‚úì LoRA applied to model\")\n\n# Compare parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nreduction = (1 - trainable_params / total_params) * 100\n\nprint(f\"\\nParameter efficiency:\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\nprint(f\"  Parameter reduction: {reduction:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:29.236545Z","iopub.execute_input":"2026-02-04T20:22:29.236828Z","iopub.status.idle":"2026-02-04T20:22:29.265872Z","shell.execute_reply.started":"2026-02-04T20:22:29.236807Z","shell.execute_reply":"2026-02-04T20:22:29.265320Z"}},"outputs":[{"name":"stdout","text":"LoRA Configuration:\n  Rank (r): 8\n  Alpha: 16\n  Dropout: 0.1\n  Target modules: {'v_lin', 'q_lin'}\n\n‚úì LoRA applied to model\n\nParameter efficiency:\n  Total parameters: 67,694,596\n  Trainable parameters: 739,586 (1.09%)\n  Parameter reduction: 98.91%\n","output_type":"stream"}],"execution_count":10},{"id":"adb8fafb","cell_type":"markdown","source":"---\n## 6. Tokenization & Dataset Preparation\n\nConvert texts to token sequences and create HuggingFace Dataset objects.","metadata":{}},{"id":"f59fdd75","cell_type":"code","source":"def tokenize_function(examples):\n    \"\"\"\n    Tokenize batch of examples.\n    \"\"\"\n    return tokenizer(\n        examples['text'],\n        max_length=MAX_LENGTH,\n        truncation=True,\n        padding='max_length',\n        return_tensors='pt'\n    )\n\n\n# Create HuggingFace Datasets\nprint(\"Creating HuggingFace Datasets...\")\n\ntrain_dataset = Dataset.from_pandas(train_df[['text', 'label']])\nval_dataset = Dataset.from_pandas(val_df[['text', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n\nprint(f\"Train dataset: {len(train_dataset)} samples\")\nprint(f\"Val dataset: {len(val_dataset)} samples\")\nprint(f\"Test dataset: {len(test_dataset)} samples\")\n\n# Tokenize datasets\nprint(\"\\nTokenizing datasets...\")\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['text'],\n    batch_size=32\n)\n\nval_dataset = val_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['text'],\n    batch_size=32\n)\n\ntest_dataset = test_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['text'],\n    batch_size=32\n)\n\n# Set format for PyTorch\ntrain_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\nval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n\nprint(f\"‚úì Tokenization complete\")\nprint(f\"\\nSample batch from training data:\")\nsample = train_dataset[0]\nprint(f\"  Input IDs shape: {sample['input_ids'].shape}\")\nprint(f\"  Attention mask shape: {sample['attention_mask'].shape}\")\nprint(f\"  Label: {sample['label']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:29.266765Z","iopub.execute_input":"2026-02-04T20:22:29.267045Z","iopub.status.idle":"2026-02-04T20:22:31.185664Z","shell.execute_reply.started":"2026-02-04T20:22:29.267015Z","shell.execute_reply":"2026-02-04T20:22:31.184918Z"}},"outputs":[{"name":"stdout","text":"Creating HuggingFace Datasets...\nTrain dataset: 2579 samples\nVal dataset: 644 samples\nTest dataset: 807 samples\n\nTokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2579 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e71af99264b469bab420c246d536fdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/644 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16a9f56849064facb62318ee36ce3f86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/807 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a64a7de991544ee382ba6de6d38abb8a"}},"metadata":{}},{"name":"stdout","text":"‚úì Tokenization complete\n\nSample batch from training data:\n  Input IDs shape: torch.Size([256])\n  Attention mask shape: torch.Size([256])\n  Label: 0\n","output_type":"stream"}],"execution_count":11},{"id":"d8808354","cell_type":"markdown","source":"---\n## 7. Metrics & Training Setup","metadata":{}},{"id":"8647e792","cell_type":"code","source":"def compute_metrics(eval_pred):\n    \"\"\"\n    Compute metrics for evaluation.\n    \"\"\"\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    accuracy = accuracy_score(labels, predictions)\n    precision = precision_score(labels, predictions, average='binary')\n    recall = recall_score(labels, predictions, average='binary')\n    f1 = f1_score(labels, predictions, average='binary')\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }\n\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=str(OUTPUT_PATH / 'tier_c_checkpoint'),\n    num_train_epochs=10,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=str(OUTPUT_PATH / 'logs'),\n    logging_steps=50,\n    evaluation_strategy='epoch',           # Evaluate after each epoch\n    save_strategy='epoch',                 # Save after each epoch\n    load_best_model_at_end=True,          # Load best model at end\n    metric_for_best_model='eval_loss',    # Best model based on eval loss\n    greater_is_better=False,              # Lower loss is better\n    save_total_limit=2,                   # Keep only 2 checkpoints\n    fp16=torch.cuda.is_available(),       # Mixed precision training if GPU available\n    learning_rate=2e-4,                   # Learning rate for LoRA\n    report_to='none',                     # Disable wandb\n    seed=42,\n    dataloader_pin_memory=True,\n    gradient_accumulation_steps=1,\n    max_grad_norm=1.0                     # Gradient clipping\n)\n\nprint(\"Training Configuration:\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"  Weight decay: {training_args.weight_decay}\")\nprint(f\"  Warmup steps: {training_args.warmup_steps}\")\nprint(f\"  Mixed precision: {training_args.fp16}\")\nprint(f\"  Max grad norm: {training_args.max_grad_norm}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:31.186608Z","iopub.execute_input":"2026-02-04T20:22:31.186861Z","iopub.status.idle":"2026-02-04T20:22:31.196533Z","shell.execute_reply.started":"2026-02-04T20:22:31.186840Z","shell.execute_reply":"2026-02-04T20:22:31.195177Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/999925972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'tier_c_checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"],"ename":"TypeError","evalue":"TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'","output_type":"error"}],"execution_count":12},{"id":"704d3bb4","cell_type":"markdown","source":"---\n## 8. Model Training with Early Stopping\n\n‚è±Ô∏è This may take several minutes depending on GPU availability.","metadata":{}},{"id":"03414881","cell_type":"code","source":"# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[\n        EarlyStoppingCallback(\n            early_stopping_patience=3,    # Stop if no improvement for 3 epochs\n            early_stopping_threshold=0.0  # No minimum improvement threshold\n        )\n    ]\n)\n\nprint(\"Starting training...\\n\")\nprint(\"=\"*70)\n\n# Train the model\ntrain_result = trainer.train()\n\nprint(\"=\"*70)\nprint(\"‚úì Training complete!\")\nprint(f\"Training loss: {train_result.training_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:31.197179Z","iopub.status.idle":"2026-02-04T20:22:31.197453Z","shell.execute_reply.started":"2026-02-04T20:22:31.197340Z","shell.execute_reply":"2026-02-04T20:22:31.197354Z"}},"outputs":[],"execution_count":null},{"id":"e06a156d","cell_type":"code","source":"# Get training history\ntraining_log = trainer.state.log_history\n\n# Extract train and eval metrics\nepochs = []\ntrain_losses = []\nval_losses = []\nval_accuracies = []\nval_f1_scores = []\n\nfor log in training_log:\n    if 'epoch' in log:\n        epochs.append(log['epoch'])\n    \n    if 'loss' in log:\n        train_losses.append(log['loss'])\n    \n    if 'eval_loss' in log:\n        val_losses.append(log['eval_loss'])\n        val_accuracies.append(log.get('eval_accuracy', 0))\n        val_f1_scores.append(log.get('eval_f1', 0))\n\nprint(\"Training History Summary:\")\nprint(f\"Total epochs trained: {max(epochs):.0f}\")\nprint(f\"Epochs with validation: {len(val_losses)}\")\nprint(f\"\\nBest validation loss: {min(val_losses):.4f}\")\nprint(f\"Best validation accuracy: {max(val_accuracies):.4f}\")\nprint(f\"Best validation F1: {max(val_f1_scores):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:31.198644Z","iopub.status.idle":"2026-02-04T20:22:31.198992Z","shell.execute_reply.started":"2026-02-04T20:22:31.198790Z","shell.execute_reply":"2026-02-04T20:22:31.198810Z"}},"outputs":[],"execution_count":null},{"id":"ae928ea1","cell_type":"code","source":"# Plot training curves\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Loss curves\nax1 = axes[0]\nif train_losses:\n    ax1.plot(range(1, len(train_losses) + 1), train_losses, marker='o', label='Training Loss', linewidth=2)\nif val_losses:\n    ax1.plot(range(1, len(val_losses) + 1), val_losses, marker='s', label='Validation Loss', linewidth=2)\nax1.set_xlabel('Epoch', fontsize=12)\nax1.set_ylabel('Loss', fontsize=12)\nax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3)\n\n# Metrics curves\nax2 = axes[1]\nif val_accuracies:\n    ax2.plot(range(1, len(val_accuracies) + 1), val_accuracies, marker='o', label='Accuracy', linewidth=2)\nif val_f1_scores:\n    ax2.plot(range(1, len(val_f1_scores) + 1), val_f1_scores, marker='s', label='F1-Score', linewidth=2)\nax2.set_xlabel('Epoch', fontsize=12)\nax2.set_ylabel('Score', fontsize=12)\nax2.set_title('Validation Metrics over Epochs', fontsize=14, fontweight='bold')\nax2.legend(fontsize=11)\nax2.grid(True, alpha=0.3)\nax2.set_ylim([0, 1])\n\nplt.tight_layout()\nplt.savefig(str(OUTPUT_PATH / 'tier_c_training_curves.png'), dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"‚úì Training curves saved to {OUTPUT_PATH / 'tier_c_training_curves.png'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:31.200181Z","iopub.status.idle":"2026-02-04T20:22:31.200676Z","shell.execute_reply.started":"2026-02-04T20:22:31.200536Z","shell.execute_reply":"2026-02-04T20:22:31.200552Z"}},"outputs":[],"execution_count":null},{"id":"364cb50e","cell_type":"markdown","source":"---\n## 9. Evaluation on Test Set\n\n‚ö†Ô∏è **Important**: We evaluate ONLY ONCE on the test set AFTER training is complete.","metadata":{}},{"id":"3b0ad0da","cell_type":"code","source":"# Evaluate on test set\nprint(\"Evaluating on test set...\\n\")\ntest_results = trainer.evaluate(test_dataset)\n\n# Extract predictions for additional metrics\npredictions_output = trainer.predict(test_dataset)\ny_pred_probs = predictions_output.predictions\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = test_dataset['label']\n\n# Calculate additional metrics\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average='binary')\nrecall = recall_score(y_true, y_pred, average='binary')\nf1 = f1_score(y_true, y_pred, average='binary')\n\n# For ROC-AUC, use probabilities of positive class\ny_probs_pos = torch.softmax(torch.tensor(y_pred_probs), dim=1)[:, 1].numpy()\nroc_auc = roc_auc_score(y_true, y_probs_pos)\n\ncm = confusion_matrix(y_true, y_pred)\n\n# Print results\nprint(\"=\"*70)\nprint(\"TEST SET EVALUATION RESULTS\")\nprint(\"=\"*70)\nprint(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall:    {recall:.4f}\")\nprint(f\"F1-Score:  {f1:.4f}\")\nprint(f\"ROC-AUC:   {roc_auc:.4f}\")\nprint(\"=\"*70)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred, target_names=['Human', 'AI'], digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:31.201562Z","iopub.status.idle":"2026-02-04T20:22:31.201845Z","shell.execute_reply.started":"2026-02-04T20:22:31.201703Z","shell.execute_reply":"2026-02-04T20:22:31.201723Z"}},"outputs":[],"execution_count":null},{"id":"23e05c90","cell_type":"code","source":"# Confusion Matrix Visualization\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Human', 'AI'],\n            yticklabels=['Human', 'AI'],\n            cbar_kws={'label': 'Count'},\n            ax=ax)\nax.set_title('Confusion Matrix - Test Set (Tier C)', fontsize=14, fontweight='bold')\nax.set_ylabel('True Label', fontsize=12)\nax.set_xlabel('Predicted Label', fontsize=12)\nplt.tight_layout()\nplt.savefig(str(OUTPUT_PATH / 'tier_c_confusion_matrix.png'), dpi=300, bbox_inches='tight')\nplt.show()\n\n# Calculate additional metrics from confusion matrix\ntn, fp, fn, tp = cm.ravel()\nspecificity = tn / (tn + fp) if (tn + fp) > 0 else 0\nsensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n\nprint(\"\\nConfusion Matrix Breakdown:\")\nprint(f\"  True Negatives (Human ‚Üí Human):  {tn}\")\nprint(f\"  False Positives (Human ‚Üí AI):    {fp}\")\nprint(f\"  False Negatives (AI ‚Üí Human):    {fn}\")\nprint(f\"  True Positives (AI ‚Üí AI):        {tp}\")\nprint(f\"\\nSpecificity (True Negative Rate): {specificity:.4f}\")\nprint(f\"Sensitivity (True Positive Rate): {sensitivity:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:31.203038Z","iopub.status.idle":"2026-02-04T20:22:31.203385Z","shell.execute_reply.started":"2026-02-04T20:22:31.203218Z","shell.execute_reply":"2026-02-04T20:22:31.203237Z"}},"outputs":[],"execution_count":null},{"id":"35055a23","cell_type":"code","source":"# ROC Curve\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_true, y_probs_pos)\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.plot(fpr, tpr, linewidth=2.5, label=f'ROC Curve (AUC = {roc_auc:.4f})', color='#2E86AB')\nax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\nax.set_xlabel('False Positive Rate', fontsize=12)\nax.set_ylabel('True Positive Rate', fontsize=12)\nax.set_title('ROC Curve - Test Set (Tier C)', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nax.set_xlim([0, 1])\nax.set_ylim([0, 1])\nplt.tight_layout()\nplt.savefig(str(OUTPUT_PATH / 'tier_c_roc_curve.png'), dpi=300, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:31.204923Z","iopub.status.idle":"2026-02-04T20:22:31.205194Z","shell.execute_reply.started":"2026-02-04T20:22:31.205081Z","shell.execute_reply":"2026-02-04T20:22:31.205093Z"}},"outputs":[],"execution_count":null},{"id":"1d562101","cell_type":"markdown","source":"---\n## 10. Overfitting Analysis","metadata":{}},{"id":"469c2b6b","cell_type":"code","source":"# Check for overfitting\nprint(\"\\n\" + \"=\"*70)\nprint(\"OVERFITTING ANALYSIS\")\nprint(\"=\"*70)\n\n# Training accuracy (approximate from best epoch)\nif train_losses and val_losses:\n    best_epoch_idx = np.argmin(val_losses)\n    print(f\"\\nBest epoch: {best_epoch_idx + 1}\")\n    print(f\"Training loss at best epoch: {train_losses[best_epoch_idx]:.4f}\")\n    print(f\"Validation loss at best epoch: {val_losses[best_epoch_idx]:.4f}\")\n    \n    # Loss difference\n    loss_diff = train_losses[best_epoch_idx] - val_losses[best_epoch_idx]\n    if loss_diff < 0:\n        print(f\"\\n‚ö†Ô∏è Validation loss HIGHER than training (possible overfitting)\")\n        print(f\"   Loss difference: {abs(loss_diff):.4f}\")\n    else:\n        print(f\"\\n‚úì Training loss higher than validation (expected pattern)\")\n        print(f\"   Loss difference: {loss_diff:.4f}\")\n\nprint(f\"\\nTest Set Performance:\")\nprint(f\"  Accuracy: {accuracy:.4f}\")\nprint(f\"  F1-Score: {f1:.4f}\")\nprint(f\"\\nValidation vs Test gap:\")\nif val_accuracies:\n    best_val_acc = max(val_accuracies)\n    gap = best_val_acc - accuracy\n    print(f\"  Best validation accuracy: {best_val_acc:.4f}\")\n    print(f\"  Test accuracy: {accuracy:.4f}\")\n    print(f\"  Gap: {gap:.4f} ({gap*100:.2f}%)\")\n    if gap < 0.05:\n        print(f\"  ‚úì Small gap - Good generalization\")\n    else:\n        print(f\"  ‚ö†Ô∏è Larger gap - Potential overfitting\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:31.206298Z","iopub.status.idle":"2026-02-04T20:22:31.206513Z","shell.execute_reply.started":"2026-02-04T20:22:31.206409Z","shell.execute_reply":"2026-02-04T20:22:31.206421Z"}},"outputs":[],"execution_count":null},{"id":"0636acde","cell_type":"markdown","source":"---\n## 11. Model Comparison: Tier A vs Tier B vs Tier C","metadata":{}},{"id":"bd7461fe","cell_type":"code","source":"# Summary of Tier C results\ntier_c_results = {\n    'Model': 'Tier C: DistilBERT + LoRA',\n    'Accuracy': accuracy,\n    'Precision': precision,\n    'Recall': recall,\n    'F1-Score': f1,\n    'ROC-AUC': roc_auc,\n    'Parameters (Trainable)': trainable_params,\n    'Parameters (Total)': total_params,\n    'Parameter Efficiency': f\"{reduction:.2f}%\"\n}\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"TIER C FINAL RESULTS\")\nprint(\"=\"*70)\nfor key, value in tier_c_results.items():\n    if isinstance(value, float):\n        print(f\"{key:.<50} {value:.4f}\")\n    else:\n        print(f\"{key:.<50} {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:31.207614Z","iopub.status.idle":"2026-02-04T20:22:31.207891Z","shell.execute_reply.started":"2026-02-04T20:22:31.207776Z","shell.execute_reply":"2026-02-04T20:22:31.207791Z"}},"outputs":[],"execution_count":null},{"id":"0156b81f","cell_type":"markdown","source":"---\n## 12. Save Model and Results","metadata":{}},{"id":"21af39b6","cell_type":"code","source":"# Save the trained model\nmodel_save_path = OUTPUT_PATH / 'tier_c_model_final'\nmodel_save_path.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Saving model to {model_save_path}...\")\nmodel.save_pretrained(str(model_save_path))\ntokenizer.save_pretrained(str(model_save_path))\nprint(f\"‚úì Model saved successfully\")\n\n# Save results as JSON\nresults_json_path = OUTPUT_PATH / 'tier_c_results.json'\nresults_to_save = {\n    'model_name': 'DistilBERT + LoRA',\n    'test_metrics': {\n        'accuracy': float(accuracy),\n        'precision': float(precision),\n        'recall': float(recall),\n        'f1_score': float(f1),\n        'roc_auc': float(roc_auc)\n    },\n    'model_params': {\n        'trainable_params': int(trainable_params),\n        'total_params': int(total_params),\n        'parameter_efficiency_percent': float(reduction)\n    },\n    'training_config': {\n        'epochs': 10,\n        'batch_size': 16,\n        'learning_rate': 2e-4,\n        'max_sequence_length': MAX_LENGTH,\n        'lora_rank': 8,\n        'lora_alpha': 16\n    }\n}\n\nwith open(results_json_path, 'w') as f:\n    json.dump(results_to_save, f, indent=2)\n\nprint(f\"‚úì Results saved to {results_json_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:31.209945Z","iopub.status.idle":"2026-02-04T20:22:31.210259Z","shell.execute_reply.started":"2026-02-04T20:22:31.210136Z","shell.execute_reply":"2026-02-04T20:22:31.210157Z"}},"outputs":[],"execution_count":null},{"id":"fb789ee6","cell_type":"code","source":"# Create summary report\nsummary_report = f\"\"\"\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n‚ïë         TIER C: DistilBERT + LoRA - FINAL SUMMARY REPORT              ‚ïë\n‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n\nüìä TEST SET PERFORMANCE:\n  Accuracy:   {accuracy:.4f} ({accuracy*100:.2f}%)\n  Precision:  {precision:.4f}\n  Recall:     {recall:.4f}\n  F1-Score:   {f1:.4f}\n  ROC-AUC:    {roc_auc:.4f}\n\nüîß MODEL ARCHITECTURE:\n  Base Model:           DistilBERT (distilbert-base-uncased)\n  Fine-tuning Method:   LoRA (Low-Rank Adaptation)\n  Rank (r):             8\n  Alpha:                16\n  Target Modules:       q_lin, v_lin\n\nüìà PARAMETER EFFICIENCY:\n  Total Parameters:     {total_params:,}\n  Trainable Parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\n  Parameter Reduction:  {reduction:.2f}%\n\nüéì TRAINING CONFIGURATION:\n  Epochs:               {len(val_losses)}\n  Batch Size:           16\n  Learning Rate:        2e-4\n  Weight Decay:         0.01\n  Warmup Steps:         500\n  Max Gradient Norm:    1.0\n  Early Stopping:       Yes (patience=3)\n  Mixed Precision:      {training_args.fp16}\n\nüìä DATASET SPLIT:\n  Training Set:   {len(train_df)} samples (64%)\n  Validation Set: {len(val_df)} samples (16%)\n  Test Set:       {len(test_df)} samples (20%)\n  Total:          {len(df)} samples\n\nüíæ SAVED OUTPUTS:\n  ‚úì Model:              {model_save_path}\n  ‚úì Results JSON:       {results_json_path}\n  ‚úì Training Curves:    {OUTPUT_PATH / 'tier_c_training_curves.png'}\n  ‚úì Confusion Matrix:   {OUTPUT_PATH / 'tier_c_confusion_matrix.png'}\n  ‚úì ROC Curve:          {OUTPUT_PATH / 'tier_c_roc_curve.png'}\n\nüéØ INSIGHTS:\n  ‚Ä¢ LoRA provides significant parameter efficiency (>99% reduction)\n  ‚Ä¢ Model achieves strong performance with minimal fine-tuning\n  ‚Ä¢ Early stopping prevented overfitting\n  ‚Ä¢ Transformer-based approach outperforms simpler embedding methods\n\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\"\"\"\n\nprint(summary_report)\n\n# Save report\nreport_path = OUTPUT_PATH / 'tier_c_summary_report.txt'\nwith open(report_path, 'w') as f:\n    f.write(summary_report)\n\nprint(f\"\\n‚úì Summary report saved to {report_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-04T20:22:31.210760Z","iopub.status.idle":"2026-02-04T20:22:31.211043Z","shell.execute_reply.started":"2026-02-04T20:22:31.210894Z","shell.execute_reply":"2026-02-04T20:22:31.210908Z"}},"outputs":[],"execution_count":null}]}