{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9895fc",
   "metadata": {},
   "source": [
    "# TASK ZERO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8de404",
   "metadata": {},
   "source": [
    "### UNDERSTANDING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33700976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f0a54bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read the text files (keeps original behaviour for quick inspection)\n",
    "base_path = r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\novels'\n",
    "\n",
    "# Example: load two files (these variables are used by legacy analysis cells)\n",
    "with open(os.path.join(base_path, 'metamorphosis.txt'), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    metamorphosis_text = f.read()\n",
    "\n",
    "with open(os.path.join(base_path, 'the_trial.txt'), 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    trial_text = f.read()\n",
    "\n",
    "print(\"Files loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7961a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Analysis for: The Metamorphosis\n",
      "==================================================\n",
      "Total characters: 138,259\n",
      "Total words: 25,602\n",
      "Unique words: 3,001\n",
      "\n",
      "Top 10 most common words:\n",
      "  the: 1348\n",
      "  to: 835\n",
      "  and: 710\n",
      "  he: 593\n",
      "  of: 557\n",
      "  his: 550\n",
      "  in: 411\n",
      "  was: 411\n",
      "  it: 385\n",
      "  that: 360\n",
      "\n",
      "==================================================\n",
      "Analysis for: The Trial\n",
      "==================================================\n",
      "Total characters: 469,532\n",
      "Total words: 89,557\n",
      "Unique words: 5,090\n",
      "\n",
      "Top 10 most common words:\n",
      "  the: 4930\n",
      "  to: 2937\n",
      "  he: 2077\n",
      "  and: 2074\n",
      "  of: 1742\n",
      "  it: 1482\n",
      "  that: 1446\n",
      "  a: 1380\n",
      "  you: 1343\n",
      "  in: 1307\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "def analyze_text(text, title):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Analysis for: {title}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total characters: {len(text):,}\")\n",
    "    print(f\"Total words: {len(words):,}\")\n",
    "    print(f\"Unique words: {len(set(words)):,}\")\n",
    "    \n",
    "    # Top 10 most common words\n",
    "    word_counts = Counter(words)\n",
    "    print(f\"\\nTop 10 most common words:\")\n",
    "    for word, count in word_counts.most_common(10):\n",
    "        print(f\"  {word}: {count}\")\n",
    "\n",
    "analyze_text(metamorphosis_text, \"The Metamorphosis\")\n",
    "analyze_text(trial_text, \"The Trial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c19884",
   "metadata": {},
   "source": [
    "### FUNCTIONS FOR CLEANING THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec918c",
   "metadata": {},
   "source": [
    "#### READ + GUTENBERG HEADERS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec13087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_text(path):\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def remove_gutenberg_headers(text):\n",
    "    \"\"\"Strip Project Gutenberg header/footer using common markers and fallbacks.\"\"\"\n",
    "    start = re.search(r\"\\*\\*\\*\\s*START OF (THE )?PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\", text, flags=re.IGNORECASE|re.DOTALL)\n",
    "    end = re.search(r\"\\*\\*\\*\\s*END OF (THE )?PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\", text, flags=re.IGNORECASE|re.DOTALL)\n",
    "    if start and end:\n",
    "        return text[start.end():end.start()]\n",
    "    if start:\n",
    "        return text[start.end():]\n",
    "    if end:\n",
    "        return text[:end.start()]\n",
    "    # fallback: strip leading license block up to first major section marker\n",
    "    fallback = re.search(r\"\\n(?:CONTENTS|TABLE OF CONTENTS|CHAPTER [IVXLC0-9]+\\.|CHAPTER\\s+I\\b|BOOK I\\b)\", text, flags=re.IGNORECASE)\n",
    "    if fallback:\n",
    "        return text[fallback.start():]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e976e",
   "metadata": {},
   "source": [
    "#### BASIC CLEANUP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff3bf567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_cleanup(text):\n",
    "    # normalize newlines\n",
    "    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "    # remove obvious Gutenberg boilerplate lines and short ALL-CAPS headers\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "    for ln in lines:\n",
    "        s = ln.strip()\n",
    "        if not s:\n",
    "            cleaned.append('')\n",
    "            continue\n",
    "        low = s.lower()\n",
    "        if 'project gutenberg' in low or 'this ebook' in low or 'http' in low:\n",
    "            continue\n",
    "        # skip short all-caps lines that are likely headers/footers\n",
    "        if re.match(r'^[A-Z0-9 \\-\\(\\)\\/\\:]{1,60}$', s) and len(s.split()) <= 6:\n",
    "            continue\n",
    "        cleaned.append(ln)\n",
    "    text = '\\n'.join(cleaned)\n",
    "    # collapse multiple blank lines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c843c3",
   "metadata": {},
   "source": [
    "#### REMOVE CHAPTER HEADINGS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fb5bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chapter_headings(text):\n",
    "    # More aggressive removal of chapter/book/part headings in many formats.\n",
    "    # 1) Remove explicit lines starting with chapter/book/part (case-insensitive),\n",
    "    #    but only if they're short (likely a heading) to avoid removing body text.\n",
    "    def _drop_short_heading(m):\n",
    "        line = m.group(0).strip()\n",
    "        words = re.findall(r\"\\w+['-]?\\w*\", line)\n",
    "        if len(words) <= 6:\n",
    "            return ''\n",
    "        return m.group(0)\n",
    "    text = re.sub(r'(?im)^[ \\t]*(?:CHAPTER|CHAP\\.?|BOOK|PART)\\b[^\\n]*\\n', _drop_short_heading, text)\n",
    "\n",
    "    # 2) Remove standalone lines that look like chapter headings even when not\n",
    "    #    fully ALL-CAPS (e.g. \"Chapter Two\", \"Chapter I.\"). Match short lines\n",
    "    #    that begin with chapter/chap/book/part and contain only a few words/punct.\n",
    "    text = re.sub(r\"(?im)^[ \\t]*(?:chapter|chap\\.?|book|part)\\b[\\sA-Za-z0-9\\-\\.,:\\(\\)]+\\n\", '', text)\n",
    "\n",
    "    # 3) Keep the previous heuristic: drop short ALL-CAPS centered headings.\n",
    "    def _drop_if_heading(m):\n",
    "        line = m.group(0).strip()\n",
    "        if len(line) <= 60 and len(line.split()) <= 6 and re.match(r\"^[A-Z0-9 ,:\\'\\\"\\-()]+$\", line):\n",
    "            return ''\n",
    "        return m.group(0)\n",
    "    text = re.sub(r'(?m)^[ \\t]*[A-Z0-9 ,:\\'\\\"\\-()]{1,60}\\n', _drop_if_heading, text)\n",
    "\n",
    "    # Collapse any resulting runs of blank lines introduced by removals\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e06ce8",
   "metadata": {},
   "source": [
    "#### CLEAN TEXT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "908d1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = remove_gutenberg_headers(text)\n",
    "    text = basic_cleanup(text)\n",
    "    text = remove_chapter_headings(text)\n",
    "    # normalize common Unicode punctuation\n",
    "    repl = {'\\u2018': \"'\", '\\u2019': \"'\", '\\u201c': '\"', '\\u201d': '\"', '\\u2013': '-', '\\u2014': ' - '}\n",
    "    for k, v in repl.items():\n",
    "        text = text.replace(k, v)\n",
    "    # normalize spaces\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cd72bd",
   "metadata": {},
   "source": [
    "#### SPLIT SENTENCES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2266a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:18: SyntaxWarning: invalid escape sequence '\\-'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\-'\n",
      "C:\\Users\\eisas\\AppData\\Local\\Temp\\ipykernel_35928\\3026888763.py:18: SyntaxWarning: invalid escape sequence '\\-'\n",
      "  s = s.strip(' \"\\'\\-–—')\n"
     ]
    }
   ],
   "source": [
    "def split_sentences(text, min_words=6):\n",
    "    \"\"\"Heuristic sentence splitter that avoids breaking on many abbreviations.\n",
    "    Returns sentences with at least `min_words` words.\"\"\"\n",
    "    parts = re.split(r'(?<=[\\.\\?\\!])\\s+(?=(?:[\"\\'\\(]?[A-Z0-9]))', text)\n",
    "    if len(parts) < 20:\n",
    "        parts = re.split(r'(?<=[\\.\\?\\!])\\s+', text)\n",
    "    sentences = []\n",
    "    for p in parts:\n",
    "        s = p.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        wc = len(re.findall(r'\\b\\w+\\b', s))\n",
    "        if wc < min_words:\n",
    "            continue\n",
    "        # drop enumerated list items\n",
    "        if re.match(r'^\\d+[\\.)]\\s+', s):\n",
    "            continue\n",
    "        s = s.strip(' \"\\'\\-–—')\n",
    "        sentences.append(s)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b095e14",
   "metadata": {},
   "source": [
    "#### BUILD DATASET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81d2922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_class1_dataset(novels_dir, output_dir, sample_size=100):\n",
    "    novels_dir = Path(novels_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    summary = {}\n",
    "    for txt in sorted(novels_dir.glob('*.txt')):\n",
    "        raw = read_text(txt)\n",
    "        cleaned = clean_text(raw)\n",
    "        sentences = split_sentences(cleaned, min_words=6)\n",
    "        sampled = random.sample(sentences, min(sample_size, len(sentences)))\n",
    "        # write cleaned text and samples\n",
    "        (output_dir / f\"{txt.stem}_cleaned.txt\").write_text(cleaned, encoding='utf-8')\n",
    "        (output_dir / f\"{txt.stem}_samples.json\").write_text(json.dumps({'samples': sampled, 'num_sentences': len(sentences)}, indent=2), encoding='utf-8')\n",
    "        summary[txt.stem] = {'num_sentences': len(sentences), 'num_samples': len(sampled)}\n",
    "    (output_dir / 'task_zero_summary.json').write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2566c8",
   "metadata": {},
   "source": [
    "### FINAL OUTPUT GENERATION FOR DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3226cd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-1 cleaning complete. Summary: {'heart_of_darkness': {'num_sentences': 1829, 'num_samples': 100}, 'lord_jim': {'num_sentences': 5529, 'num_samples': 100}, 'metamorphosis': {'num_sentences': 724, 'num_samples': 100}, 'the_trial': {'num_sentences': 3013, 'num_samples': 100}, 'typhoon': {'num_sentences': 1429, 'num_samples': 100}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base = r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\novels'\n",
    "out = r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\class1'\n",
    "summary = build_class1_dataset(base, out, sample_size=100)\n",
    "print('Class-1 cleaning complete. Summary:', summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802bf490",
   "metadata": {},
   "source": [
    "### FINDING RELEVANT TOPIC ASSOICIATED WITH EACH NOVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f859472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152aa63",
   "metadata": {},
   "source": [
    "#### TF-IDF VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7ddb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences(sentences):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words=\"english\",\n",
    "        min_df=5,        # ignore extremely rare words\n",
    "        max_df=0.7,      # ignore overly common words\n",
    "        max_features=5000\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return tfidf_matrix, feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051baf63",
   "metadata": {},
   "source": [
    "#### KMEANS CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c2b93d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_sentences(tfidf_matrix, k=7):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        random_state=42,\n",
    "        n_init=10\n",
    "    )\n",
    "    \n",
    "    labels = kmeans.fit_predict(tfidf_matrix)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    return labels, centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c442da",
   "metadata": {},
   "source": [
    "#### Extract Top Keywords per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27d185bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_terms_per_cluster(centroids, feature_names, top_n=15):\n",
    "    top_terms = {}\n",
    "    \n",
    "    for cluster_id, centroid in enumerate(centroids):\n",
    "        top_indices = centroid.argsort()[::-1][:top_n]\n",
    "        terms = [feature_names[i] for i in top_indices]\n",
    "        top_terms[cluster_id] = terms\n",
    "    \n",
    "    return top_terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b89797",
   "metadata": {},
   "source": [
    "#### Inspect Clusters (keywords + example sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90ff5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_clusters(sentences, labels, top_terms, samples_per_cluster=5):\n",
    "    for cluster_id, terms in top_terms.items():\n",
    "        print(f\"\\n=== Cluster {cluster_id} ===\")\n",
    "        print(\"Top terms:\", terms)\n",
    "        \n",
    "        cluster_sentences = [\n",
    "            s for s, label in zip(sentences, labels) if label == cluster_id\n",
    "        ]\n",
    "        \n",
    "        print(f\"Number of sentences: {len(cluster_sentences)}\")\n",
    "        print(\"Sample sentences:\")\n",
    "        \n",
    "        for s in cluster_sentences[:samples_per_cluster]:\n",
    "            print(\"-\", s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3ad66",
   "metadata": {},
   "source": [
    "#### Run everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03cff6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics(sentences, k=7):\n",
    "    tfidf_matrix, feature_names = vectorize_sentences(sentences)\n",
    "    labels, centroids = cluster_sentences(tfidf_matrix, k=k)\n",
    "    top_terms = get_top_terms_per_cluster(centroids, feature_names)\n",
    "    \n",
    "    inspect_clusters(sentences, labels, top_terms)\n",
    "    \n",
    "    return labels, top_terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e4f56",
   "metadata": {},
   "source": [
    "#### RUN TOPIC DISCOVERY ON ALL NOVELS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5137367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences: 1,829\n",
      "Discovering 8 topics...\n",
      "\n",
      "\n",
      "=== Cluster 0 ===\n",
      "Top terms: ['said', 'did', 'mr', 'think', 'just', 'company', 'way', 'tone', 'like', 'good', 'talked', 'attack', 'suddenly', 'tell', 've']\n",
      "Number of sentences: 118\n",
      "Sample sentences:\n",
      "- Between us there was, as I have already said somewhere, the bond of the\n",
      "sea.\n",
      "- For some reason or\n",
      "other we did not begin that game of dominoes.\n",
      "- And this also,\" said Marlow suddenly, \"has been one of the dark places\n",
      "of the earth.\"\n",
      "\n",
      "He was the only man of us who still \"followed the sea.\" The worst that\n",
      "could be said of him was that he did not represent his class.\n",
      "- His remark did not seem at all surprising.\n",
      "- No one took the trouble to grunt even; and\n",
      "presently he said, very slow - \"I was thinking of very old times, when\n",
      "the Romans first came here, nineteen hundred years ago - the other day\n",
      "....\n",
      "\n",
      "=== Cluster 1 ===\n",
      "Top terms: ['like', 'little', 'came', 'river', 'men', 'think', 'got', 'eyes', 'black', 'look', 'great', 'going', 'long', 'station', 'things']\n",
      "Number of sentences: 1262\n",
      "Sample sentences:\n",
      "- Heart of Darkness\n",
      "\n",
      "by Joseph Conrad\n",
      "\n",
      "Contents\n",
      "\n",
      "The Nellie, a cruising yawl, swung to her anchor without a flutter of\n",
      "the sails, and was at rest.\n",
      "- The flood had made, the wind was nearly\n",
      "calm, and being bound down the river, the only thing for it was to come\n",
      "to and wait for the turn of the tide.\n",
      "- The sea-reach of the Thames stretched before us like the beginning of\n",
      "an interminable waterway.\n",
      "- In the offing the sea and the sky were welded\n",
      "together without a joint, and in the luminous space the tanned sails of\n",
      "the barges drifting up with the tide seemed to stand still in red\n",
      "clusters of canvas sharply peaked, with gleams of varnished sprits.\n",
      "- A\n",
      "haze rested on the low shores that ran out to sea in vanishing\n",
      "flatness.\n",
      "\n",
      "=== Cluster 2 ===\n",
      "Top terms: ['know', 'did know', 'did', 'man', 'turned', 'course', 'darkness', 'things', 'forget', 'doing', 'used', 'said', 'yes', 'felt', 'coming']\n",
      "Number of sentences: 57\n",
      "Sample sentences:\n",
      "- Or think of a decent young citizen in a toga - perhaps too\n",
      "much dice, you know - coming out here in the train of some prefect, or\n",
      "tax-gatherer, or trader even, to mend his fortunes.\n",
      "- The\n",
      "fascination of the abomination - you know, imagine the growing regrets,\n",
      "the longing to escape, the powerless disgust, the surrender, the hate.\"\n",
      "\n",
      "He paused.\n",
      "- I was not used to get things that way, you know.\n",
      "- I know the wife of a very high\n",
      "personage in the Administration, and also a man who has lots of\n",
      "influence with,' etc.\n",
      "- No doubt he was; but he had been a couple of years\n",
      "already out there engaged in the noble cause, you know, and he probably\n",
      "felt the need at last of asserting his self-respect in some way.\n",
      "\n",
      "=== Cluster 3 ===\n",
      "Top terms: ['don', 'don know', 'know', 'understand', 'don like', 'like', 'don want', 'want', 'said', 'mr', 'man', 'talk', 'cried', 'work', 'little']\n",
      "Number of sentences: 49\n",
      "Sample sentences:\n",
      "- I don't want to bother you much with what happened to me personally,\"\n",
      "he began, showing in this remark the weakness of many tellers of tales\n",
      "who seem so often unaware of what their audience would like best to\n",
      "hear; \"yet to understand the effect of it on me you ought to know how I\n",
      "got out there, what I saw, how I went up that river to the place where\n",
      "I first met the poor chap.\n",
      "- What became of the hens\n",
      "I don't know either.\n",
      "- It was\n",
      "just as though I had been let into some conspiracy - I don't\n",
      "know - something not quite right; and I was glad to get out.\n",
      "- In the course of these confidences it became quite plain to\n",
      "me I had been represented to the wife of the high dignitary, and\n",
      "goodness knows to how many more people besides, as an exceptional and\n",
      "gifted creature - a piece of good fortune for the Company - a man you don't\n",
      "get hold of every day.\n",
      "- In the street - I don't know why - a queer\n",
      "feeling came to me that I was an imposter.\n",
      "\n",
      "=== Cluster 4 ===\n",
      "Top terms: ['time', 'heard', 'saw', 'did', 'long', 'got', 'oh', 'mr', 'took', 'voice', 'said', 'came', 'say', 'brother', 'seaman']\n",
      "Number of sentences: 87\n",
      "Sample sentences:\n",
      "- It had\n",
      "borne all the ships whose names are like jewels flashing in the night\n",
      "of time, from the _Golden Hind_ returning with her rotund flanks full\n",
      "of treasure, to be visited by the Queen's Highness and thus pass out of\n",
      "the gigantic tale, to the _Erebus_ and _Terror_, bound on other\n",
      "conquests - and that never returned.\n",
      "- Did it very well, too, no doubt, and without thinking much about it\n",
      "either, except afterwards to brag of what he had gone through in his\n",
      "time, perhaps.\n",
      "- It was very fine for a time, but after a bit I did get\n",
      "tired of resting.\n",
      "- At that time there were many blank\n",
      "spaces on the earth, and when I saw one that looked particularly\n",
      "inviting on a map (but they all look that) I would put my finger on it\n",
      "and say, 'When I grow up I will go there.' The North Pole was one of\n",
      "these places, I remember.\n",
      "- True, by this time it was not a blank space any more.\n",
      "\n",
      "=== Cluster 5 ===\n",
      "Top terms: ['manager', 'head', 'lifted', 'pilgrims', 'appeared', 'said', 'saw', 'right', 'house', 'doorway', 'lifting', 'board', 'tone', 'vanished', 'just']\n",
      "Number of sentences: 63\n",
      "Sample sentences:\n",
      "- A door opened, a white-haired secretarial head, but wearing a\n",
      "compassionate expression, appeared, and a skinny forefinger beckoned me\n",
      "into the sanctuary.\n",
      "- She wore a starched white\n",
      "affair on her head, had a wart on one cheek, and silver-rimmed\n",
      "spectacles hung on the tip of her nose.\n",
      "- Good, good for there,' he mumbled, and then with a certain\n",
      "eagerness asked me whether I would let him measure my head.\n",
      "- As we left the miserable little wharf, he tossed his head\n",
      "contemptuously at the shore.\n",
      "- A slight clinking behind me made me turn my head.\n",
      "\n",
      "=== Cluster 6 ===\n",
      "Top terms: ['man', 'looked', 'white', 'white man', 'little', 'look', 'great', 'just', 'did', 'arm', 'hair', 'said', 'eyes', 'like', 'long']\n",
      "Number of sentences: 109\n",
      "Sample sentences:\n",
      "- On the whole river there was nothing that looked half so\n",
      "nautical.\n",
      "- And\n",
      "indeed nothing is easier for a man who has, as the phrase goes,\n",
      "\"followed the sea\" with reverence and affection, than to evoke the\n",
      "great spirit of the past upon the lower reaches of the Thames.\n",
      "- Sand-banks, marshes, forests, savages, - precious little to eat fit for a\n",
      "civilized man, nothing but Thames water to drink.\n",
      "- And as I looked at the map of it in a\n",
      "shop-window, it fascinated me as a snake would a bird - a silly little\n",
      "bird.\n",
      "- Therefore he whacked the old nigger mercilessly, while a big crowd of\n",
      "his people watched him, thunderstruck, till some man - I was told the\n",
      "chief's son - in desperation at hearing the old chap yell, made a\n",
      "tentative jab with a spear at the white man - and of course it went quite\n",
      "easy between the shoulder-blades.\n",
      "\n",
      "=== Cluster 7 ===\n",
      "Top terms: ['kurtz', 'did', 'time', 'mr', 'station', 'man', 'good', 'soon', 'remarkable', 'thought', 'day', 'talked', 'said', 'understand', 'ordered']\n",
      "Number of sentences: 84\n",
      "Sample sentences:\n",
      "- Kurtz.' On my asking who Mr.\n",
      "- Kurtz was, he said\n",
      "he was a first-class agent; and seeing my disappointment at this\n",
      "information, he added slowly, laying down his pen, 'He is a very\n",
      "remarkable person.' Further questions elicited from him that Mr.\n",
      "- Kurtz\n",
      "was at present in charge of a trading-post, a very important one, in\n",
      "the true ivory-country, at 'the very bottom of there.\n",
      "- Kurtz'\n",
      "he went on, 'tell him from me that everything here' - he glanced at the\n",
      "deck - 'is very satisfactory.\n",
      "- Kurtz was the best agent he had,\n",
      "an exceptional man, of the greatest importance to the Company;\n",
      "therefore I could understand his anxiety.\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences: 5,529\n",
      "Discovering 8 topics...\n",
      "\n",
      "\n",
      "=== Cluster 0 ===\n",
      "Top terms: ['head', 'hands', 'like', 'hand', 'eyes', 'feet', 'said', 'lifted head', 'little', 'lifted', 'shook head', 'saw', 'shook', 'sat', 'right']\n",
      "Number of sentences: 198\n",
      "Sample sentences:\n",
      "- He was an inch, perhaps two, under six feet, powerfully built, and he\n",
      "advanced straight at you with a slight stoop of the shoulders, head\n",
      "forward, and a fixed from-under stare which made you think of a charging\n",
      "bull.\n",
      "- Having a steady head with an\n",
      "excellent physique, he was very smart aloft.\n",
      "- The old training-ship chained to her moorings\n",
      "quivered all over, bowing gently head to wind, and with her scanty\n",
      "rigging humming in a deep bass the breathless song of her youth at sea.\n",
      "- He narrated: 'I just saw his head\n",
      "bobbing, and I dashed my boat-hook in the water.\n",
      "- They came covered with dust, with sweat, with grime, with\n",
      "rags--the strong men at the head of family parties, the lean old men\n",
      "pressing forward without hope of return; young boys with fearless eyes\n",
      "glancing curiously, shy little girls with tumbled long hair; the timid\n",
      "women muffled up and clasping to their breasts, wrapped in loose ends of\n",
      "soiled head-cloths, their sleeping babies, the unconscious pilgrims of\n",
      "an exacting belief.\n",
      "\n",
      "=== Cluster 1 ===\n",
      "Top terms: ['life', 'man', 'said', 'jim', 'sea', 'men', 'way', 'told', 'death', 'good', 'like', 'want', 'know', 'hope', 'glamour']\n",
      "Number of sentences: 146\n",
      "Sample sentences:\n",
      "- I am a great foe to favouritism in public life, in private life,\n",
      "and even in the delicate relationship of an author to his works.\n",
      "- He could see the big ships departing,\n",
      "the broad-beamed ferries constantly on the move, the little boats\n",
      "floating far below his feet, with the hazy splendour of the sea in the\n",
      "distance, and the hope of a stirring life in the world of adventure.\n",
      "- On the lower deck in the babel of two hundred voices he would forget\n",
      "himself, and beforehand live in his mind the sea-life of light\n",
      "literature.\n",
      "- Yet he could not go back, because there is nothing more enticing,\n",
      "disenchanting, and enslaving than the life at sea.\n",
      "- They were the best parts of life, its secret truth, its\n",
      "hidden reality.\n",
      "\n",
      "=== Cluster 2 ===\n",
      "Top terms: ['old', 'old man', 'man', 'old chap', 'chap', 'son', 'doramin', 'good old', 'old nakhoda', 'nakhoda', 'like', 'captain', 'good', 'eyes', 'woman']\n",
      "Number of sentences: 137\n",
      "Sample sentences:\n",
      "- Old Symons is a fine old chap.\n",
      "- Old Symons is awfully\n",
      "excitable--isn't he?\n",
      "- The Patna was a local steamer as old as the hills, lean like a\n",
      "greyhound, and eaten up with rust worse than a condemned water-tank.\n",
      "- The well-to-do had made for their families shelters with\n",
      "heavy boxes and dusty mats; the poor reposed side by side with all they\n",
      "had on earth tied up in a rag under their heads; the lone old men slept,\n",
      "with drawn-up legs, upon their prayer-carpets, with their hands over\n",
      "their ears and one elbow on each side of the face; a father, his\n",
      "shoulders up and his knees under his forehead, dozed dejectedly by a\n",
      "boy who slept on his back with tousled hair and one arm commandingly\n",
      "extended; a woman covered from head to foot, like a corpse, with a piece\n",
      "of white sheeting, had a naked child in the hollow of each arm; the\n",
      "Arab's belongings, piled right aft, made a heavy mound of broken\n",
      "outlines, with a cargo-lamp swung above, and a great confusion of\n",
      "vague forms behind: gleams of paunchy brass pots, the foot-rest of a\n",
      "deck-chair, blades of spears, the straight scabbard of an old sword\n",
      "leaning against a heap of pillows, the spout of a tin coffee-pot.\n",
      "- The chief had given him a four-finger\n",
      "nip about ten o'clock--'only one, s'elp me!'--good old chief; but as to\n",
      "getting the old fraud out of his bunk--a five-ton crane couldn't do\n",
      "it.\n",
      "\n",
      "=== Cluster 3 ===\n",
      "Top terms: ['like', 'man', 'jim', 'did', 'little', 'men', 'way', 'thing', 'say', 'come', 'brown', 'eyes', 'came', 'think', 'away']\n",
      "Number of sentences: 4155\n",
      "Sample sentences:\n",
      "- Produced by Forrest Wasserman and David Widger\n",
      "\n",
      "AUTHOR'S NOTE\n",
      "\n",
      "When this novel first appeared in book form a notion got about that\n",
      "I had been bolted away with.\n",
      "- Some reviewers maintained that the work\n",
      "starting as a short story had got beyond the writer's control.\n",
      "- One or\n",
      "two discovered internal evidence of the fact, which seemed to amuse\n",
      "them.\n",
      "- They pointed out the limitations of the narrative form.\n",
      "- After thinking it over for something like sixteen years, I am not so\n",
      "sure about that.\n",
      "\n",
      "=== Cluster 4 ===\n",
      "Top terms: ['said', 'voice', 'jim', 'did', 'man', 'time', 'yes', 'said jim', 'like', 'good', 'don', 'tone', 'shall', 'come', 'look']\n",
      "Number of sentences: 301\n",
      "Sample sentences:\n",
      "- It was not, they said, very credible.\n",
      "- You know,' she said, 'it is all so\n",
      "morbid.'\n",
      "\n",
      "The pronouncement gave me food for an hour's anxious thought.\n",
      "- They said\n",
      "'Confounded fool!' as soon as his back was turned.\n",
      "- Look at dese cattle,' said the German skipper to his new chief mate.\n",
      "- Hot is no name\n",
      "for it down below,' said a voice.\n",
      "\n",
      "=== Cluster 5 ===\n",
      "Top terms: ['tamb', 'tamb itam', 'itam', 'master', 'jim', 'said', 'saw', 'door', 'girl', 'dain waris', 'dain', 'canoe', 'waris', 'time', 'stood']\n",
      "Number of sentences: 74\n",
      "Sample sentences:\n",
      "- The\n",
      "third man in, it seems, had been Tamb' Itam, Jim's own servant.\n",
      "- The very Tamb' Itam, marching on our journeys upon the heels of his\n",
      "white lord, with his head thrown back, truculent and be-weaponed like a\n",
      "janissary, with kriss, chopper, and lance (besides carrying Jim's gun);\n",
      "even Tamb' Itam allowed himself to put on the airs of uncompromising\n",
      "guardianship, like a surly devoted jailer ready to lay down his life for\n",
      "his captive.\n",
      "- Later on, tossing on my bed under the mosquito-net, I\n",
      "was sure to hear slight creakings, faint breathing, a throat cleared\n",
      "cautiously--and I would know that Tamb' Itam was still on the prowl.\n",
      "- In the middle of a hedged path I saw the\n",
      "arrested, gaunt, watchful, and apparently one-legged silhouette of Tamb'\n",
      "Itam; and across the dusky space my eye detected something white moving\n",
      "to and fro behind the supports of the roof.\n",
      "- As soon as Jim, with Tamb'\n",
      "Itam at his heels, had started upon his evening rounds, I went up to the\n",
      "house alone, and, unexpectedly, found myself waylaid by the girl, who\n",
      "had been clearly waiting for this opportunity.\n",
      "\n",
      "=== Cluster 6 ===\n",
      "Top terms: ['know', 'time', 'don', 'don know', 'think', 'don think', 'like', 'want', 'did', 'man', 'jim', 'say', 'little', 'don want', 'men']\n",
      "Number of sentences: 380\n",
      "Sample sentences:\n",
      "- They\n",
      "argued that no man could have been expected to talk all that time, and\n",
      "other men to listen so long.\n",
      "- As to the mere physical possibility\n",
      "we all know that some speeches in Parliament have taken nearer six than\n",
      "three hours in delivery; whereas all that part of the book which is\n",
      "Marlow's narrative can be read through aloud, I should say, in less than\n",
      "three hours.\n",
      "- After writing a few pages,\n",
      "however, I became for some reason discontented and I laid them aside for\n",
      "a time.\n",
      "- But all these preliminary moods\n",
      "and stirrings of spirit were rather obscure at the time, and they do not\n",
      "appear clearer to me now after the lapse of so many years.\n",
      "- As a\n",
      "matter of principle I will have no favourites; but I don't go so far\n",
      "as to feel grieved and annoyed by the preference some people give to\n",
      "my Lord Jim.\n",
      "\n",
      "=== Cluster 7 ===\n",
      "Top terms: ['went', 'went away', 'jim', 'away', 'tell', 'came', 'like', 'night', 'brierly', 'day', 'people', 'ship went', 'nearly', 'old', 'try']\n",
      "Number of sentences: 138\n",
      "Sample sentences:\n",
      "- It caught in his\n",
      "breeches and I nearly went overboard, as I thought I would, only old\n",
      "Symons let go the tiller and grabbed my legs--the boat nearly swamped.\n",
      "- It went into his leg so far.' He showed the boat-hook, which\n",
      "he had carried below for the purpose, and produced a sensation.\n",
      "- After two years of training he went to sea, and entering the regions so\n",
      "well known to his imagination, found them strangely barren of adventure.\n",
      "- Shut up--and when anything goes wrong you fly to\n",
      "us, don't you?' went on the other.\n",
      "- Jim went on smiling at the retreating\n",
      "horizon; his heart was full of generous impulses, and his thought was\n",
      "contemplating his own superiority.\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences: 724\n",
      "Discovering 7 topics...\n",
      "\n",
      "\n",
      "=== Cluster 0 ===\n",
      "Top terms: ['sister', 'father', 'gregor', 'mother', 'gregor father', 'gregor sister', 'gregor mother', 'mother sister', 'said', 'room', 'soon', 'work', 'did', 'hand', 'father mother']\n",
      "Number of sentences: 95\n",
      "Sample sentences:\n",
      "- Gregor!\" At the\n",
      "other side door his sister came plaintively: \"Gregor?\n",
      "- So why did his sister not go and join the others?\n",
      "- Mother?\"\n",
      "his sister called from the other side.\n",
      "- How had his sister managed to get\n",
      "dressed so quickly?\n",
      "- His father looked hostile, and\n",
      "clenched his fists as if wanting to knock Gregor back into his room.\n",
      "\n",
      "=== Cluster 1 ===\n",
      "Top terms: ['samsa', 'little', 'grete', 'bed', 'time', 'said', 'mr', 'body', 'legs', 'floor', 'just', 'good', 'left', 'quite', 'make']\n",
      "Number of sentences: 304\n",
      "Sample sentences:\n",
      "- He lay on his\n",
      "armour-like back, and if he lifted his head a little he could see his\n",
      "brown belly, slightly domed and divided by arches into stiff sections.\n",
      "- His many legs, pitifully thin compared with the size of the\n",
      "rest of him, waved about helplessly as he looked.\n",
      "- A collection of textile samples lay spread out\n",
      "on the table - Samsa was a travelling salesman - and above it there hung a\n",
      "picture that he had recently cut out of an illustrated magazine and\n",
      "housed in a nice, gilded frame.\n",
      "- It showed a lady fitted out with a fur\n",
      "hat and fur boa who sat upright, raising a heavy fur muff that covered\n",
      "the whole of her lower arm towards the viewer.\n",
      "- Drops of\n",
      "rain could be heard hitting the pane, which made him feel quite sad.\n",
      "\n",
      "=== Cluster 2 ===\n",
      "Top terms: ['chief', 'chief clerk', 'clerk', 'gregor', 'said', 'said chief', 'let', 'father', 'door', 'parents', 'room', 'did', 'left', 'speak', 'voice']\n",
      "Number of sentences: 34\n",
      "Sample sentences:\n",
      "- Gregor only needed to hear the visitor's first words of greeting and he\n",
      "knew who it was - the chief clerk himself.\n",
      "- Was it really not enough to let one\n",
      "of the trainees make enquiries - assuming enquiries were even\n",
      "necessary - did the chief clerk have to come himself, and did they have\n",
      "to show the whole, innocent family that this was so suspicious that\n",
      "only the chief clerk could be trusted to have the wisdom to investigate\n",
      "it?\n",
      "- Something's fallen down in there\", said the chief clerk in the room on\n",
      "the left.\n",
      "- Gregor tried to imagine whether something of the sort that\n",
      "had happened to him today could ever happen to the chief clerk too; you\n",
      "had to concede that it was possible.\n",
      "- But as if in gruff reply to this\n",
      "question, the chief clerk's firm footsteps in his highly polished boots\n",
      "could now be heard in the adjoining room.\n",
      "\n",
      "=== Cluster 3 ===\n",
      "Top terms: ['door', 'open', 'room', 'gregor', 'opened', 'open door', 'shut', 'went', 'door gregor', 'head', 'pressed', 'living room', 'did', 'living', 'straight']\n",
      "Number of sentences: 58\n",
      "Sample sentences:\n",
      "- There was a\n",
      "cautious knock at the door near his head.\n",
      "- His father went back to his breakfast, but his sister\n",
      "whispered: \"Gregor, open the door, I beg of you.\" Gregor, however, had\n",
      "no thought of opening the door, and instead congratulated himself for\n",
      "his cautious habit, acquired from his travelling, of locking all doors\n",
      "at night even when he was at home.\n",
      "- Then there was a ring at the door of the flat.\n",
      "- They're not opening the door\",\n",
      "Gregor said to himself, caught in some nonsensical hope.\n",
      "- But then of\n",
      "course, the maid's firm steps went to the door as ever and opened it.\n",
      "\n",
      "=== Cluster 4 ===\n",
      "Top terms: ['thought', 've', 'got', 'gregor', 'understand', 'quite', 'used', 'knew', 'struck', 'easy', 'day', 'hear', 'getting', 'family', 'parents']\n",
      "Number of sentences: 30\n",
      "Sample sentences:\n",
      "- What's happened to me?\" he thought.\n",
      "- How about if I sleep a little bit longer and forget all this\n",
      "nonsense\", he thought, but that was something he was unable to do\n",
      "because he was used to sleeping on his right, and in his present state\n",
      "couldn't get into that position.\n",
      "- Oh, God\", he thought, \"what a strenuous career it is that I've chosen!\n",
      "- Getting up early all the time\",\n",
      "he thought, \"it makes you stupid.\n",
      "- You've got to get enough sleep.\n",
      "\n",
      "=== Cluster 5 ===\n",
      "Top terms: ['way', 'room', 'just', 'gregor', 'think', 'things', 'gregor room', 'got', 'went', 'gentlemen', 'head', 'quite', 'round', 'family', 'did']\n",
      "Number of sentences: 81\n",
      "Sample sentences:\n",
      "- His room, a\n",
      "proper human room although a little too small, lay peacefully between\n",
      "its four familiar walls.\n",
      "- If I didn't\n",
      "have my parents to think about I'd have given in my notice a long time\n",
      "ago, I'd have gone up to the boss and told him just what I think, tell\n",
      "him everything I would, let him know just what I feel.\n",
      "- But when he had at last\n",
      "got his head out of the bed and into the fresh air it occurred to him\n",
      "that if he let himself fall it would be a miracle if his head were not\n",
      "injured, so he became afraid to carry on pushing himself forward the\n",
      "same way.\n",
      "- It took just as much effort to get back to where he had been earlier,\n",
      "but when he lay there sighing, and was once more watching his legs as\n",
      "they struggled against each other even harder than before, if that was\n",
      "possible, he could think of no way of bringing peace and order to this\n",
      "chaos.\n",
      "- He told himself once more that it was not possible for him to\n",
      "stay in bed and that the most sensible thing to do would be to get free\n",
      "of it in whatever way he could at whatever sacrifice.\n",
      "\n",
      "=== Cluster 6 ===\n",
      "Top terms: ['gregor', 'did', 'hardly', 'family', 'time', 'couch', 'heard', 'father', 'money', 'came', 'long', 'little', 'turned', 'room', 'able']\n",
      "Number of sentences: 122\n",
      "Sample sentences:\n",
      "- Metamorphosis\n",
      "\n",
      "by Franz Kafka\n",
      "\n",
      "Translated by David Wyllie\n",
      "\n",
      "One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin.\n",
      "- The bedding was hardly able to cover it and seemed ready to slide off\n",
      "any moment.\n",
      "- Gregor then turned to look out the window at the dull weather.\n",
      "- And even if he did catch the train he would not avoid his\n",
      "boss's anger as the office assistant would have been there to see the\n",
      "five o'clock train go, he would have put in his report about Gregor's\n",
      "not being there a long time ago.\n",
      "- But that would be extremely strained and suspicious as in five\n",
      "years of service Gregor had never once yet been ill.\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences: 3,013\n",
      "Discovering 8 topics...\n",
      "\n",
      "\n",
      "=== Cluster 0 ===\n",
      "Top terms: ['just', 'man', 'did', 'like', 'lawyer', 'hand', 'people', 'quite', 'really', 'make', 'thought', 'trial', 'wanted', 'doorkeeper', 'say']\n",
      "Number of sentences: 1535\n",
      "Sample sentences:\n",
      "- Franz Kafka\n",
      "\n",
      "Translation Copyright © by David Wyllie\n",
      "Translator contact email: dandelion@post.cz\n",
      "\n",
      "Arrest--Conversation with Mrs.\n",
      "- Every day at eight in\n",
      "the morning he was brought his breakfast by Mrs.\n",
      "- Grubach was his landlady--but today she didn't come.\n",
      "- He had never seen the man in this house before.\n",
      "- He\n",
      "was slim but firmly built, his clothes were black and close-fitting,\n",
      "with many folds and pockets, buckles and buttons and a belt, all of\n",
      "which gave the impression of being very practical but without making it\n",
      "very clear what they were actually for.\n",
      "\n",
      "=== Cluster 1 ===\n",
      "Top terms: ['miss', 'bürstner', 'miss bürstner', 'montag', 'miss montag', 'said', 'said miss', 'room', 'grubach', 'went', 'did', 'll', 'know', 'time', 'little']\n",
      "Number of sentences: 90\n",
      "Sample sentences:\n",
      "- Grubach--Then Miss Bürstner\n",
      "\n",
      "Someone must have been telling lies about Josef K., he knew he had done\n",
      "nothing wrong but, one morning, he was arrested.\n",
      "- K. knew very well that this room had recently been let to a\n",
      "typist called 'Miss Bürstner'.\n",
      "- In one corner of the room there were three young people looking at the\n",
      "photographs belonging to Miss Bürstner that had been put into a piece of\n",
      "fabric on the wall.\n",
      "- But\n",
      "instead, he stood up, picked up a hard round hat that was laying on\n",
      "Miss Bürstner's bed and put it carefully onto his head, using both hands\n",
      "as if trying on a new hat.\n",
      "- Before going out the door he asked, \"Is Miss Bürstner home?\" \"No,\" said\n",
      "Mrs.\n",
      "\n",
      "=== Cluster 2 ===\n",
      "Top terms: ['court', 'room', 'court said', 'said', 'offices', 'court offices', 'officials', 'people', 'waiting', 'especially', 'attic', 'free', 'lawyers', 'wanted', 'painter']\n",
      "Number of sentences: 205\n",
      "Sample sentences:\n",
      "- I want to see who that is in the next\n",
      "room, and why it is that Mrs.\n",
      "- Grubach's living room, over-filled with furniture,\n",
      "tablecloths, porcelain and photographs.\n",
      "- You should have stayed in your room!\n",
      "- Go into your room and wait\n",
      "there.\n",
      "- If you carry on having as much good luck as you have\n",
      "been with your arresting officers then you can reckon on things going\n",
      "well with you.\" K. wanted to sit down, but then he saw that, apart from\n",
      "the chair by the window, there was nowhere anywhere in the room where he\n",
      "could sit.\n",
      "\n",
      "=== Cluster 3 ===\n",
      "Top terms: ['way', 'said', 'did', 'help', 'say', 'painter', 'like', 'long', 'want', 'hand', 'don', 'important', 'right', 'people', 'door']\n",
      "Number of sentences: 153\n",
      "Sample sentences:\n",
      "- Grubach has let me be disturbed in this\n",
      "way.\" It immediately occurred to him that he needn't have said this out\n",
      "loud, and that he must to some extent have acknowledged their authority\n",
      "by doing so, but that didn't seem important to him at the time.\n",
      "- It won't do you any good to get us on the wrong side,\n",
      "even if you think it will--we're probably more on your side that anyone\n",
      "else you know!\" \"That's true, you know, you'd better believe it,\" said\n",
      "Franz, holding a cup of coffee in his hand which he did not lift to his\n",
      "mouth but looked at K. in a way that was probably meant to be full of\n",
      "meaning but could not actually be understood.\n",
      "- The\n",
      "way you're carrying on, it's worse than a child.\n",
      "- It's probably exists only in your\n",
      "heads,\" said K., he wanted, in some way, to insinuate his way into the\n",
      "thoughts of the policemen, to re-shape those thoughts to his benefit or\n",
      "to make himself at home there.\n",
      "- You've not behaved towards us the way we\n",
      "deserve after being so good to you, you forget that we, whatever we are,\n",
      "we're still free men and you're not, and that's quite an advantage.\n",
      "\n",
      "=== Cluster 4 ===\n",
      "Top terms: ['door', 'looked', 'opened', 'open', 'room', 'stood', 'round', 'went', 'looked round', 'painter', 'man', 'said', 'did', 'opened door', 'called']\n",
      "Number of sentences: 228\n",
      "Sample sentences:\n",
      "- K. waited a little while, looked from his pillow at the\n",
      "old woman who lived opposite and who was watching him with an\n",
      "inquisitiveness quite unusual for her, and finally, both hungry and\n",
      "disconcerted, rang the bell.\n",
      "- There was immediately a knock at the door\n",
      "and a man entered.\n",
      "- He tried to work\n",
      "out who the man actually was, first in silence, just through\n",
      "observation and by thinking about it, but the man didn't stay still to\n",
      "be looked at for very long.\n",
      "- Instead he went over to the door, opened it\n",
      "slightly, and said to someone who was clearly standing immediately\n",
      "behind it, \"He\n",
      "wants Anna to bring him his breakfast.\" There was a little laughter in\n",
      "the neighbouring room, it was not clear from the sound of it whether\n",
      "there were several people laughing.\n",
      "- The next room, which K. entered more slowly than he had\n",
      "intended, looked at first glance exactly the same as it had the previous\n",
      "evening.\n",
      "\n",
      "=== Cluster 5 ===\n",
      "Top terms: ['asked', 'lawyer', 'leni', 'want', 'asked lawyer', 'like', 'come', 'just', 'asked painter', 'know', 'painter', 'block', 'voice', 'lover', 'man']\n",
      "Number of sentences: 123\n",
      "Sample sentences:\n",
      "- Who are you?\" asked K., sitting\n",
      "half upright in his bed.\n",
      "- And why am I under arrest?\" he then asked.\n",
      "- He only saw her for an\n",
      "instant, for as soon as she recognised K. she was clearly embarrassed,\n",
      "asked for forgiveness and disappeared, closing the door behind her very\n",
      "carefully.\n",
      "- Why didn't she come in?\" he asked.\n",
      "- I may sit down, mayn't I?\" he asked.\n",
      "\n",
      "=== Cluster 6 ===\n",
      "Top terms: ['time', 've', 'don', 'got', 'know', 'long', 've got', 'come', 'just', 'long time', 'like', 'don know', 'said', 'think', 'll']\n",
      "Number of sentences: 272\n",
      "Sample sentences:\n",
      "- That, at\n",
      "least, is how the stranger took it, as he said, \"Don't you think you'd\n",
      "better stay where you are?\" \"I want neither to stay here nor to be\n",
      "spoken to by you until you've introduced yourself.\" \"I meant it for your\n",
      "own good,\" said the stranger and opened the door, this time without\n",
      "being asked.\n",
      "- Proceedings are underway and you'll learn about everything all in\n",
      "good time.\n",
      "- Things have a tendency to go missing in the storeroom, and after\n",
      "a certain amount of time they sell things off, whether the case involved\n",
      "has come to an end or not.\n",
      "- And cases like this can last a long time,\n",
      "especially the ones that have been coming up lately.\n",
      "- He didn't want that to happen again, not\n",
      "this time at least; if they were play-acting he would act along with\n",
      "them.\n",
      "\n",
      "=== Cluster 7 ===\n",
      "Top terms: ['said', 'businessman', 'yes', 'painter', 'said painter', 'said businessman', 'don', 'said lawyer', 'lawyer', 'yes said', 'leni', 'said priest', 'priest', 'judge', 'like']\n",
      "Number of sentences: 407\n",
      "Sample sentences:\n",
      "- The strange man could not have\n",
      "learned anything from it that he hadn't known already, but now he said\n",
      "to K., as if making his report \"It is not possible.\" \"It would be the\n",
      "first time that's happened,\" said K., as he jumped out of bed and\n",
      "quickly pulled on his trousers.\n",
      "- Didn't Franz tell you?\" \"And what\n",
      "is it you want, then?\" said K., looking back and forth between this new\n",
      "acquaintance and the one named Franz, who had remained in the doorway.\n",
      "- Grubach ...,\" said K.,\n",
      "making a movement as if tearing himself away from the two men--even\n",
      "though they were standing well away from him--and wanted to go.\n",
      "- No,\"\n",
      "said the man at the window, who threw his book down on a coffee table\n",
      "and stood up.\n",
      "- You can't go away when you're under arrest.\" \"That's how\n",
      "it seems,\" said K.\n",
      "\n",
      "======================================================================\n",
      "NOVEL: TYPHOON\n",
      "======================================================================\n",
      "Total sentences: 1,429\n",
      "Discovering 8 topics...\n",
      "\n",
      "\n",
      "=== Cluster 0 ===\n",
      "Top terms: ['sir', 'jukes', 'yes sir', 'yes', 'boatswain', 'don', 'body', 'said', 'isn', 'men', 'cried', 'deck', 'asked', 'storm', 'say']\n",
      "Number of sentences: 40\n",
      "Sample sentences:\n",
      "- Young Jukes, the chief\n",
      "mate, attending his commander to the gangway, would sometimes venture\n",
      "to say, with the greatest gentleness, \"Allow me, sir\"--and possessing\n",
      "himself of the umbrella deferentially, would elevate the ferule, shake\n",
      "the folds, twirl a neat furl in a jiffy, and hand it back; going through\n",
      "the performance with a face of such portentous gravity, that Mr.\n",
      "- D'ye hear, Jukes?\"\n",
      "\n",
      "Jukes took care to punctuate these instructions in proper places with\n",
      "the obligatory \"Yes, sir,\" ejaculated without enthusiasm.\n",
      "- By-and-by he says: 'Was that you talking just now in the\n",
      "port alleyway?' 'Yes, sir.' 'With the third engineer?' 'Yes, sir.' He\n",
      "walks off to starboard, and sits under the dodger on a little campstool\n",
      "of his, and for half an hour perhaps he makes no sound, except that I\n",
      "heard him sneeze once.\n",
      "- We shall want them for the\n",
      "next coaling, sir.\"\n",
      "\n",
      "\"What became of the others?\"\n",
      "\n",
      "\"Why, worn out of course, sir.\"\n",
      "\n",
      "Captain MacWhirr, after glaring down irresolutely at his chief mate,\n",
      "disclosed the gloomy and cynical conviction that more than half of them\n",
      "had been lost overboard, \"if only the truth was known,\" and retired\n",
      "to the other end of the bridge.\n",
      "- What was that for?\"\n",
      "\n",
      "\"It's a manner of speaking, sir,\" said Jukes, stolidly.\n",
      "\n",
      "=== Cluster 1 ===\n",
      "Top terms: ['ship', 'rolling', 'water', 'sea', 'moment', 'wind', 'went', 'nan shan', 'nan', 'shan', 'seas', 'motion', 'board', 'deck', 'little']\n",
      "Number of sentences: 83\n",
      "Sample sentences:\n",
      "- I never met anybody personally concerned in this affair, the interest of\n",
      "which for us was, of course, not the bad weather but the extraordinary\n",
      "complication brought into the ship's life at a moment of exceptional\n",
      "stress by the human element below her deck.\n",
      "- A thin silver\n",
      "watch chain looped his waistcoat, and he never left his ship for the\n",
      "shore without clutching in his powerful, hairy fist an elegant umbrella\n",
      "of the very best quality, but generally unrolled.\n",
      "- It was short, and contained the statement:\n",
      "\"We had very fine weather on our passage out.\" But evidently, in the\n",
      "writer's mind, the only important intelligence was to the effect that\n",
      "his captain had, on the very day of writing, entered him regularly on\n",
      "the ship's articles as Ordinary Seaman.\n",
      "- A cross swell had set in from the direction of Formosa Channel about ten\n",
      "o'clock, without disturbing these passengers much, because the Nan-Shan,\n",
      "with her flat bottom, rolling chocks on bilges, and great breadth of\n",
      "beam, had the reputation of an exceptionally steady ship in a sea-way.\n",
      "- She was a good ship, undoubtedly, and not old either.\n",
      "\n",
      "=== Cluster 2 ===\n",
      "Top terms: ['jukes', 'said', 'said jukes', 'face', 'captain', 'wind', 'look', 'directed', 'great', 'head', 'got', 'hands', 'don', 'began', 'know']\n",
      "Number of sentences: 130\n",
      "Sample sentences:\n",
      "- At the news of the contemplated transfer Jukes grew restless, as if\n",
      "under a sense of personal affront.\n",
      "- Well, it looks queer to me,\" burst out Jukes, greatly exasperated, and\n",
      "flung off the bridge.\n",
      "- You were wrong, Jukes. . . .\"\n",
      "\n",
      "\"Well, sir,\" began Jukes, getting up excitedly, \"all I can say--\" He\n",
      "fumbled for the end of the coil of line with trembling hands.\n",
      "- He smiled from on high at Jukes, and went on smoking and glancing about\n",
      "quietly, in the manner of a kind uncle lending an ear to the tale of an\n",
      "excited schoolboy.\n",
      "- Then, greatly amused but impassive, he asked:\n",
      "\n",
      "\"And did you throw up the billet?\"\n",
      "\n",
      "\"No,\" cried Jukes, raising a weary, discouraged voice above the harsh\n",
      "buzz of the Nan-Shan's friction winches.\n",
      "\n",
      "=== Cluster 3 ===\n",
      "Top terms: ['man', 'old', 'old man', 'said', 'sol', 'rout', 'board', 'chief', 'skipper', 'old sol', 'woman', 'coming', 'tell', 'says', 'second']\n",
      "Number of sentences: 71\n",
      "Sample sentences:\n",
      "- Directly I perceived him\n",
      "I could see that he was the man for the situation.\n",
      "- He was a corpulent man, with\n",
      "a gift for sly chaffing, which to the end of his life he exercised\n",
      "in his intercourse with his son, a little pityingly, as if upon a\n",
      "half-witted person.\n",
      "- The fall--taking into\n",
      "account the excellence of the instrument, the time of the year, and\n",
      "the ship's position on the terrestrial globe--was of a nature ominously\n",
      "prophetic; but the red face of the man betrayed no sort of inward\n",
      "disturbance.\n",
      "- He's the very man,\" declared the senior, without a moment's\n",
      "hesitation.\n",
      "- I admit he has nothing of your fancy skipper about him, if that's what\n",
      "you mean,\" said the elder man, curtly.\n",
      "\n",
      "=== Cluster 4 ===\n",
      "Top terms: ['head', 'time', 'deck', 'thought', 'mr', 'good', 'way', 'sea', 'said', 'come', 'second', 'eyes', 'light', 'bridge', 'hands']\n",
      "Number of sentences: 862\n",
      "Sample sentences:\n",
      "- [The other stories included in this volume (\"Amy Foster,\" \"Falk: A\n",
      "Reminiscence,\" and \"To-morrow\") being already available in another\n",
      "volume, have not been entered here.]\n",
      "\n",
      "Far as the mariner on highest mast Can see all around upon the calmed\n",
      "vast, So wide was Neptune's hall . . . -- KEATS\n",
      "\n",
      "The main characteristic of this volume consists in this, that all the\n",
      "stories composing it belong not only to the same period but have been\n",
      "written one after another in the order in which they appear in the book.\n",
      "- The period is that which follows on my connection with Blackwood's\n",
      "Magazine.\n",
      "- I had just finished writing \"The End of the Tether\" and was\n",
      "casting about for some subject which could be developed in a shorter\n",
      "form than the tales in the volume of \"Youth\" when the instance of a\n",
      "steamship full of returning coolies from Singapore to some port in\n",
      "northern China occurred to my recollection.\n",
      "- Years before I had heard\n",
      "it being talked about in the East as a recent occurrence.\n",
      "- It was for us\n",
      "merely one subject of conversation amongst many others of the kind.\n",
      "\n",
      "=== Cluster 5 ===\n",
      "Top terms: ['like', 'man', 'ship', 'arms', 'end', 'head', 'did', 'jukes', 'rush', 'away', 'great', 'went', 'face', 'came', 'open']\n",
      "Number of sentences: 93\n",
      "Sample sentences:\n",
      "- In that company each of us could\n",
      "imagine easily what the whole thing was like.\n",
      "- In\n",
      "these missives could be found sentences like this: \"The heat here is\n",
      "very great.\" Or: \"On Christmas day at 4 P.\n",
      "- The morning was\n",
      "fine, the oily sea heaved without a sparkle, and there was a queer white\n",
      "misty patch in the sky like a halo of the sun.\n",
      "- I don't believe you\n",
      "can make a man like that understand anything.\n",
      "- We have no brass-bound uniforms, but then we are like brothers here,\"\n",
      " he wrote.\n",
      "\n",
      "=== Cluster 6 ===\n",
      "Top terms: ['don', 'right', 'know', 'make', 'want', 'said', 'ship', 'come', 'captain', 'mr', 'say', 'hands', 'think', 'speak', 'head']\n",
      "Number of sentences: 51\n",
      "Sample sentences:\n",
      "- Falk obeys the law\n",
      "of self-preservation without the slightest misgivings as to his right,\n",
      "but at a crucial turn of that ruthlessly preserved life he will not\n",
      "condescend to dodge the truth.\n",
      "- Sigg, you know--and doubtless they'll continue you out\n",
      "there in command,\" said the junior partner.\n",
      "- Don't it make you sick, Mr.\n",
      "- Seems all\n",
      "right to me.\" And he walked across to the end of the bridge to have a\n",
      "good look.\n",
      "- I thought the people ashore would know how to\n",
      "make the local flag.\n",
      "\n",
      "=== Cluster 7 ===\n",
      "Top terms: ['macwhirr', 'captain macwhirr', 'captain', 'jukes', 'said captain', 'said', 'voice', 'come', 'ship', 'gone', 'thought', 'like', 'half', 'room', 'arm']\n",
      "Number of sentences: 99\n",
      "Sample sentences:\n",
      "- What was needed of course was Captain MacWhirr.\n",
      "- I don't mean to\n",
      "say that I ever saw Captain MacWhirr in the flesh, or had ever come in\n",
      "contact with his literal mind and his dauntless temperament.\n",
      "- MacWhirr is\n",
      "not an acquaintance of a few hours, or a few weeks, or a few months.\n",
      "- If it is true that Captain MacWhirr never\n",
      "walked and breathed on this earth (which I find for my part extremely\n",
      "difficult to believe) I can also assure my readers that he is perfectly\n",
      "authentic.\n",
      "- Both the typhoon and Captain MacWhirr\n",
      "presented themselves to me as the necessities of the deep conviction\n",
      "with which I approached the subject of the story.\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TOPIC DISCOVERY COMPLETE\n",
      "======================================================================\n",
      "Processed 5 novels\n"
     ]
    }
   ],
   "source": [
    "# Load sentences from cleaned novels and run topic discovery per novel\n",
    "\n",
    "output_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\class1')\n",
    "novels_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\novels')\n",
    "\n",
    "# Collect all novels with their sentences\n",
    "all_novels_topics = {}\n",
    "\n",
    "for cleaned_file in sorted(output_dir.glob('*_cleaned.txt')):\n",
    "    novel_name = cleaned_file.stem.replace('_cleaned', '')\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"NOVEL: {novel_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Read cleaned text and split into sentences\n",
    "    cleaned_text = cleaned_file.read_text(encoding='utf-8')\n",
    "    sentences = split_sentences(cleaned_text, min_words=6)\n",
    "    \n",
    "    print(f\"Total sentences: {len(sentences):,}\")\n",
    "    \n",
    "    # Run topic discovery (adjust k based on novel size if needed)\n",
    "    k = min(8, len(sentences) // 100)  # ~1 cluster per 100 sentences, max 8\n",
    "    k = max(5, k)  # at least 5 clusters\n",
    "    \n",
    "    print(f\"Discovering {k} topics...\\n\")\n",
    "    \n",
    "    try:\n",
    "        labels, top_terms = extract_topics(sentences, k=k)\n",
    "        all_novels_topics[novel_name] = {\n",
    "            'labels': labels,\n",
    "            'top_terms': top_terms,\n",
    "            'num_sentences': len(sentences)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {novel_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"TOPIC DISCOVERY COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Processed {len(all_novels_topics)} novels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f731d7",
   "metadata": {},
   "source": [
    "### TOPIC EXTRACTION (EXPORT FOR GEMINI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e978c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export topics from TF-IDF clustering for Gemini API generation\n",
    "# This creates JSON files with topics for each novel\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "output_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\class1')\n",
    "topics_output = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\topics')\n",
    "topics_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_topics_export = {}\n",
    "\n",
    "for cleaned_file in sorted(output_dir.glob('*_cleaned.txt')):\n",
    "    novel_name = cleaned_file.stem.replace('_cleaned', '')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXTRACTING TOPICS: {novel_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Read cleaned text and split into sentences\n",
    "    cleaned_text = cleaned_file.read_text(encoding='utf-8')\n",
    "    sentences = split_sentences(cleaned_text, min_words=6)\n",
    "    \n",
    "    print(f\"Total sentences: {len(sentences):,}\")\n",
    "    \n",
    "    # Determine number of topics (5-10 range)\n",
    "    k = min(10, max(5, len(sentences) // 150))\n",
    "    print(f\"Extracting {k} topics...\")\n",
    "    \n",
    "    try:\n",
    "        # Run TF-IDF + KMeans clustering\n",
    "        tfidf_matrix, feature_names = vectorize_sentences(sentences)\n",
    "        labels, centroids = cluster_sentences(tfidf_matrix, k=k)\n",
    "        top_terms = get_top_terms_per_cluster(centroids, feature_names, top_n=15)\n",
    "        \n",
    "        # Build export structure\n",
    "        topics_export = {\n",
    "            'novel': novel_name,\n",
    "            'num_sentences': len(sentences),\n",
    "            'num_topics': k,\n",
    "            'topics': []\n",
    "        }\n",
    "        \n",
    "        for cluster_id, terms in top_terms.items():\n",
    "            cluster_sentences = [s for s, label in enumerate(labels) if label == cluster_id]\n",
    "            \n",
    "            topics_export['topics'].append({\n",
    "                'topic_id': cluster_id,\n",
    "                'keywords': terms[:10],  # top 10 keywords\n",
    "                'num_sentences': len(cluster_sentences)\n",
    "            })\n",
    "            \n",
    "            print(f\"  Topic {cluster_id}: {', '.join(terms[:5])}\")\n",
    "        \n",
    "        # Save per-novel topics\n",
    "        novel_json = topics_output / f\"{novel_name}_topics.json\"\n",
    "        novel_json.write_text(json.dumps(topics_export, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "        print(f\"✓ Saved to {novel_json.name}\")\n",
    "        \n",
    "        all_topics_export[novel_name] = topics_export\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save master file\n",
    "master_file = topics_output / 'all_novels_topics.json'\n",
    "master_file.write_text(json.dumps(all_topics_export, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TOPIC EXTRACTION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Processed {len(all_topics_export)} novels\")\n",
    "print(f\"Output: {topics_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26030a",
   "metadata": {},
   "source": [
    "#### TOPIC FINDING USING BERT LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d55de4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eisas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745d945",
   "metadata": {},
   "source": [
    "##### Load & prepare a novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "062e19de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_novel(path):\n",
    "    text = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    # Basic safety cleanup (metadata should already be removed)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Remove very short / junk sentences\n",
    "    sentences = [\n",
    "        s for s in sentences\n",
    "        if len(s.split()) >= 8\n",
    "    ]\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2788a",
   "metadata": {},
   "source": [
    "##### Build the BERTopic model (key part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "48c0890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_topic_model(min_df=2):\n",
    "    \"\"\"Build BERTopic model with configurable min_df for flexibility.\"\"\"\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    vectorizer_model = CountVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=min_df,  # flexible minimum\n",
    "        max_df=0.95     # ignore very common terms\n",
    "    )\n",
    "\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        calculate_probabilities=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    return topic_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de3b69",
   "metadata": {},
   "source": [
    "##### Run topic extraction on one novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "850157c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics(sentences, topic_model):\n",
    "    topics, probs = topic_model.fit_transform(sentences)\n",
    "    return topic_model, topics, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4d620",
   "metadata": {},
   "source": [
    "##### Reduce to 5–10 core topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efcb9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_topics(topic_model, sentences, target_topics=8):\n",
    "    topic_model.reduce_topics(\n",
    "        sentences,\n",
    "        nr_topics=target_topics\n",
    "    )\n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a5936e",
   "metadata": {},
   "source": [
    "##### Inspect and export topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26a05699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(topic_model):\n",
    "    topics = topic_model.get_topics()\n",
    "\n",
    "    for topic_id, terms in topics.items():\n",
    "        if topic_id == -1:\n",
    "            continue  # skip outliers\n",
    "\n",
    "        keywords = [term for term, _ in terms[:10]]\n",
    "        print(f\"\\nTopic {topic_id}\")\n",
    "        print(\"Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542aedd7",
   "metadata": {},
   "source": [
    "#### RUN BERT TOPIC EXTRACTION ON ALL NOVELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b55c6444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f099c20824b4f9bfd3d21003ba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f099c20824b4f9bfd3d21003ba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:17,161 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f099c20824b4f9bfd3d21003ba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:17,161 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28dfe1ea590404baa4359399c02792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f099c20824b4f9bfd3d21003ba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:17,161 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28dfe1ea590404baa4359399c02792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:36,814 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:36,815 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:36,835 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:36,837 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:37,081 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:37,085 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,162 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:37,207 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:37,208 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:37,209 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,326 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f099c20824b4f9bfd3d21003ba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:17,161 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28dfe1ea590404baa4359399c02792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:36,814 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:36,815 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:36,835 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:36,837 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:37,081 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:37,085 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,162 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:37,207 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:37,208 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:37,209 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,326 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['trial', 'lawyer', 'lawyers', 'acquittal', 'judges', 'men', 'proceedings', 'accused', 'defendant', 'cases']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['mr', 'watching', 'men', 'said looking', 'wiped', 'mr said', 'cassock', 'looking round', 'greatly', 'glanced']\n",
      " Saved topics to the_trial_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: TYPHOON\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,308\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f099c20824b4f9bfd3d21003ba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:17,161 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28dfe1ea590404baa4359399c02792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:36,814 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:36,815 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:36,835 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:36,837 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:37,081 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:37,085 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,162 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:37,207 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:37,208 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:37,209 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,326 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['trial', 'lawyer', 'lawyers', 'acquittal', 'judges', 'men', 'proceedings', 'accused', 'defendant', 'cases']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['mr', 'watching', 'men', 'said looking', 'wiped', 'mr said', 'cassock', 'looking round', 'greatly', 'glanced']\n",
      " Saved topics to the_trial_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: TYPHOON\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,308\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03fc2943ef345e6b05cfe8faf3e7a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f099c20824b4f9bfd3d21003ba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:17,161 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28dfe1ea590404baa4359399c02792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:36,814 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:36,815 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:36,835 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:36,837 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:37,081 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:37,085 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,162 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:37,207 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:37,208 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:37,209 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,326 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['trial', 'lawyer', 'lawyers', 'acquittal', 'judges', 'men', 'proceedings', 'accused', 'defendant', 'cases']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['mr', 'watching', 'men', 'said looking', 'wiped', 'mr said', 'cassock', 'looking round', 'greatly', 'glanced']\n",
      " Saved topics to the_trial_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: TYPHOON\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,308\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03fc2943ef345e6b05cfe8faf3e7a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:42,298 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f099c20824b4f9bfd3d21003ba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:17,161 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28dfe1ea590404baa4359399c02792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:36,814 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:36,815 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:36,835 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:36,837 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:37,081 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:37,085 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,162 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:37,207 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:37,208 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:37,209 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,326 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['trial', 'lawyer', 'lawyers', 'acquittal', 'judges', 'men', 'proceedings', 'accused', 'defendant', 'cases']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['mr', 'watching', 'men', 'said looking', 'wiped', 'mr said', 'cassock', 'looking round', 'greatly', 'glanced']\n",
      " Saved topics to the_trial_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: TYPHOON\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,308\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03fc2943ef345e6b05cfe8faf3e7a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:42,298 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f099c20824b4f9bfd3d21003ba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:17,161 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28dfe1ea590404baa4359399c02792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:36,814 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:36,815 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:36,835 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:36,837 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:37,081 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:37,085 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,162 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:37,207 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:37,208 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:37,209 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,326 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['trial', 'lawyer', 'lawyers', 'acquittal', 'judges', 'men', 'proceedings', 'accused', 'defendant', 'cases']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['mr', 'watching', 'men', 'said looking', 'wiped', 'mr said', 'cassock', 'looking round', 'greatly', 'glanced']\n",
      " Saved topics to the_trial_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: TYPHOON\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,308\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03fc2943ef345e6b05cfe8faf3e7a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:42,298 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6701ee08a76945fa83b699af7aeb355c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f099c20824b4f9bfd3d21003ba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:17,161 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28dfe1ea590404baa4359399c02792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:36,814 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:36,815 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:36,835 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:36,837 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:37,081 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:37,085 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,162 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:37,207 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:37,208 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:37,209 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,326 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['trial', 'lawyer', 'lawyers', 'acquittal', 'judges', 'men', 'proceedings', 'accused', 'defendant', 'cases']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['mr', 'watching', 'men', 'said looking', 'wiped', 'mr said', 'cassock', 'looking round', 'greatly', 'glanced']\n",
      " Saved topics to the_trial_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: TYPHOON\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,308\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03fc2943ef345e6b05cfe8faf3e7a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:42,298 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6701ee08a76945fa83b699af7aeb355c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:49,955 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:49,956 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:49,971 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:49,973 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:50,076 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:50,084 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:50,145 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:50,173 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:50,174 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:50,175 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:50,278 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOVEL: HEART_OF_DARKNESS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,678\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035af14d0e74184ace602f739ad6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:17,992 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a71773fc6ae4e639a04a382320cafbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:03:26,153 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:03:26,154 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:03:26,166 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:03:26,167 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:03:26,277 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:03:26,281 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,358 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:03:26,409 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:03:26,410 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:03:26,411 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:03:26,490 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['like', 'man', 'know', 'came', 'little', 'river', 'looked', 'long', 'men', 'white']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['rivets', 'envy', 'excited', 'heard kurtz', 'excuse', 'delay', 'sole purpose', 'ripe', 'time ripe', 'favour']\n",
      " Saved topics to heart_of_darkness_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: LORD_JIM\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 5,178\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239c711382d64de18bffe2835ce0751d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:03:31,508 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd599055094a4fa01be6964024720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:02,320 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:02,321 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:02,391 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:02,392 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:02,763 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:02,769 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:02,882 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:02,912 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:02,913 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:02,914 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:03,040 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['man', 'said', 'old', 'men', 'away', 'say', 'white', 'cornelius', 'thing', 'people']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['addressing', 'believing', 'child started', 'clawed', 'choked', 'did dare', 'crammed', 'telling just', 'sorcerer', 'didn care']\n",
      " Saved topics to lord_jim_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: METAMORPHOSIS\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 702\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d5c97ec08c41e9bdbf16b3001ffee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:07,746 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298e48d5fc9047bb8ff62e8691c6644f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:12,451 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:12,452 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:12,462 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:12,464 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:12,499 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:12,503 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,531 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:12,555 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:12,556 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:12,557 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:12,606 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['really', 'legs', 'feel', 'lay', 'wanted', 'food', 'window', 'great', 'set', 'pain']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['gregor father', 'gregor mother', 'gregor room', 'gregor sister', 'heard', 'wanted', 'furniture', 'help', 'straight', 'new']\n",
      " Saved topics to metamorphosis_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: THE_TRIAL\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 3,201\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f099c20824b4f9bfd3d21003ba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:17,161 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28dfe1ea590404baa4359399c02792c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:36,814 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:36,815 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:36,835 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:36,837 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:37,081 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:37,085 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,162 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:37,207 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:37,208 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:37,209 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:37,326 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['trial', 'lawyer', 'lawyers', 'acquittal', 'judges', 'men', 'proceedings', 'accused', 'defendant', 'cases']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['mr', 'watching', 'men', 'said looking', 'wiped', 'mr said', 'cassock', 'looking round', 'greatly', 'glanced']\n",
      " Saved topics to the_trial_topics_bert.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: TYPHOON\n",
      "======================================================================\n",
      "Total sentences (≥8 words): 1,308\n",
      "Building BERTopic model (min_df=2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03fc2943ef345e6b05cfe8faf3e7a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "2026-01-29 19:04:42,298 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BERTopic extraction...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6701ee08a76945fa83b699af7aeb355c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-29 19:04:49,955 - BERTopic - Embedding - Completed ✓\n",
      "2026-01-29 19:04:49,956 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2026-01-29 19:04:49,971 - BERTopic - Dimensionality - Completed ✓\n",
      "2026-01-29 19:04:49,973 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2026-01-29 19:04:50,076 - BERTopic - Cluster - Completed ✓\n",
      "2026-01-29 19:04:50,084 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:50,145 - BERTopic - Representation - Completed ✓\n",
      "2026-01-29 19:04:50,173 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2026-01-29 19:04:50,174 - BERTopic - Topic reduction - Number of topics (5) is equal or higher than the clustered topics(3).\n",
      "2026-01-29 19:04:50,175 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2026-01-29 19:04:50,278 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing to 5 core topics...\n",
      "\n",
      "Topic 0\n",
      "Keywords: ['ship', 'macwhirr', 'captain', 'captain macwhirr', 'sea', 'end', 'way', 'room', 'went', 'black']\n",
      "\n",
      "Topic 1\n",
      "Keywords: ['mr jukes', 'caused', 'louder', 'jukes turned', 'like man', 'catching', 'ventilators', 'hastily', 'meaning', 'doorway']\n",
      " Saved topics to typhoon_topics_bert.json\n",
      "\n",
      "\n",
      "======================================================================\n",
      "BERT TOPIC EXTRACTION COMPLETE\n",
      "======================================================================\n",
      "Processed 5 novels\n",
      "Topics saved to: c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\topics_bert\n",
      "Master file: all_novels_topics_bert.json\n"
     ]
    }
   ],
   "source": [
    "# Run BERTopic-based topic extraction on all cleaned novels\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "output_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\class1')\n",
    "topics_output = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\topics_bert')\n",
    "topics_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_bert_topics = {}\n",
    "\n",
    "for cleaned_file in sorted(output_dir.glob('*_cleaned.txt')):\n",
    "    novel_name = cleaned_file.stem.replace('_cleaned', '')\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"NOVEL: {novel_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Load and split into sentences\n",
    "        sentences = load_and_split_novel(cleaned_file)\n",
    "        print(f\"Total sentences (≥8 words): {len(sentences):,}\")\n",
    "        \n",
    "        if len(sentences) < 50:\n",
    "            print(f\"⚠ Skipping {novel_name}: too few sentences for meaningful clustering\")\n",
    "            continue\n",
    "        \n",
    "        # Build fresh topic model for this novel (BERTopic models can't be reused)\n",
    "        # Use conservative min_df (BERTopic creates topic-level docs, not sentence-level)\n",
    "        # so min_df must be small enough to work with ~5-10 topic clusters\n",
    "        adaptive_min_df = 2  # safe for all corpus sizes\n",
    "        print(f\"Building BERTopic model (min_df={adaptive_min_df})...\")\n",
    "        topic_model_fresh = build_topic_model(min_df=adaptive_min_df)\n",
    "        \n",
    "        # Extract topics using BERTopic\n",
    "        print(\"Running BERTopic extraction...\")\n",
    "        topic_model, topics, probs = extract_topics(sentences, topic_model_fresh)\n",
    "        \n",
    "        # Reduce to 5-10 core topics\n",
    "        target_topics = min(10, max(5, len(set(topics)) // 2))\n",
    "        print(f\"Reducing to {target_topics} core topics...\")\n",
    "        topic_model = reduce_topics(topic_model, sentences, target_topics=target_topics)\n",
    "        \n",
    "        # Print topics to console\n",
    "        print_topics(topic_model)\n",
    "        \n",
    "        # Export to JSON\n",
    "        topics_dict = topic_model.get_topics()\n",
    "        topics_export = {\n",
    "            'novel': novel_name,\n",
    "            'num_sentences': len(sentences),\n",
    "            'num_topics': len([t for t in topics_dict.keys() if t != -1]),\n",
    "            'topics': []\n",
    "        }\n",
    "        \n",
    "        for topic_id, terms in topics_dict.items():\n",
    "            if topic_id == -1:\n",
    "                continue  # skip outliers\n",
    "            \n",
    "            keywords = [term for term, score in terms[:15]]\n",
    "            topic_sentences = [s for s, t in zip(sentences, topics) if t == topic_id]\n",
    "            \n",
    "            topics_export['topics'].append({\n",
    "                'topic_id': topic_id,\n",
    "                'keywords': keywords,\n",
    "                'num_sentences': len(topic_sentences),\n",
    "                'sample_sentences': topic_sentences[:3]\n",
    "            })\n",
    "        \n",
    "        # Save per-novel topics\n",
    "        novel_json = topics_output / f\"{novel_name}_topics_bert.json\"\n",
    "        novel_json.write_text(json.dumps(topics_export, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "        print(f\" Saved topics to {novel_json.name}\")\n",
    "        \n",
    "        all_bert_topics[novel_name] = topics_export\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error processing {novel_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Save master file with all novels\n",
    "master_file = topics_output / 'all_novels_topics_bert.json'\n",
    "master_file.write_text(json.dumps(all_bert_topics, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(\"BERT TOPIC EXTRACTION COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Processed {len(all_bert_topics)} novels\")\n",
    "print(f\"Topics saved to: {topics_output}\")\n",
    "print(f\"Master file: {master_file.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75042b2b",
   "metadata": {},
   "source": [
    "#### GEMINI-BASED APPROACH (GOOGLE AI STUDIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622f213",
   "metadata": {},
   "source": [
    "### SETUP: Gemini API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74a8d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gemini API configured successfully\n",
      "✓ Model: gemini-3-flash-preview\n",
      "✓ API Key: AIzaSyD9iULbli9fennY...\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Gemini API\n",
    "api_key = os.environ.get('GEMINI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in environment variables!\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "\n",
    "MODEL_NAME = 'gemini-3-flash-preview'  \n",
    "\n",
    "# Initialize model\n",
    "model = genai.GenerativeModel(MODEL_NAME)\n",
    "\n",
    "print(\"✓ Gemini API configured successfully\")\n",
    "print(f\"✓ Model: {MODEL_NAME}\")\n",
    "print(f\"✓ API Key: {api_key[:20]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d81cf",
   "metadata": {},
   "source": [
    "### TASK 1: Topic Extraction Using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df41f61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Topic extraction function ready\n"
     ]
    }
   ],
   "source": [
    "def extract_topics_gemini(novel_text, novel_name, num_topics=8):\n",
    "    \"\"\"\n",
    "    Use Gemini to extract main topics from a novel.\n",
    "    \n",
    "    Args:\n",
    "        novel_text: Full text of the novel\n",
    "        novel_name: Name of the novel\n",
    "        num_topics: Number of topics to extract (5-10 range)\n",
    "    \n",
    "    Returns:\n",
    "        List of topic strings\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Analyze the following novel excerpt and identify {num_topics} main themes or topics.\n",
    "\n",
    "Novel: {novel_name}\n",
    "\n",
    "Text excerpt (first 15000 characters):\n",
    "{novel_text[:15000]}\n",
    "\n",
    "Please provide exactly {num_topics} distinct topics/themes that are central to this novel. \n",
    "Format your response as a JSON array of strings, like this:\n",
    "[\"topic1\", \"topic2\", \"topic3\", ...]\n",
    "\n",
    "Each topic should be 2-4 words describing a key theme, subject, or motif in the novel.\n",
    "Examples: \"colonial exploitation\", \"moral ambiguity\", \"existential dread\", \"social alienation\"\n",
    "\n",
    "Return ONLY the JSON array, no additional text.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        topics_text = response.text.strip()\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        if '```json' in topics_text:\n",
    "            topics_text = topics_text.split('```json')[1].split('```')[0].strip()\n",
    "        elif '```' in topics_text:\n",
    "            topics_text = topics_text.split('```')[1].split('```')[0].strip()\n",
    "        \n",
    "        topics = json.loads(topics_text)\n",
    "        \n",
    "        return topics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting topics: {e}\")\n",
    "        print(f\"Response was: {response.text[:500]}\")\n",
    "        return None\n",
    "\n",
    "print(\"✓ Topic extraction function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c863e8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TASK 1: TOPIC EXTRACTION USING GEMINI\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "NOVEL: heart_of_darkness by Joseph Conrad\n",
      "======================================================================\n",
      "Asking Gemini to extract topics...\n",
      "✓ Extracted 8 topics:\n",
      "  1. colonial exploitation\n",
      "  2. savagery versus civilization\n",
      "  3. the nature of darkness\n",
      "  4. moral ambiguity\n",
      "  5. the lure of exploration\n",
      "  6. fascination of the abomination\n",
      "  7. devotion to efficiency\n",
      "  8. imperialist hypocrisy\n",
      "✓ Saved to heart_of_darkness_topics.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: lord_jim by Joseph Conrad\n",
      "======================================================================\n",
      "Asking Gemini to extract topics...\n",
      "✓ Extracted 8 topics:\n",
      "  1. lost honour\n",
      "  2. romantic idealism\n",
      "  3. moral failure\n",
      "  4. identity and reputation\n",
      "  5. the seafaring life\n",
      "  6. heroism and cowardice\n",
      "  7. moral ambiguity\n",
      "  8. the weight of the past\n",
      "✓ Saved to lord_jim_topics.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: metamorphosis by Franz Kafka\n",
      "======================================================================\n",
      "Asking Gemini to extract topics...\n",
      "✓ Extracted 8 topics:\n",
      "  1. social alienation\n",
      "  2. existential absurdity\n",
      "  3. dehumanization of labor\n",
      "  4. familial obligation\n",
      "  5. loss of identity\n",
      "  6. physical transformation\n",
      "  7. communication breakdown\n",
      "  8. capitalist oppression\n",
      "✓ Saved to metamorphosis_topics.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: the_trial by Franz Kafka\n",
      "======================================================================\n",
      "Asking Gemini to extract topics...\n",
      "✓ Extracted 8 topics:\n",
      "  1. bureaucratic absurdity\n",
      "  2. existential isolation\n",
      "  3. pervasive surveillance\n",
      "  4. the nature of guilt\n",
      "  5. faceless authority\n",
      "  6. powerlessness of the individual\n",
      "  7. absurdity of the law\n",
      "  8. arbitrary arrest\n",
      "✓ Saved to the_trial_topics.json\n",
      "\n",
      "======================================================================\n",
      "NOVEL: typhoon by Joseph Conrad\n",
      "======================================================================\n",
      "Asking Gemini to extract topics...\n",
      "✓ Extracted 8 topics:\n",
      "  1. elemental fury of nature\n",
      "  2. unimaginative literalism\n",
      "  3. maritime professional duty\n",
      "  4. unflappable human resilience\n",
      "  5. cross-cultural tension\n",
      "  6. isolation of command\n",
      "  7. unpredictability of fate\n",
      "  8. social order under stress\n",
      "✓ Saved to typhoon_topics.json\n",
      "\n",
      "======================================================================\n",
      "TOPIC EXTRACTION COMPLETE\n",
      "======================================================================\n",
      "Processed 5 novels\n",
      "Output: c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\topics_gemini\n"
     ]
    }
   ],
   "source": [
    "# Extract topics for all novels using Gemini\n",
    "output_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\class1')\n",
    "topics_output = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\topics_gemini')\n",
    "topics_output.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Author mapping\n",
    "author_map = {\n",
    "    'heart_of_darkness': 'Joseph Conrad',\n",
    "    'lord_jim': 'Joseph Conrad',\n",
    "    'metamorphosis': 'Franz Kafka',\n",
    "    'the_trial': 'Franz Kafka',\n",
    "    'typhoon': 'Joseph Conrad'\n",
    "}\n",
    "\n",
    "all_topics_gemini = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TASK 1: TOPIC EXTRACTION USING GEMINI\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for cleaned_file in sorted(output_dir.glob('*_cleaned.txt')):\n",
    "    novel_name = cleaned_file.stem.replace('_cleaned', '')\n",
    "    author = author_map.get(novel_name, 'Unknown')\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"NOVEL: {novel_name} by {author}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Read cleaned text\n",
    "    novel_text = cleaned_file.read_text(encoding='utf-8')\n",
    "    \n",
    "    # Extract topics using Gemini\n",
    "    print(\"Asking Gemini to extract topics...\")\n",
    "    topics = extract_topics_gemini(novel_text, novel_name, num_topics=8)\n",
    "    \n",
    "    if topics:\n",
    "        print(f\"✓ Extracted {len(topics)} topics:\")\n",
    "        for i, topic in enumerate(topics, 1):\n",
    "            print(f\"  {i}. {topic}\")\n",
    "        \n",
    "        # Save topics\n",
    "        topic_data = {\n",
    "            'novel': novel_name,\n",
    "            'author': author,\n",
    "            'topics': topics,\n",
    "            'num_topics': len(topics)\n",
    "        }\n",
    "        \n",
    "        topic_file = topics_output / f\"{novel_name}_topics.json\"\n",
    "        topic_file.write_text(json.dumps(topic_data, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "        print(f\"✓ Saved to {topic_file.name}\")\n",
    "        \n",
    "        all_topics_gemini[novel_name] = topic_data\n",
    "    else:\n",
    "        print(f\"✗ Failed to extract topics\")\n",
    "    \n",
    "    # Rate limiting\n",
    "    time.sleep(2)\n",
    "\n",
    "# Save master file\n",
    "master_file = topics_output / 'all_topics.json'\n",
    "master_file.write_text(json.dumps(all_topics_gemini, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TOPIC EXTRACTION COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Processed {len(all_topics_gemini)} novels\")\n",
    "print(f\"Output: {topics_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67898d0",
   "metadata": {},
   "source": [
    "### TASK 2: Class 2 - Generic Paragraph Generation (500 paragraphs on topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c96f9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Class 2 generation function ready\n"
     ]
    }
   ],
   "source": [
    "def generate_generic_paragraphs(topics, novel_name, n_paragraphs=500, min_words=100, max_words=200, batch_size=20):\n",
    "    \"\"\"\n",
    "    Generate generic paragraphs on given topics (no author style mimicking).\n",
    "    \n",
    "    Args:\n",
    "        topics: List of topic strings\n",
    "        novel_name: Name of the novel (for file naming)\n",
    "        n_paragraphs: Total paragraphs to generate\n",
    "        min_words: Minimum words per paragraph\n",
    "        max_words: Maximum words per paragraph\n",
    "        batch_size: Paragraphs per API call\n",
    "    \n",
    "    Returns:\n",
    "        Path to output file\n",
    "    \"\"\"\n",
    "    output_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\class2')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / f\"{novel_name}_generic.jsonl\"\n",
    "    \n",
    "    # Remove existing file if present\n",
    "    if output_file.exists():\n",
    "        output_file.unlink()\n",
    "    \n",
    "    topics_str = \", \".join(topics)\n",
    "    generated_count = 0\n",
    "    \n",
    "    print(f\"Generating {n_paragraphs} generic paragraphs...\")\n",
    "    print(f\"Topics: {topics_str[:100]}...\")\n",
    "    \n",
    "    while generated_count < n_paragraphs:\n",
    "        to_generate = min(batch_size, n_paragraphs - generated_count)\n",
    "        \n",
    "        prompt = f\"\"\"Write {to_generate} distinct paragraphs about the following topics: {topics_str}\n",
    "\n",
    "Requirements:\n",
    "- Each paragraph should be {min_words}-{max_words} words\n",
    "- Write in a clear, general prose style (not mimicking any specific author)\n",
    "- Each paragraph should explore different aspects of the topics\n",
    "- Make each paragraph standalone and coherent\n",
    "- Focus on thematic exploration, not storytelling\n",
    "\n",
    "Return ONLY the paragraphs, separated by three dashes (---), like this:\n",
    "paragraph 1 text here...\n",
    "---\n",
    "paragraph 2 text here...\n",
    "---\n",
    "paragraph 3 text here...\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            text = response.text.strip()\n",
    "            \n",
    "            # Split by delimiter\n",
    "            paragraphs = [p.strip() for p in text.split('---') if p.strip()]\n",
    "            \n",
    "            # Save paragraphs\n",
    "            with output_file.open('a', encoding='utf-8') as f:\n",
    "                for para in paragraphs[:to_generate]:\n",
    "                    # Basic word count check\n",
    "                    word_count = len(para.split())\n",
    "                    if word_count < min_words * 0.8:  # Allow 20% tolerance\n",
    "                        continue\n",
    "                    \n",
    "                    entry = {\n",
    "                        'novel': novel_name,\n",
    "                        'class': 'class2_generic',\n",
    "                        'text': para,\n",
    "                        'word_count': word_count\n",
    "                    }\n",
    "                    f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "                    generated_count += 1\n",
    "                    \n",
    "                    if generated_count >= n_paragraphs:\n",
    "                        break\n",
    "            \n",
    "            print(f\"  Progress: {generated_count}/{n_paragraphs} paragraphs\")\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in batch: {e}\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "    \n",
    "    print(f\"✓ Generated {generated_count} paragraphs\")\n",
    "    return output_file\n",
    "\n",
    "print(\"✓ Class 2 generation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf3bcd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TASK 2: CLASS 2 - GENERIC PARAGRAPH GENERATION\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "NOVEL: heart_of_darkness\n",
      "======================================================================\n",
      "Generating 500 generic paragraphs...\n",
      "Topics: colonial exploitation, savagery versus civilization, the nature of darkness, moral ambiguity, the lu...\n",
      "  Progress: 20/500 paragraphs\n",
      "  Progress: 40/500 paragraphs\n",
      "  Progress: 60/500 paragraphs\n",
      "  Progress: 80/500 paragraphs\n",
      "  Progress: 100/500 paragraphs\n",
      "  Progress: 120/500 paragraphs\n",
      "  Progress: 140/500 paragraphs\n",
      "  Progress: 160/500 paragraphs\n",
      "  Progress: 180/500 paragraphs\n",
      "  Progress: 200/500 paragraphs\n",
      "  Progress: 220/500 paragraphs\n",
      "  Progress: 240/500 paragraphs\n",
      "  Progress: 260/500 paragraphs\n",
      "  Progress: 280/500 paragraphs\n",
      "  Progress: 300/500 paragraphs\n",
      "  Progress: 320/500 paragraphs\n",
      "  Progress: 340/500 paragraphs\n",
      "  Progress: 360/500 paragraphs\n",
      "  Progress: 380/500 paragraphs\n",
      "  Progress: 400/500 paragraphs\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 51.45170251s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 49.295988123s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 47.141838324s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 47\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 44.985729397s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 42.846158411s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 42\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 40.701188839s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 38.542614939s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 36.395656938s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 34.237474437s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 34\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 32.097122198s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 32\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 29.940338029s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 29\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 27.796958066s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 27\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 25.620067445s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 23.465579766s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 21.312838257s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 19.162312316s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 17.013639525s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 17\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 14.859145452s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 12.719399558s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 12\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 10.578933206s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 10\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 8.431516109s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 8\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 6.292632648s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 6\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 4.143276332s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 4\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 2.000846625s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 59.8658767s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 59\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 57.727666469s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 55.595781065s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 53.451444916s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 51.296912146s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 49.152828899s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 47.001565575s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 47\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 44.862075569s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 42.707414259s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 42\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 40.547346906s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 38.406578782s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 36.265983987s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 34.127032517s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 34\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 31.980858569s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 31\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 29.839934433s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 29\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 27.684545515s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 27\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 25.538185494s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 23.402952105s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 21.260530461s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 19.120246371s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 16.98535239s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 16\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 14.84906331s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 12.693515385s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 12\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 10.551507484s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 10\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 8.409114249s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 8\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 6.267845029s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 6\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 4.119998226s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 4\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 1.977459897s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 1\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 59.834093427s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 59\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 57.698769711s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 55.553973435s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 53.414550577s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 51.275633419s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 49.120949512s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 46.977820907s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 46\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 44.841719001s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 42.698902757s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 42\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 40.558996431s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 38.4242154s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 36.287675772s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 34.143067463s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 34\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 32.000197217s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 32\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 29.865194512s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 29\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 27.72988173s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 27\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 25.592357196s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 23.453059559s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 21.307129914s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 19.144679002s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 17.01498061s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 17\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 14.873925943s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 12.732508695s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 12\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 10.59982373s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 10\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 8.471628209s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 8\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 6.318360405s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 6\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 4.180833749s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 4\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 2.048680386s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 59.914166578s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 59\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 57.761282221s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 55.634210103s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 53.50528995s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 51.374460688s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 49.236603561s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 47.096244211s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 47\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 44.963992247s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 44\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 42.825764693s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 42\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 40.696610811s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 38.549223322s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 36.413169945s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 34.275733649s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 34\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 32.138866569s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 32\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 29.991379083s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 29\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 27.857443678s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 27\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 25.72718061s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 23.59652747s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 21.461716784s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 19.331844356s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 17.199646499s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 17\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 15.067554817s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 12.939279517s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 12\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 10.803298031s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 10\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 8.67050874s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 8\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 6.538913602s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 6\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 4.398399196s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 4\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 2.265969914s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 127.370425ms. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 57.982810088s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 57\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 55.85560216s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 55\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 53.721799479s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 51.588465188s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 49.454598105s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 47.312166134s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 47\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 45.177698928s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 45\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 43.037380702s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 43\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 40.904543599s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 40\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 38.777900342s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 38\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 36.640472353s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 34.508967886s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 34\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 32.376402013s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 32\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 30.244253629s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 30\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 28.106005233s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 28\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 25.973515113s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 23.83844452s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 23\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 21.711231321s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 21\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 19.578976375s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 19\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 17.44832886s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 17\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 15.311074721s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 13.174068086s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 13\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 11.041372225s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 11\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 8.894783186s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 8\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 6.766441013s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 6\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 4.628618327s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 4\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 2.492567787s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 2\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 354.755544ms. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 58.211461092s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 56.077914208s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 56\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 53.942114311s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 53\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 51.806604126s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 51\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 49.663214308s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 49\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 47.53303197s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 47\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 45.3999819s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 45\n",
      "}\n",
      "]\n",
      "  Error in batch: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n",
      "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\n",
      "Please retry in 43.263587203s. [links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-3-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 20\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 43\n",
      "}\n",
      "]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhausted\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mgenerate_generic_paragraphs\u001b[39m\u001b[34m(topics, novel_name, n_paragraphs, min_words, max_words, batch_size)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     text = response.text.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\.venv\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[39m, in \u001b[36mGenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types.GenerateContentResponse.from_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\.venv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\.venv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\.venv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\.venv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\.venv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:77\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mResourceExhausted\u001b[39m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-3-flash\nPlease retry in 43.263587203s. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-3-flash\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n  quota_value: 20\n}\n, retry_delay {\n  seconds: 43\n}\n]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m     \u001b[38;5;66;03m# Generate 500 generic paragraphs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     output_file = \u001b[43mgenerate_generic_paragraphs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnovel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnovel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_paragraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_words\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_words\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mgenerate_generic_paragraphs\u001b[39m\u001b[34m(topics, novel_name, n_paragraphs, min_words, max_words, batch_size)\u001b[39m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Error in batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerated_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m paragraphs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run Class 2 generation for all novels\n",
    "print(\"=\"*70)\n",
    "print(\"TASK 2: CLASS 2 - GENERIC PARAGRAPH GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "topics_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\topics_gemini')\n",
    "\n",
    "for topic_file in sorted(topics_dir.glob('*_topics.json')):\n",
    "    if topic_file.name == 'all_topics.json':\n",
    "        continue\n",
    "    \n",
    "    # Load topics\n",
    "    with topic_file.open() as f:\n",
    "        topic_data = json.load(f)\n",
    "    \n",
    "    novel_name = topic_data['novel']\n",
    "    topics = topic_data['topics']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"NOVEL: {novel_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Generate 500 generic paragraphs\n",
    "    output_file = generate_generic_paragraphs(\n",
    "        topics=topics,\n",
    "        novel_name=novel_name,\n",
    "        n_paragraphs=500,\n",
    "        min_words=100,\n",
    "        max_words=200,\n",
    "        batch_size=20\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Saved to: {output_file}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLASS 2 GENERATION COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853e305",
   "metadata": {},
   "source": [
    "### RESUME CLASS 2 GENERATION (Continue from where it left off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bfb8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_existing_paragraphs(output_file):\n",
    "    \"\"\"Count how many paragraphs already exist in a JSONL file.\"\"\"\n",
    "    if not output_file.exists():\n",
    "        return 0\n",
    "    \n",
    "    count = 0\n",
    "    with output_file.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4f3b4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Resume function ready\n",
      "Usage: resume_generic_paragraphs('novel_name', topics_list, target_total=500)\n"
     ]
    }
   ],
   "source": [
    "def resume_generic_paragraphs(novel_name, topics, target_total=500, min_words=100, max_words=200, batch_size=20):\n",
    "    \"\"\"\n",
    "    Resume generating paragraphs for a specific novel.\n",
    "    Only generates the remaining paragraphs needed to reach target_total.\n",
    "    \"\"\"\n",
    "    output_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\class2')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / f\"{novel_name}_generic.jsonl\"\n",
    "    \n",
    "    # Count existing paragraphs\n",
    "    existing_count = count_existing_paragraphs(output_file)\n",
    "    print(f\"Novel: {novel_name}\")\n",
    "    print(f\"Existing paragraphs: {existing_count}\")\n",
    "    print(f\"Target total: {target_total}\")\n",
    "    \n",
    "    if existing_count >= target_total:\n",
    "        print(f\"✓ Already complete! ({existing_count}/{target_total})\")\n",
    "        return output_file\n",
    "    \n",
    "    remaining = target_total - existing_count\n",
    "    print(f\"Generating {remaining} more paragraphs...\\n\")\n",
    "    \n",
    "    topics_str = \", \".join(topics)\n",
    "    generated_count = 0\n",
    "    \n",
    "    while generated_count < remaining:\n",
    "        to_generate = min(batch_size, remaining - generated_count)\n",
    "        \n",
    "        prompt = f\"\"\"Write {to_generate} distinct paragraphs about the following topics: {topics_str}\n",
    "\n",
    "Requirements:\n",
    "- Each paragraph should be {min_words}-{max_words} words\n",
    "- Write in a clear, general prose style (not mimicking any specific author)\n",
    "- Each paragraph should explore different aspects of the topics\n",
    "- Make each paragraph standalone and coherent\n",
    "- Focus on thematic exploration, not storytelling\n",
    "\n",
    "Return ONLY the paragraphs, separated by three dashes (---), like this:\n",
    "paragraph 1 text here...\n",
    "---\n",
    "paragraph 2 text here...\n",
    "---\n",
    "paragraph 3 text here...\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            text = response.text.strip()\n",
    "            \n",
    "            # Split by delimiter\n",
    "            paragraphs = [p.strip() for p in text.split('---') if p.strip()]\n",
    "            \n",
    "            # Append paragraphs (not overwrite!)\n",
    "            with output_file.open('a', encoding='utf-8') as f:\n",
    "                for para in paragraphs[:to_generate]:\n",
    "                    # Basic word count check\n",
    "                    word_count = len(para.split())\n",
    "                    if word_count < min_words * 0.8:  # Allow 20% tolerance\n",
    "                        continue\n",
    "                    \n",
    "                    entry = {\n",
    "                        'novel': novel_name,\n",
    "                        'class': 'class2_generic',\n",
    "                        'text': para,\n",
    "                        'word_count': word_count\n",
    "                    }\n",
    "                    f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "                    generated_count += 1\n",
    "                    \n",
    "                    if generated_count >= remaining:\n",
    "                        break\n",
    "            \n",
    "            total_now = existing_count + generated_count\n",
    "            print(f\"  Progress: {total_now}/{target_total} paragraphs (added {generated_count})\")\n",
    "            time.sleep(1.5)  # Slightly longer delay to avoid rate limits\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"  Error in batch: {error_msg[:200]}\")\n",
    "            \n",
    "            # If rate limit error, show how to resume\n",
    "            if \"429\" in error_msg or \"quota\" in error_msg.lower():\n",
    "                print(f\"\\n⚠ Rate limit hit!\")\n",
    "                print(f\"Progress saved: {existing_count + generated_count}/{target_total}\")\n",
    "                print(f\"To resume, run this cell again after waiting.\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(3)\n",
    "            continue\n",
    "    \n",
    "    final_count = count_existing_paragraphs(output_file)\n",
    "    print(f\"\\n✓ Final count: {final_count}/{target_total} paragraphs\")\n",
    "    return output_file\n",
    "\n",
    "print(\"✓ Resume function ready\")\n",
    "print(\"Usage: resume_generic_paragraphs('novel_name', topics_list, target_total=500)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7621ff6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESUMING GENERATION FOR: typhoon\n",
      "======================================================================\n",
      "\n",
      "Novel: typhoon\n",
      "Existing paragraphs: 326\n",
      "Target total: 500\n",
      "Generating 174 more paragraphs...\n",
      "\n",
      "  Progress: 346/500 paragraphs (added 20)\n",
      "  Progress: 366/500 paragraphs (added 40)\n",
      "  Progress: 386/500 paragraphs (added 60)\n",
      "  Progress: 406/500 paragraphs (added 80)\n",
      "  Progress: 426/500 paragraphs (added 100)\n",
      "  Progress: 446/500 paragraphs (added 120)\n",
      "  Progress: 466/500 paragraphs (added 140)\n",
      "  Progress: 486/500 paragraphs (added 160)\n",
      "  Progress: 500/500 paragraphs (added 174)\n",
      "\n",
      "✓ Final count: 500/500 paragraphs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/eisas/OneDrive/Desktop/PROJECTS/Precog_task/output/class2/typhoon_generic.jsonl')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RESUME GENERATION FOR SPECIFIC NOVEL\n",
    "# Edit the novel_name below to resume generation for any incomplete novel\n",
    "\n",
    "# Load topics for the novel\n",
    "topics_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\topics_gemini')\n",
    "\n",
    "# Specify which novel to resume (change this as needed)\n",
    "novel_to_resume = 'typhoon'  # Change to: 'lord_jim', 'metamorphosis', 'the_trial', 'typhoon'\n",
    "\n",
    "# Load topics for this novel\n",
    "topic_file = topics_dir / f\"{novel_to_resume}_topics.json\"\n",
    "with topic_file.open() as f:\n",
    "    topic_data = json.load(f)\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"RESUMING GENERATION FOR: {novel_to_resume}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Resume generation\n",
    "resume_generic_paragraphs(\n",
    "    novel_name=novel_to_resume,\n",
    "    topics=topic_data['topics'],\n",
    "    target_total=500,\n",
    "    min_words=100,\n",
    "    max_words=200,\n",
    "    batch_size=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6031a43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CLASS 2 GENERATION STATUS\n",
      "======================================================================\n",
      "heart_of_darkness   : 500/500  ✓ COMPLETE\n",
      "lord_jim            : 500/500  ✓ COMPLETE\n",
      "metamorphosis       : 500/500  ✓ COMPLETE\n",
      "the_trial           : 500/500  ✓ COMPLETE\n",
      "typhoon             : 500/500  ✓ COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# CHECK STATUS OF ALL NOVELS\n",
    "# Run this to see which novels are incomplete\n",
    "\n",
    "output_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\class2')\n",
    "topics_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\topics_gemini')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLASS 2 GENERATION STATUS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "novel_names = ['heart_of_darkness', 'lord_jim', 'metamorphosis', 'the_trial', 'typhoon']\n",
    "\n",
    "for novel_name in novel_names:\n",
    "    output_file = output_dir / f\"{novel_name}_generic.jsonl\"\n",
    "    count = count_existing_paragraphs(output_file)\n",
    "    status = \"✓ COMPLETE\" if count >= 500 else f\"⚠ INCOMPLETE ({500 - count} remaining)\"\n",
    "    print(f\"{novel_name:20s}: {count:3d}/500  {status}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb5f66",
   "metadata": {},
   "source": [
    "### TASK 3: Class 3 - Style-Matched Paragraph Generation (500 paragraphs mimicking author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65cd541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_style_matched_paragraphs(topics, novel_name, author, style_sample, n_paragraphs=500, min_words=100, max_words=200, batch_size=15):\n",
    "    \"\"\"\n",
    "    Generate paragraphs mimicking the author's writing style.\n",
    "    \n",
    "    Args:\n",
    "        topics: List of topic strings\n",
    "        novel_name: Name of the novel\n",
    "        author: Author name\n",
    "        style_sample: Sample text from the author (for style reference)\n",
    "        n_paragraphs: Total paragraphs to generate\n",
    "        min_words: Minimum words per paragraph\n",
    "        max_words: Maximum words per paragraph\n",
    "        batch_size: Paragraphs per API call\n",
    "    \n",
    "    Returns:\n",
    "        Path to output file\n",
    "    \"\"\"\n",
    "    output_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\class3_styled')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    output_file = output_dir / f\"{novel_name}_styled.jsonl\"\n",
    "    \n",
    "    # Remove existing file if present\n",
    "    if output_file.exists():\n",
    "        output_file.unlink()\n",
    "    \n",
    "    topics_str = \", \".join(topics)\n",
    "    generated_count = 0\n",
    "    \n",
    "    print(f\"Generating {n_paragraphs} style-matched paragraphs...\")\n",
    "    print(f\"Author: {author}\")\n",
    "    print(f\"Topics: {topics_str[:100]}...\")\n",
    "    \n",
    "    while generated_count < n_paragraphs:\n",
    "        to_generate = min(batch_size, n_paragraphs - generated_count)\n",
    "        \n",
    "        prompt = f\"\"\"You are a skilled writer tasked with writing in the style of {author}.\n",
    "\n",
    "Here is a sample of {author}'s writing style from their work:\n",
    "{style_sample[:3000]}\n",
    "\n",
    "Now, write {to_generate} distinct paragraphs in {author}'s distinctive writing style, exploring these topics: {topics_str}\n",
    "\n",
    "Requirements:\n",
    "- Each paragraph should be {min_words}-{max_words} words\n",
    "- Mimic {author}'s sentence structure, vocabulary, tone, and literary techniques\n",
    "- Maintain {author}'s characteristic voice and perspective\n",
    "- Each paragraph should explore different aspects of the topics\n",
    "- Make each paragraph standalone and coherent\n",
    "\n",
    "Return ONLY the paragraphs, separated by three dashes (---), like this:\n",
    "paragraph 1 text here...\n",
    "---\n",
    "paragraph 2 text here...\n",
    "---\n",
    "paragraph 3 text here...\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            text = response.text.strip()\n",
    "            \n",
    "            # Split by delimiter\n",
    "            paragraphs = [p.strip() for p in text.split('---') if p.strip()]\n",
    "            \n",
    "            # Save paragraphs\n",
    "            with output_file.open('a', encoding='utf-8') as f:\n",
    "                for para in paragraphs[:to_generate]:\n",
    "                    # Basic word count check\n",
    "                    word_count = len(para.split())\n",
    "                    if word_count < min_words * 0.8:  # Allow 20% tolerance\n",
    "                        continue\n",
    "                    \n",
    "                    entry = {\n",
    "                        'novel': novel_name,\n",
    "                        'author': author,\n",
    "                        'class': 'class3_styled',\n",
    "                        'text': para,\n",
    "                        'word_count': word_count\n",
    "                    }\n",
    "                    f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "                    generated_count += 1\n",
    "                    \n",
    "                    if generated_count >= n_paragraphs:\n",
    "                        break\n",
    "            \n",
    "            print(f\"  Progress: {generated_count}/{n_paragraphs} paragraphs\")\n",
    "            time.sleep(1)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in batch: {e}\")\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "    \n",
    "    print(f\"✓ Generated {generated_count} paragraphs\")\n",
    "    return output_file\n",
    "\n",
    "print(\"✓ Class 3 generation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Class 3 generation for all novels\n",
    "print(\"=\"*70)\n",
    "print(\"TASK 3: CLASS 3 - STYLE-MATCHED PARAGRAPH GENERATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "topics_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\topics_gemini')\n",
    "class1_dir = Path(r'c:\\Users\\eisas\\OneDrive\\Desktop\\PROJECTS\\Precog_task\\output\\class1')\n",
    "\n",
    "for topic_file in sorted(topics_dir.glob('*_topics.json')):\n",
    "    if topic_file.name == 'all_topics.json':\n",
    "        continue\n",
    "    \n",
    "    # Load topics\n",
    "    with topic_file.open() as f:\n",
    "        topic_data = json.load(f)\n",
    "    \n",
    "    novel_name = topic_data['novel']\n",
    "    author = topic_data['author']\n",
    "    topics = topic_data['topics']\n",
    "    \n",
    "    # Load style sample from cleaned novel\n",
    "    cleaned_file = class1_dir / f\"{novel_name}_cleaned.txt\"\n",
    "    style_sample = cleaned_file.read_text(encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"NOVEL: {novel_name} by {author}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Generate 500 style-matched paragraphs\n",
    "    output_file = generate_style_matched_paragraphs(\n",
    "        topics=topics,\n",
    "        novel_name=novel_name,\n",
    "        author=author,\n",
    "        style_sample=style_sample,\n",
    "        n_paragraphs=500,\n",
    "        min_words=100,\n",
    "        max_words=200,\n",
    "        batch_size=15\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Saved to: {output_file}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLASS 3 GENERATION COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\n🎉 ALL TASKS COMPLETE! 🎉\")\n",
    "print(\"\\nSummary:\")\n",
    "print(\"✓ Task 1: Topics extracted using Gemini\")\n",
    "print(\"✓ Task 2: 500 generic paragraphs per novel (class2_generic/)\")\n",
    "print(\"✓ Task 3: 500 style-matched paragraphs per novel (class3_styled/)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
